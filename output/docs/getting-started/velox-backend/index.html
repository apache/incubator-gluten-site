<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li:not(:nth-child(1)) > a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(2) > ul > li:nth-child(1) > ul > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(2) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(2) > ul > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(2) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(2) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Getting Start with Velox Backend | Apache Gluten incubating</title> <meta name="generator" content="Jekyll v3.9.3" /> <meta property="og:title" content="Getting Start with Velox Backend" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Gluten is a middle layer responsible for offloading JVM-based SQL engines’ execution to native engines." /> <meta property="og:description" content="Gluten is a middle layer responsible for offloading JVM-based SQL engines’ execution to native engines." /> <link rel="canonical" href="https://gluten.apache.org/docs/getting-started/velox-backend/" /> <meta property="og:url" content="https://gluten.apache.org/docs/getting-started/velox-backend/" /> <meta property="og:site_name" content="Apache Gluten incubating" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Getting Start with Velox Backend" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Gluten is a middle layer responsible for offloading JVM-based SQL engines’ execution to native engines.","headline":"Getting Start with Velox Backend","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://gluten.apache.org/assets/images/gluten-logo-blue.png"}},"url":"https://gluten.apache.org/docs/getting-started/velox-backend/"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> <div class="site-logo" role="img" aria-label="Apache Gluten incubating"></div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Documentations category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/" class="nav-list-link">Documentations</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Getting-Started category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/getting-started/" class="nav-list-link">Getting-Started</a><ul class="nav-list"><li class="nav-list-item"> <a href="/docs/getting-started/velox-backend/" class="nav-list-link">Getting Start with Velox Backend</a> </li><li class="nav-list-item"> <a href="/docs/getting-started/clickhouse-backend/" class="nav-list-link">Getting start with ClickHouse Backend</a> </li><li class="nav-list-item"> <a href="/docs/getting-started/build-guide/" class="nav-list-link">Build Parameters for Velox Backend</a> </li><li class="nav-list-item"> <a href="/docs/getting-started/s3/" class="nav-list-link">Using S3 with Gluten</a> </li><li class="nav-list-item"> <a href="/docs/getting-started/gcs/" class="nav-list-link">Using GCS with Gluten</a> </li><li class="nav-list-item"> <a href="/docs/getting-started/abfs/" class="nav-list-link">Using ABFS with Gluten</a> </li></ul></li><li class="nav-list-item"><a href="/docs/configuration/" class="nav-list-link">Configuration</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Developers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/developers/" class="nav-list-link">Developers</a><ul class="nav-list"><li class="nav-list-item"> <a href="/docs/developers/how-to-use/" class="nav-list-link">How To Use Gluten</a> </li><li class="nav-list-item"> <a href="/docs/developers/new-to/" class="nav-list-link">New To Gluten</a> </li><li class="nav-list-item"> <a href="/docs/developers/microbenchmarks/" class="nav-list-link">Micro Benchmarks for Velox Backend</a> </li><li class="nav-list-item"> <a href="/docs/developers/cpp-code-style/" class="nav-list-link">CPP Code Style</a> </li><li class="nav-list-item"> <a href="/docs/developers/velox-function-dev/" class="nav-list-link">Velox Function Development</a> </li><li class="nav-list-item"> <a href="/docs/developers/docker-ubuntu/" class="nav-list-link">Docker script for Ubuntu 22.04/20.04</a> </li><li class="nav-list-item"> <a href="/docs/developers/docker-centos7/" class="nav-list-link">Docker script for CentOS 7</a> </li><li class="nav-list-item"> <a href="/docs/developers/docker-centos8/" class="nav-list-link">Docker script for CentOS 8</a> </li><li class="nav-list-item"> <a href="/docs/developers/substrait/" class="nav-list-link">Substrait Modifications</a> </li><li class="nav-list-item"> <a href="/docs/developers/how-to-release/" class="nav-list-link">How To Release</a> </li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Velox Backend category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/velox-backend/" class="nav-list-link">Velox Backend</a><ul class="nav-list"><li class="nav-list-item"> <a href="/docs/velox-backend/support/" class="nav-list-link">Supported Operators & Functions</a> </li><li class="nav-list-item"> <a href="/docs/velox-backend/limitations/" class="nav-list-link">Limitations</a> </li><li class="nav-list-item"> <a href="/docs/velox-backend/troubleshooting/" class="nav-list-link">Troubleshooting</a> </li><li class="nav-list-item"> <a href="/docs/velox-backend/udf/" class="nav-list-link">Velox UDF</a> </li></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Archives category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/archives/" class="nav-list-link">Archives</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in v1.1.1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/archives/v1.1.1/" class="nav-list-link">v1.1.1</a><ul class="nav-list"><li class="nav-list-item"> <a href="/archives/v1.1.1/docs/" class="nav-list-link">v1.1.1/Documentation</a> </li><li class="nav-list-item"> <a href="/archives/v1.1.1/developers/" class="nav-list-link">v1.1.1/Developers</a> </li></ul></li></ul></li><li class="nav-list-item"><a href="/release.html" class="nav-list-link">Gluten Release</a></li><li class="nav-list-item"><a href="/references/" class="nav-list-link">Gluten References</a></li><li class="nav-list-item"><a href="/contributing.html" class="nav-list-link">Contributing to Gluten</a></li><li class="nav-list-item"><a href="/asf/" class="nav-list-link">Apache Software Foundation</a></li><li class="nav-list-item"><a href="/contact-us.html" class="nav-list-link">Contact Us</a></li><li class="nav-list-item"><a href="/about/" class="nav-list-link">About</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Apache Gluten incubating" aria-label="Search Apache Gluten incubating" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="//github.com/apache/incubator-gluten" class="site-button" target="_blank" rel="noopener noreferrer" > Apache Gluten Github </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/docs/">Documentations</a></li> <li class="breadcrumb-nav-list-item"><a href="/docs/getting-started/">Getting-Started</a></li> <li class="breadcrumb-nav-list-item"><span>Getting Start with Velox Backend</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="supported-version"> <a href="#supported-version" class="anchor-heading" aria-labelledby="supported-version"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Supported Version </h1> <div class="table-wrapper"><table> <thead> <tr> <th>Type</th> <th>Version</th> </tr> </thead> <tbody> <tr> <td>Spark</td> <td>3.2.2, 3.3.1, 3.4.2, 3.5.1</td> </tr> <tr> <td>OS</td> <td>Ubuntu20.04/22.04, Centos7/8</td> </tr> <tr> <td>jdk</td> <td>openjdk8/jdk17</td> </tr> <tr> <td>scala</td> <td>2.12</td> </tr> </tbody> </table></div> <h1 id="prerequisite"> <a href="#prerequisite" class="anchor-heading" aria-labelledby="prerequisite"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Prerequisite </h1> <p>Currently, Gluten+Velox backend is only tested on <strong>Ubuntu20.04/Ubuntu22.04/Centos7/Centos8</strong>. Other kinds of OS support are still in progress. The long term goal is to support several common OS and conda env deployment.</p> <p>Currently, the officially supported Spark versions are 3.2.2, 3.3.1, 3.4.2 and 3.5.1.</p> <p>We need to set up the <code class="language-plaintext highlighter-rouge">JAVA_HOME</code> env. Currently, Gluten supports <strong>java 8</strong> and <strong>java 17</strong>.</p> <p><strong>For x86_64</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## make sure jdk8 is used</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/java-8-openjdk-amd64
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$JAVA_HOME</span>/bin:<span class="nv">$PATH</span>
</code></pre></div></div> <p><strong>For aarch64</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## make sure jdk8 is used</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/java-8-openjdk-arm64
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$JAVA_HOME</span>/bin:<span class="nv">$PATH</span>
</code></pre></div></div> <p><strong>Get gluten</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## config maven, like proxy in ~/.m2/settings.xml</span>

<span class="c">## fetch gluten code</span>
git clone https://github.com/apache/incubator-gluten.git
</code></pre></div></div> <h1 id="build-gluten-with-velox-backend"> <a href="#build-gluten-with-velox-backend" class="anchor-heading" aria-labelledby="build-gluten-with-velox-backend"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Build Gluten with Velox Backend </h1> <p>It’s recommended to use buildbundle-veloxbe.sh to build gluten in one script. <a href="/docs/getting-started/build-guide/">Gluten build guide</a> listed the parameters and their default value of build command for your reference.</p> <p><strong>For x86_64 build</strong></p> <p>First time build for all supported spark versions.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./dev/buildbundle-veloxbe.sh
</code></pre></div></div> <p>After a complete build, if only some gluten code is changed, you can use the following command to skip building velox/arrow and setting up build dependencies.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./dev/buildbundle-veloxbe.sh <span class="nt">--enable_ep_cache</span><span class="o">=</span>ON <span class="nt">--build_arrow</span><span class="o">=</span>OFF <span class="nt">--run_setup_script</span><span class="o">=</span>OFF
</code></pre></div></div> <p><strong>For aarch64 build:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">CPU_TARGET</span><span class="o">=</span><span class="s2">"aarch64"</span>

./dev/builddeps-veloxbe.sh
</code></pre></div></div> <p><strong>Build Velox separately</strong></p> <p>Currently, Gluten is using a <a href="https://github.com/oap-project/velox/">forked Velox</a> which is daily updated based on <a href="https://github.com/facebookincubator/velox">upstream Velox</a>.</p> <p>Scripts under <code class="language-plaintext highlighter-rouge">/path/to/gluten/ep/build-velox/src</code> provide <code class="language-plaintext highlighter-rouge">get_velox.sh</code> and <code class="language-plaintext highlighter-rouge">build_velox.sh</code> to build Velox separately, you could use these scripts with custom repo/branch/location.</p> <p>Velox provides arrow/parquet lib. Gluten cpp module need a required VELOX_HOME parsed by –velox_home, if you specify custom ep location, make sure these variables be passed correctly.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## fetch Velox and compile</span>
./dev/builddeps-veloxbe.sh build_velox

<span class="c">## compile Gluten cpp module</span>
./dev/builddeps-veloxbe.sh build_gluten_cpp

<span class="c">## compile Gluten java module and create package jar</span>
<span class="nb">cd</span> /path/to/gluten
<span class="c"># For spark3.2.x</span>
mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pceleborn</span> <span class="nt">-Puniffle</span> <span class="nt">-Pspark-3</span>.2 <span class="nt">-DskipTests</span>
<span class="c"># For spark3.3.x</span>
mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pceleborn</span> <span class="nt">-Puniffle</span> <span class="nt">-Pspark-3</span>.3 <span class="nt">-DskipTests</span>
<span class="c"># For spark3.4.x</span>
mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pceleborn</span> <span class="nt">-Puniffle</span> <span class="nt">-Pspark-3</span>.4 <span class="nt">-DskipTests</span>
<span class="c"># For spark3.5.x</span>
mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pceleborn</span> <span class="nt">-Puniffle</span> <span class="nt">-Pspark-3</span>.5 <span class="nt">-DskipTests</span>
</code></pre></div></div> <p>Notes： Building Velox may fail caused by <code class="language-plaintext highlighter-rouge">oom</code>. You can prevent this failure by adjusting <code class="language-plaintext highlighter-rouge">NUM_THREADS</code> (e.g., <code class="language-plaintext highlighter-rouge">export NUM_THREADS=4</code>) before building Gluten/Velox.</p> <p>Once building successfully, the Jar file will be generated in the directory: package/target/&lt;gluten-jar&gt; for Spark 3.2.x/Spark 3.3.x/Spark 3.4.x/Spark 3.5.x.</p> <h2 id="dependency-library-deployment"> <a href="#dependency-library-deployment" class="anchor-heading" aria-labelledby="dependency-library-deployment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Dependency library deployment </h2> <p>With config <code class="language-plaintext highlighter-rouge">enable_vcpkg=ON</code>, the dependency libraries will be built and statically linked into libvelox.so and libgluten.so, which is packed into the gluten-jar. In this way, only the gluten-jar is needed to add to <code class="language-plaintext highlighter-rouge">spark.&lt;driver|executor&gt;.extraClassPath</code> and spark will deploy the jar to each worker node. It’s better to build the static version using a clean docker image without any extra libraries installed. On host with some libraries like jemalloc installed, the script may crash with odd message. You may need to uninstall those libraries to get a clean host.</p> <p>With config <code class="language-plaintext highlighter-rouge">enable_vcpkg=OFF</code>, not all dependency libraries will be statically linked, instead the script will install the libraries to system then pack the dependency libraries into another jar named <code class="language-plaintext highlighter-rouge">gluten-package-${Maven-artifact-version}.jar</code>. Then you need to add the jar to <code class="language-plaintext highlighter-rouge">extraClassPath</code> and set <code class="language-plaintext highlighter-rouge">spark.gluten.loadLibFromJar=true</code>. Otherwise, you need to install shared dependency libraries on each worker node. You may find the libraries list from the gluten-package jar.</p> <h2 id="hdfs-support"> <a href="#hdfs-support" class="anchor-heading" aria-labelledby="hdfs-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> HDFS support </h2> <p>Hadoop hdfs support is ready via the <a href="https://github.com/apache/hawq/tree/master/depends/libhdfs3">libhdfs3</a> library. The libhdfs3 provides native API for Hadoop I/O without the drawbacks of JNI. It also provides advanced authentication like Kerberos based. Please note this library has several dependencies which may require extra installations on Driver and Worker node.</p> <h3 id="build-with-hdfs-support"> <a href="#build-with-hdfs-support" class="anchor-heading" aria-labelledby="build-with-hdfs-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Build with HDFS support </h3> <p>To build Gluten with HDFS support, below command is suggested:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/gluten
./dev/buildbundle-veloxbe.sh <span class="nt">--enable_hdfs</span><span class="o">=</span>ON
</code></pre></div></div> <h3 id="configuration-about-hdfs-support"> <a href="#configuration-about-hdfs-support" class="anchor-heading" aria-labelledby="configuration-about-hdfs-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Configuration about HDFS support </h3> <p>HDFS uris (hdfs://host:port) will be extracted from a valid hdfs file path to initialize hdfs client, you do not need to specify it explicitly.</p> <p>libhdfs3 need a configuration file and <a href="https://github.com/apache/hawq/blob/e9d43144f7e947e071bba48871af9da354d177d0/src/backend/utils/misc/etc/hdfs-client.xml">example here</a>, this file is a bit different from hdfs-site.xml and core-site.xml. Download that example config file to local and do some needed modifications to support HA or else, then set env variable like below to use it, or upload it to HDFS to use, more details <a href="https://github.com/apache/hawq/blob/e9d43144f7e947e071bba48871af9da354d177d0/depends/libhdfs3/src/client/Hdfs.cpp#L171-L189">here</a>.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Spark <span class="nb">local </span>mode
<span class="nb">export </span><span class="nv">LIBHDFS3_CONF</span><span class="o">=</span><span class="s2">"/path/to/hdfs-client.xml"</span>

// Spark Yarn cluster mode
<span class="nt">--conf</span> spark.executorEnv.LIBHDFS3_CONF<span class="o">=</span><span class="s2">"/path/to/hdfs-client.xml"</span>

// Spark Yarn cluster mode and upload hdfs config file
<span class="nb">cp</span> /path/to/hdfs-client.xml hdfs-client.xml
<span class="nt">--files</span> hdfs-client.xml
</code></pre></div></div> <p>One typical deployment on Spark/HDFS cluster is to enable <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">short-circuit reading</a>. Short-circuit reads provide a substantial performance boost to many applications.</p> <p>By default libhdfs3 does not set the default hdfs domain socket path to support HDFS short-circuit read. If this feature is required in HDFS setup, users may need to setup the domain socket path correctly by patching the libhdfs3 source code or by setting the correct config environment. In Gluten the short-circuit domain socket path is set to “/var/lib/hadoop-hdfs/dn_socket” in <a href="https://github.com/apache/incubator-gluten/blob/main/ep/build-velox/src/build_velox.sh">build_velox.sh</a> So we need to make sure the folder existed and user has write access as below script.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /var/lib/hadoop-hdfs/
<span class="nb">sudo chown</span> &lt;sparkuser&gt;:&lt;sparkuser&gt; /var/lib/hadoop-hdfs/
</code></pre></div></div> <p>You also need to add configuration to the “hdfs-site.xml” as below:</p> <div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.client.read.shortcircuit<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.domain.socket.path<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/var/lib/hadoop-hdfs/dn_socket<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div> <h3 id="kerberos-support"> <a href="#kerberos-support" class="anchor-heading" aria-labelledby="kerberos-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Kerberos support </h3> <p>Here are two steps to enable kerberos.</p> <ul> <li>Make sure the hdfs-client.xml contains</li> </ul> <div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>hadoop.security.authentication<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>kerberos<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div> <ul> <li>Specify the environment variable <a href="https://github.com/apache/hawq/blob/e9d43144f7e947e071bba48871af9da354d177d0/depends/libhdfs3/src/client/FileSystem.cpp#L56">KRB5CCNAME</a> and upload the kerberos ticket cache file</li> </ul> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">--conf</span> spark.executorEnv.KRB5CCNAME<span class="o">=</span>krb5cc_0000  <span class="nt">--files</span> /tmp/krb5cc_0000
</code></pre></div></div> <p>The ticket cache file can be found by <code class="language-plaintext highlighter-rouge">klist</code>.</p> <h2 id="azure-blob-file-system-abfs-support"> <a href="#azure-blob-file-system-abfs-support" class="anchor-heading" aria-labelledby="azure-blob-file-system-abfs-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Azure Blob File System (ABFS) support </h2> <p>Velox supports ABFS with the open source <a href="https://github.com/Azure/azure-sdk-for-cpp">Azure SDK for C++</a> and Gluten uses the Velox ABFS connector to connect with ABFS. The build option for ABFS (enable_abfs) must be set to enable this feature as listed below.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/gluten
./dev/buildbundle-veloxbe.sh <span class="nt">--enable_abfs</span><span class="o">=</span>ON
</code></pre></div></div> <p>Please refer <a href="/docs/getting-started/abfs/">Velox ABFS</a> part for more detailed configurations.</p> <h2 id="aws-s3-support"> <a href="#aws-s3-support" class="anchor-heading" aria-labelledby="aws-s3-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> AWS S3 support </h2> <p>Velox supports S3 with the open source <a href="https://github.com/aws/aws-sdk-cpp">AWS C++ SDK</a> and Gluten uses Velox S3 connector to connect with S3. A new build option for S3(enable_s3) is added. Below command is used to enable this feature</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/gluten
./dev/buildbundle-veloxbe.sh <span class="nt">--enable_s3</span><span class="o">=</span>ON
</code></pre></div></div> <p>Currently there are several ways to asscess S3 in Spark. Please refer <a href="/docs/getting-started/s3/">Velox S3</a> part for more detailed configurations</p> <h2 id="celeborn-support"> <a href="#celeborn-support" class="anchor-heading" aria-labelledby="celeborn-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Celeborn support </h2> <p>Gluten with velox backend supports <a href="https://github.com/apache/celeborn">Celeborn</a> as remote shuffle service. Currently, the supported Celeborn versions are <code class="language-plaintext highlighter-rouge">0.3.x</code>, <code class="language-plaintext highlighter-rouge">0.4.x</code> and <code class="language-plaintext highlighter-rouge">0.5.0</code>.</p> <p>Below introduction is used to enable this feature.</p> <p>First refer to this URL(https://github.com/apache/celeborn) to setup a celeborn cluster.</p> <p>When compiling the Gluten Java module, it’s required to enable <code class="language-plaintext highlighter-rouge">celeborn</code> profile, as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pspark-3</span>.3 <span class="nt">-Pceleborn</span> <span class="nt">-DskipTests</span>
</code></pre></div></div> <p>Then add the Gluten and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into <code class="language-plaintext highlighter-rouge">$SPARK_HOME/jars</code>).</p> <ul> <li>Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar</li> <li>Gluten: gluten-velox-bundle-spark3.x_2.12-xx_xx_xx-SNAPSHOT.jar, gluten-celeborn-package-xx-SNAPSHOT.jar</li> </ul> <p>Currently to use Gluten following configurations are required in <code class="language-plaintext highlighter-rouge">spark-defaults.conf</code></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager

<span class="c"># celeborn master</span>
spark.celeborn.master.endpoints clb-master:9097

spark.shuffle.service.enabled <span class="nb">false</span>

<span class="c"># options: hash, sort</span>
<span class="c"># Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory.</span>
<span class="c"># Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer.  </span>
spark.celeborn.client.spark.shuffle.writer <span class="nb">hash</span>

<span class="c"># We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication</span>
<span class="c"># If you have only one worker, this setting must be false </span>
<span class="c"># If your Celeborn is using HDFS, it's recommended to set this setting to false</span>
spark.celeborn.client.push.replicate.enabled <span class="nb">true</span>

<span class="c"># Support for Spark AQE only tested under Spark 3</span>
<span class="c"># we recommend setting localShuffleReader to false to get better performance of Celeborn</span>
spark.sql.adaptive.localShuffleReader.enabled <span class="nb">false</span>

<span class="c"># If Celeborn is using HDFS</span>
spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn

<span class="c"># If you want to use dynamic resource allocation,</span>
<span class="c"># please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark.</span>
spark.dynamicAllocation.enabled <span class="nb">false</span>
</code></pre></div></div> <h2 id="uniffle-support"> <a href="#uniffle-support" class="anchor-heading" aria-labelledby="uniffle-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Uniffle support </h2> <p>Uniffle with velox backend supports <a href="https://github.com/apache/incubator-uniffle">Uniffle</a> as remote shuffle service. Currently, the supported Uniffle versions are <code class="language-plaintext highlighter-rouge">0.8.0</code>.</p> <p>First refer to this URL(https://uniffle.apache.org/docs/intro) to get start with uniffle.</p> <p>When compiling the Gluten Java module, it’s required to enable <code class="language-plaintext highlighter-rouge">uniffle</code> profile, as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pspark-3</span>.3 <span class="nt">-Puniffle</span> <span class="nt">-DskipTests</span>
</code></pre></div></div> <p>Then add the Uniffle and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into <code class="language-plaintext highlighter-rouge">$SPARK_HOME/jars</code>).</p> <ul> <li>Uniffle: rss-client-spark3-shaded-[uniffleVersion].jar</li> <li>Gluten: gluten-uniffle-velox-xxx-SNAPSHOT-3.x.jar</li> </ul> <p>Currently to use Gluten following configurations are required in <code class="language-plaintext highlighter-rouge">spark-defaults.conf</code></p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark.shuffle.manager org.apache.spark.shuffle.gluten.uniffle.UniffleShuffleManager
<span class="c"># uniffle coordinator address</span>
spark.rss.coordinator.quorum ip:port
<span class="c"># Support for Spark AQE</span>
spark.sql.adaptive.localShuffleReader.enabled <span class="nb">false
</span>spark.shuffle.service.enabled <span class="nb">false</span>
<span class="c"># Uniffle support mutilple storage types, you can choose one of them.</span>
<span class="c"># Such as MEMORY,LOCALFILE,MEMORY_LOCALFILE,HDFS,MEMORY_HDFS,LOCALFILE_HDFS,MEMORY_LOCALFILE_HDFS</span>
spark.rss.storage.type LOCALFILE_HDFS
<span class="c"># If you want to use dynamic resource allocation,</span>
<span class="c"># please refer to this URL (https://github.com/apache/incubator-uniffle/tree/master/patch/spark) to apply the patch into your own Spark.</span>
spark.dynamicAllocation.enabled <span class="nb">false</span>
</code></pre></div></div> <h2 id="deltalake-support"> <a href="#deltalake-support" class="anchor-heading" aria-labelledby="deltalake-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> DeltaLake Support </h2> <p>Gluten with velox backend supports <a href="https://delta.io/">DeltaLake</a> table.</p> <h3 id="how-to-use"> <a href="#how-to-use" class="anchor-heading" aria-labelledby="how-to-use"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How to use </h3> <p>First of all, compile gluten-delta module by a <code class="language-plaintext highlighter-rouge">delta</code> profile, as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pspark-3</span>.3 <span class="nt">-Pdelta</span> <span class="nt">-DskipTests</span>
</code></pre></div></div> <p>Then, put the additional <code class="language-plaintext highlighter-rouge">gluten-delta-XX-SNAPSHOT.jar</code> to the class path (usually it’s <code class="language-plaintext highlighter-rouge">$SPARK_HOME/jars</code>). The gluten-delta jar is in <code class="language-plaintext highlighter-rouge">gluten-delta/target</code> directory.</p> <p>After the two steps, you can query delta table by gluten/velox without scan’s fallback.</p> <p>Gluten with velox backends also support the column mapping of delta tables. About column mapping, see more <a href="https://docs.delta.io/latest/delta-column-mapping.html">here</a>.</p> <h2 id="iceberg-support"> <a href="#iceberg-support" class="anchor-heading" aria-labelledby="iceberg-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Iceberg Support </h2> <p>Gluten with velox backend supports <a href="https://iceberg.apache.org/">Iceberg</a> table. Currently, only reading COW (Copy-On-Write) tables is supported.</p> <h3 id="how-to-use-1"> <a href="#how-to-use-1" class="anchor-heading" aria-labelledby="how-to-use-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How to use </h3> <p>First of all, compile gluten-iceberg module by a <code class="language-plaintext highlighter-rouge">iceberg</code> profile, as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mvn clean package <span class="nt">-Pbackends-velox</span> <span class="nt">-Pspark-3</span>.3 <span class="nt">-Piceberg</span> <span class="nt">-DskipTests</span>
</code></pre></div></div> <p>Once built successfully, iceberg features will be included in gluten-velox-bundle-X jar. Then you can query iceberg table by gluten/velox without scan’s fallback.</p> <p>After the two steps, you can query iceberg table by gluten/velox without scan’s fallback.</p> <h1 id="coverage"> <a href="#coverage" class="anchor-heading" aria-labelledby="coverage"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Coverage </h1> <p>Spark3.3 has 387 functions in total. ~240 are commonly used. To get the support status of all Spark built-in functions, please refer to <a href="../velox-backend-support-progress.md">Velox Backend’s Supported Operators &amp; Functions</a>.</p> <blockquote> <p>Velox doesn’t support <a href="https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html">ANSI mode</a>), so as Gluten. Once ANSI mode is enabled in Spark config, Gluten will fallback to Vanilla Spark.</p> </blockquote> <p>To identify what can be offloaded in a query and detailed fallback reasons, user can follow below steps to retrieve corresponding logs.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1<span class="o">)</span> Enable Gluten by proper <span class="o">[</span>configuration]<span class="o">(</span>https://github.com/apache/incubator-gluten/blob/main/docs/Configuration.md<span class="o">)</span><span class="nb">.</span>

2<span class="o">)</span> Disable Spark AQE to trigger plan validation <span class="k">in </span>Gluten
spark.sql.adaptive.enabled <span class="o">=</span> <span class="nb">false

</span>3<span class="o">)</span> Check physical plan 
sparkSession.sql<span class="o">(</span><span class="s2">"your_sql"</span><span class="o">)</span>.explain<span class="o">()</span>
</code></pre></div></div> <p>With above steps, you will get a physical plan output like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>== Physical Plan ==
-Execute InsertIntoHiveTable (7)
  +- Coalesce (6)
    +- VeloxColumnarToRowExec (5)
      +- ^ ProjectExecTransformer (3)
        +- GlutenRowToArrowColumnar (2)
          +- Scan hive default.extracted_db_pins (1)

</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">GlutenRowToArrowColumnar</code>/<code class="language-plaintext highlighter-rouge">VeloxColumnarToRowExec</code> indicates there is a fallback operator before or after it. And you may find fallback reason like below in logs.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>native validation failed due to: in ProjectRel, Scalar function name not registered: get_struct_field, called with arguments: (ROW&lt;col_0:INTEGER,col_1:BIGINT,col_2:BIGINT&gt;, INTEGER).
</code></pre></div></div> <p>In the above, the symbol <code class="language-plaintext highlighter-rouge">^</code> indicates a plan is offloaded to Velox in a stage. In Spark DAG, all such pipelined plans (consecutive plans marked with <code class="language-plaintext highlighter-rouge">^</code>) are plotted inside an umbrella node named <code class="language-plaintext highlighter-rouge">WholeStageCodegenTransformer</code> (It’s not codegen node. The naming is just for making it well plotted like Spark Whole Stage Codegen).</p> <h1 id="spill-experimental"> <a href="#spill-experimental" class="anchor-heading" aria-labelledby="spill-experimental"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Spill (Experimental) </h1> <p>Velox backend supports spilling-to-disk.</p> <p>Using the following configuration options to customize spilling:</p> <div class="table-wrapper"><table> <thead> <tr> <th>Name</th> <th>Default Value</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>spark.gluten.sql.columnar.backend.velox.spillStrategy</td> <td>auto</td> <td>none: Disable spill on Velox backend; auto: Let Spark memory manager manage Velox’s spilling</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.spillFileSystem</td> <td>local</td> <td>The filesystem used to store spill data. local: The local file system. heap-over-local: Write files to JVM heap if having extra heap space. Otherwise write to local file system.</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.aggregationSpillEnabled</td> <td>true</td> <td>Whether spill is enabled on aggregations</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.joinSpillEnabled</td> <td>true</td> <td>Whether spill is enabled on joins</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.orderBySpillEnabled</td> <td>true</td> <td>Whether spill is enabled on sorts</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.maxSpillLevel</td> <td>4</td> <td>The max allowed spilling level with zero being the initial spilling level</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.maxSpillFileSize</td> <td>1GB</td> <td>The max allowed spill file size. If it is zero, then there is no limit</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.spillStartPartitionBit</td> <td>29</td> <td>The start partition bit which is used with ‘spillPartitionBits’ together to calculate the spilling partition number</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.spillPartitionBits</td> <td>2</td> <td>The number of bits used to calculate the spilling partition number. The number of spilling partitions will be power of two</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.spillableReservationGrowthPct</td> <td>25</td> <td>The spillable memory reservation growth percentage of the previous memory reservation size</td> </tr> <tr> <td>spark.gluten.sql.columnar.backend.velox.spillThreadNum</td> <td>0</td> <td>(Experimental) The thread num of a dedicated thread pool to do spill</td> </tr> </tbody> </table></div> <h1 id="velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf"> <a href="#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf" class="anchor-heading" aria-labelledby="velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Velox User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF) </h1> <p>Please check the <a href="../developers/VeloxNativeUDF.md">VeloxNativeUDF.md</a> for more detailed usage and configurations.</p> <h1 id="high-bandwidth-memory-hbm-support"> <a href="#high-bandwidth-memory-hbm-support" class="anchor-heading" aria-labelledby="high-bandwidth-memory-hbm-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> High-Bandwidth Memory (HBM) support </h1> <p>Gluten supports allocating memory on HBM. This feature is optional and is disabled by default. It is implemented on top of <a href="http://memkind.github.io/memkind/">Memkind library</a>. You can refer to memkind’s <a href="https://github.com/memkind/memkind#memkind">readme</a> for more details.</p> <h2 id="build-gluten-with-hbm"> <a href="#build-gluten-with-hbm" class="anchor-heading" aria-labelledby="build-gluten-with-hbm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Build Gluten with HBM </h2> <p>Gluten will internally build and link to a specific version of Memkind library and <a href="https://github.com/open-mpi/hwloc">hwloc</a>. Other dependencies should be installed on Driver and Worker node first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> autoconf automake g++ libnuma-dev libtool numactl unzip libdaxctl-dev
</code></pre></div></div> <p>After the set-up, you can now build Gluten with HBM. Below command is used to enable this feature</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/gluten

<span class="c">## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1.</span>
./dev/buildbundle-veloxbe.sh <span class="nt">--enable_hbm</span><span class="o">=</span>ON
</code></pre></div></div> <h2 id="configure-and-enable-hbm-in-spark-application"> <a href="#configure-and-enable-hbm-in-spark-application" class="anchor-heading" aria-labelledby="configure-and-enable-hbm-in-spark-application"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Configure and enable HBM in Spark Application </h2> <p>At runtime, <code class="language-plaintext highlighter-rouge">MEMKIND_HBW_NODES</code> enviroment variable is detected for configuring HBM NUMA nodes. For the explaination to this variable, please refer to memkind’s manual page. This can be set for all executors through spark conf, e.g. <code class="language-plaintext highlighter-rouge">--conf spark.executorEnv.MEMKIND_HBW_NODES=8-15</code>. Note that memory allocation fallback is also supported and cannot be turned off. If HBM is unavailable or fills up, the allocator will use default(DDR) memory.</p> <h1 id="intel-quickassist-technology-qat-support"> <a href="#intel-quickassist-technology-qat-support" class="anchor-heading" aria-labelledby="intel-quickassist-technology-qat-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Intel® QuickAssist Technology (QAT) support </h1> <p>Gluten supports using Intel® QuickAssist Technology (QAT) for data compression during Spark Shuffle. It benefits from QAT Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission.</p> <p>This feature is based on QAT driver library and <a href="https://github.com/intel/QATzip">QATzip</a> library. Please manually download QAT driver for your system, and follow its README to build and install on all Driver and Worker node: <a href="https://www.intel.com/content/www/us/en/download/765501/intel-quickassist-technology-driver-for-linux-hw-version-2-0.html?wapkw=quickassist">Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0</a>.</p> <h2 id="software-requirements"> <a href="#software-requirements" class="anchor-heading" aria-labelledby="software-requirements"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Software Requirements </h2> <ul> <li>Download QAT driver for your system, and follow its README to build and install on all Driver and Worker nodes: <a href="https://www.intel.com/content/www/us/en/download/765501/intel-quickassist-technology-driver-for-linux-hw-version-2-0.html?wapkw=quickassist">Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0</a>.</li> <li>Below compression libraries need to be installed on all Driver and Worker nodes: <ul> <li>Zlib* library of version 1.2.7 or higher</li> <li>ZSTD* library of version 1.5.4 or higher</li> <li>LZ4* library</li> </ul> </li> </ul> <h2 id="build-gluten-with-qat"> <a href="#build-gluten-with-qat" class="anchor-heading" aria-labelledby="build-gluten-with-qat"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Build Gluten with QAT </h2> <ol> <li>Setup ICP_ROOT environment variable to the directory where QAT driver is extracted. This environment variable is required during building Gluten and running Spark applications. It’s recommended to put it in .bashrc on Driver and Worker nodes.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"export ICP_ROOT=/path/to/QAT_driver"</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">source</span> ~/.bashrc

<span class="c"># Also set for root if running as non-root user</span>
<span class="nb">sudo </span>su - 
<span class="nb">echo</span> <span class="s2">"export ICP_ROOT=/path/to/QAT_driver"</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">exit</span>
</code></pre></div></div> <ol> <li><strong>This step is required if your application is running as Non-root user.</strong> The users must be added to the ‘qat’ group after QAT drvier is installed. And change the amount of max locked memory for the username that is included in the group name. This can be done by specifying the limit in /etc/security/limits.conf.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>su -
usermod <span class="nt">-aG</span> qat username <span class="c"># need relogin to take effect</span>

<span class="c"># To set 500MB add a line like this in /etc/security/limits.conf</span>
<span class="nb">echo</span> <span class="s2">"@qat - memlock 500000"</span> <span class="o">&gt;&gt;</span> /etc/security/limits.conf

<span class="nb">exit</span>
</code></pre></div></div> <ol> <li>Enable huge page. This step is required to execute each time after system reboot. We recommend using systemctl to manage at system startup. You change the values for “max_huge_pages” and “max_huge_pages_per_process” to make sure there are enough resources for your workload. As for Spark applications, one process matches one executor. Within the executor, every task is allocated a maximum of 5 huge pages.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>su -

<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /usr/local/bin/qat_startup.sh
#!/bin/bash
echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
rmmod usdm_drv
insmod </span><span class="nv">$ICP_ROOT</span><span class="sh">/build/usdm_drv.ko max_huge_pages=1024 max_huge_pages_per_process=32
</span><span class="no">EOF

</span><span class="nb">chmod</span> +x /usr/local/bin/qat_startup.sh

<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt; /etc/systemd/system/qat_startup.service
[Unit]
Description=Configure QAT

[Service]
ExecStart=/usr/local/bin/qat_startup.sh

[Install]
WantedBy=multi-user.target
</span><span class="no">EOF

</span>systemctl <span class="nb">enable </span>qat_startup.service
systemctl start qat_startup.service <span class="c"># setup immediately</span>
systemctl status qat_startup.service

<span class="nb">exit</span>
</code></pre></div></div> <ol> <li>After the setup, you are now ready to build Gluten with QAT. Use the command below to enable this feature:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/gluten

<span class="c">## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1.</span>
./dev/buildbundle-veloxbe.sh <span class="nt">--enable_qat</span><span class="o">=</span>ON
</code></pre></div></div> <h2 id="enable-qat-with-gzipzstd-for-shuffle-compression"> <a href="#enable-qat-with-gzipzstd-for-shuffle-compression" class="anchor-heading" aria-labelledby="enable-qat-with-gzipzstd-for-shuffle-compression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Enable QAT with Gzip/Zstd for shuffle compression </h2> <ol> <li>To offload shuffle compression into QAT, first make sure you have the right QAT configuration file at /etc/4xxx_devX.conf. We provide a <a href="../qat/4x16.conf">example configuration file</a>. This configuration sets up to 4 processes that can bind to 1 QAT, and each process can use up to 16 QAT DC instances.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## run as root</span>
<span class="c">## Overwrite QAT configuration file.</span>
<span class="nb">cd</span> /etc
<span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>0..7<span class="o">}</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="s2">"4xxx_dev</span><span class="nv">$i</span><span class="s2">.conf"</span><span class="p">;</span> <span class="k">done</span> | xargs <span class="nt">-i</span> <span class="nb">cp</span> <span class="nt">-f</span> /path/to/gluten/docs/qat/4x16.conf <span class="o">{}</span>
<span class="c">## Restart QAT after updating configuration files.</span>
adf_ctl restart
</code></pre></div></div> <ol> <li>Check QAT status and make sure the status is up</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>adf_ctl status
</code></pre></div></div> <p>The output should be like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Checking status of all devices.
There is 8 QAT acceleration device(s) in the system:
 qat_dev0 - type: 4xxx,  inst_id: 0,  node_id: 0,  bsf: 0000:6b:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev1 - type: 4xxx,  inst_id: 1,  node_id: 1,  bsf: 0000:70:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev2 - type: 4xxx,  inst_id: 2,  node_id: 2,  bsf: 0000:75:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev3 - type: 4xxx,  inst_id: 3,  node_id: 3,  bsf: 0000:7a:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev4 - type: 4xxx,  inst_id: 4,  node_id: 4,  bsf: 0000:e8:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev5 - type: 4xxx,  inst_id: 5,  node_id: 5,  bsf: 0000:ed:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev6 - type: 4xxx,  inst_id: 6,  node_id: 6,  bsf: 0000:f2:00.0,  #accel: 1 #engines: 9 state: up
 qat_dev7 - type: 4xxx,  inst_id: 7,  node_id: 7,  bsf: 0000:f7:00.0,  #accel: 1 #engines: 9 state: up
</code></pre></div></div> <ol> <li>Extra Gluten configurations are required when starting Spark application</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--conf spark.gluten.sql.columnar.shuffle.codec=gzip # Valid options are gzip and zstd
--conf spark.gluten.sql.columnar.shuffle.codecBackend=qat
</code></pre></div></div> <ol> <li>You can use below command to check whether QAT is working normally at run-time. The value of fw_counters should continue to increase during shuffle.</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>while :; do cat /sys/kernel/debug/qat_4xxx_0000:6b:00.0/fw_counters; sleep 1; done
</code></pre></div></div> <h2 id="qat-driver-references"> <a href="#qat-driver-references" class="anchor-heading" aria-labelledby="qat-driver-references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> QAT driver references </h2> <p><strong>Documentation</strong></p> <p><a href="https://downloadmirror.intel.com/765523/README_QAT20.L.1.0.0-00021.txt">README Text Files (README_QAT20.L.1.0.0-00021.txt)</a></p> <p><strong>Release Notes</strong></p> <p>Check out the <a href="https://www.intel.com/content/www/us/en/content-details/632507/intel-quickassist-technology-intel-qat-software-for-linux-release-notes-hardware-version-2-0.html">Intel® QuickAssist Technology Software for Linux*</a> - Release Notes for the latest changes in this release.</p> <p><strong>Getting Started Guide</strong></p> <p>Check out the <a href="https://www.intel.com/content/www/us/en/content-details/632506/intel-quickassist-technology-intel-qat-software-for-linux-getting-started-guide-hardware-version-2-0.html">Intel® QuickAssist Technology Software for Linux*</a> - Getting Started Guide for detailed installation instructions.</p> <p><strong>Programmer’s Guide</strong></p> <p>Check out the <a href="https://www.intel.com/content/www/us/en/content-details/743912/intel-quickassist-technology-intel-qat-software-for-linux-programmers-guide-hardware-version-2-0.html">Intel® QuickAssist Technology Software for Linux*</a> - Programmer’s Guide for software usage guidelines.</p> <p>For more Intel® QuickAssist Technology resources go to <a href="https://developer.intel.com/quickassist">Intel® QuickAssist Technology (Intel® QAT)</a></p> <h1 id="intel-in-memory-analytics-accelerator-iaaiax-support"> <a href="#intel-in-memory-analytics-accelerator-iaaiax-support" class="anchor-heading" aria-labelledby="intel-in-memory-analytics-accelerator-iaaiax-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Intel® In-memory Analytics Accelerator (IAA/IAX) support </h1> <p>Similar to Intel® QAT, Gluten supports using Intel® In-memory Analytics Accelerator (IAA, also called IAX) for data compression during Spark Shuffle. It benefits from IAA Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission.</p> <p>This feature is based on Intel® <a href="https://github.com/intel/qpl">QPL</a>.</p> <h2 id="build-gluten-with-iaa"> <a href="#build-gluten-with-iaa" class="anchor-heading" aria-labelledby="build-gluten-with-iaa"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Build Gluten with IAA </h2> <p>Gluten will internally build and link to a specific version of QPL library, but extra environment setup is still required. Please refer to <a href="https://intel.github.io/qpl/documentation/get_started_docs/installation.html">QPL Installation Guide</a> to install dependencies and configure accelerators.</p> <p><strong>This step is required if your application is running as Non-root user.</strong> Create a group for the users who have privilege to use IAA, and grant group iaa read/write access to the IAA Work-Queues.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>groupadd iaa
<span class="nb">sudo </span>usermod <span class="nt">-aG</span> iaa username <span class="c"># need to relogin</span>
<span class="nb">sudo chgrp</span> <span class="nt">-R</span> iaa /dev/iax
<span class="nb">sudo chmod</span> <span class="nt">-R</span> g+rw /dev/iax
</code></pre></div></div> <p>After the set-up, you can now build Gluten with QAT. Below command is used to enable this feature</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /path/to/gluten

<span class="c">## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1.</span>
./dev/buildbundle-veloxbe.sh <span class="nt">--enable_iaa</span><span class="o">=</span>ON
</code></pre></div></div> <h2 id="enable-iaa-with-gzip-compression-for-shuffle-compression"> <a href="#enable-iaa-with-gzip-compression-for-shuffle-compression" class="anchor-heading" aria-labelledby="enable-iaa-with-gzip-compression-for-shuffle-compression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Enable IAA with Gzip Compression for shuffle compression </h2> <ol> <li>To enable QAT at run-time, first make sure you have configured the IAA Work-Queues correctly, and the file permissions of /dev/iax/wqX.0 are correct.</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo ls</span> <span class="nt">-l</span> /dev/iax
</code></pre></div></div> <p>The output should be like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>total 0
crw-rw---- 1 root iaa 509, 0 Apr  5 18:54 wq1.0
crw-rw---- 1 root iaa 509, 5 Apr  5 18:54 wq11.0
crw-rw---- 1 root iaa 509, 6 Apr  5 18:54 wq13.0
crw-rw---- 1 root iaa 509, 7 Apr  5 18:54 wq15.0
crw-rw---- 1 root iaa 509, 1 Apr  5 18:54 wq3.0
crw-rw---- 1 root iaa 509, 2 Apr  5 18:54 wq5.0
crw-rw---- 1 root iaa 509, 3 Apr  5 18:54 wq7.0
crw-rw---- 1 root iaa 509, 4 Apr  5 18:54 wq9.0
</code></pre></div></div> <ol> <li>Extra Gluten configurations are required when starting Spark application</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>--conf spark.gluten.sql.columnar.shuffle.codec=gzip
--conf spark.gluten.sql.columnar.shuffle.codecBackend=iaa
</code></pre></div></div> <h2 id="iaa-references"> <a href="#iaa-references" class="anchor-heading" aria-labelledby="iaa-references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> IAA references </h2> <p><strong>Intel® IAA Enabling Guide</strong></p> <p>Check out the <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-iaa-enabling-guide.html">Intel® In-Memory Analytics Accelerator (Intel® IAA) Enabling Guide</a></p> <p><strong>Intel® QPL Documentation</strong></p> <p>Check out the <a href="https://intel.github.io/qpl/index.html">Intel® Query Processing Library (Intel® QPL) Documentation</a></p> <h1 id="test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend"> <a href="#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend" class="anchor-heading" aria-labelledby="test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Test TPC-H or TPC-DS on Gluten with Velox backend </h1> <p>All TPC-H and TPC-DS queries are supported in Gluten Velox backend.</p> <h2 id="data-preparation"> <a href="#data-preparation" class="anchor-heading" aria-labelledby="data-preparation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Data preparation </h2> <p>The data generation scripts are <a href="../../tools/workload/tpch/gen_data/parquet_dataset/tpch_datagen_parquet.sh">TPC-H dategen script</a> and <a href="../../tools/workload/tpcds/gen_data/parquet_dataset/tpcds_datagen_parquet.sh">TPC-DS dategen script</a>.</p> <p>The used TPC-H and TPC-DS queries are the original ones, and can be accessed from <a href="../../gluten-core/src/test/resources/tpcds-queries/tpcds.queries.original">TPC-DS queries</a> and <a href="../../gluten-core/src/test/resources/tpch-queries">TPC-H queries</a>.</p> <p>Some other versions of TPC-DS queries are also provided, but are <strong>not</strong> recommended for testing, including:</p> <ul> <li>the modified TPC-DS queries with “Decimal-to-Double”: <a href="../../gluten-core/src/test/resources/tpcds-queries/tpcds.queries.no-decimal">TPC-DS non-decimal queries</a> (outdated).</li> </ul> <h2 id="submit-the-spark-sql-job"> <a href="#submit-the-spark-sql-job" class="anchor-heading" aria-labelledby="submit-the-spark-sql-job"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Submit the Spark SQL job </h2> <p>Submit test script from spark-shell. You can find the scala code to <a href="../../tools/workload/tpch/run_tpch/tpch_parquet.scala">Run TPC-H</a> as an example. Please remember to modify the location of TPC-H files as well as TPC-H queries before you run the testing.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var parquet_file_path = "/PATH/TO/TPCH_PARQUET_PATH"
var gluten_root = "/PATH/TO/GLUTEN"
</code></pre></div></div> <p>Below script shows an example about how to run the testing, you should modify the parameters such as executor cores, memory, offHeap size based on your environment.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span>GLUTEN_JAR <span class="o">=</span> /PATH/TO/GLUTEN/package/target/&lt;gluten-jar&gt;
<span class="nb">cat </span>tpch_parquet.scala | spark-shell <span class="nt">--name</span> tpch_powertest_velox <span class="se">\</span>
  <span class="nt">--master</span> yarn <span class="nt">--deploy-mode</span> client <span class="se">\</span>
  <span class="nt">--conf</span> spark.plugins<span class="o">=</span>org.apache.gluten.GlutenPlugin <span class="se">\</span>
  <span class="nt">--conf</span> spark.driver.extraClassPath<span class="o">=</span><span class="k">${</span><span class="nv">GLUTEN_JAR</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.executor.extraClassPath<span class="o">=</span><span class="k">${</span><span class="nv">GLUTEN_JAR</span><span class="k">}</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.memory.offHeap.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.memory.offHeap.size<span class="o">=</span>20g <span class="se">\</span>
  <span class="nt">--conf</span> spark.gluten.sql.columnar.forceShuffledHashJoin<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
  <span class="nt">--conf</span> spark.shuffle.manager<span class="o">=</span>org.apache.spark.shuffle.sort.ColumnarShuffleManager <span class="se">\</span>
  <span class="nt">--num-executors</span> 6 <span class="se">\</span>
  <span class="nt">--executor-cores</span> 6 <span class="se">\</span>
  <span class="nt">--driver-memory</span> 20g <span class="se">\</span>
  <span class="nt">--executor-memory</span> 25g <span class="se">\</span>
  <span class="nt">--conf</span> spark.executor.memoryOverhead<span class="o">=</span>5g <span class="se">\</span>
  <span class="nt">--conf</span> spark.driver.maxResultSize<span class="o">=</span>32g
</code></pre></div></div> <p>Refer to <a href="/docs/configuration/">Gluten configuration</a> for more details.</p> <h2 id="result"> <a href="#result" class="anchor-heading" aria-labelledby="result"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Result </h2> <p><em>wholestagetransformer</em> indicates that the offloading works.</p> <p><img src="../image/TPC-H_Q6_DAG.png" alt="TPC-H Q6" /></p> <h2 id="performance"> <a href="#performance" class="anchor-heading" aria-labelledby="performance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Performance </h2> <p>Below table shows the TPC-H Q1 and Q6 Performance in a multiple-thread test (–num-executors 6 –executor-cores 6) for Velox and vanilla Spark. Both Parquet and ORC datasets are sf1024.</p> <div class="table-wrapper"><table> <thead> <tr> <th>Query Performance (s)</th> <th>Velox (ORC)</th> <th>Vanilla Spark (Parquet)</th> <th>Vanilla Spark (ORC)</th> </tr> </thead> <tbody> <tr> <td>TPC-H Q6</td> <td>13.6</td> <td>21.6</td> <td>34.9</td> </tr> <tr> <td>TPC-H Q1</td> <td>26.1</td> <td>76.7</td> <td>84.9</td> </tr> </tbody> </table></div> <h1 id="external-reference-setup"> <a href="#external-reference-setup" class="anchor-heading" aria-labelledby="external-reference-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> External reference setup </h1> <p>TO ease your first-hand experience of using Gluten, we have set up an external reference cluster. If you are interested, please contact Weiting.Chen@intel.com.</p> <h1 id="gluten-ui"> <a href="#gluten-ui" class="anchor-heading" aria-labelledby="gluten-ui"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gluten UI </h1> <h2 id="gluten-event"> <a href="#gluten-event" class="anchor-heading" aria-labelledby="gluten-event"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gluten event </h2> <p>Gluten provides two events <code class="language-plaintext highlighter-rouge">GlutenBuildInfoEvent</code> and <code class="language-plaintext highlighter-rouge">GlutenPlanFallbackEvent</code>:</p> <ul> <li> <p>GlutenBuildInfoEvent, it contains the Gluten build information so that we are able to be aware of the environment when doing some debug. It includes <code class="language-plaintext highlighter-rouge">Java Version</code>, <code class="language-plaintext highlighter-rouge">Scala Version</code>, <code class="language-plaintext highlighter-rouge">GCC Version</code>, <code class="language-plaintext highlighter-rouge">Gluten Version</code>, <code class="language-plaintext highlighter-rouge">Spark Version</code>, <code class="language-plaintext highlighter-rouge">Hadoop Version</code>, <code class="language-plaintext highlighter-rouge">Gluten Revision</code>, <code class="language-plaintext highlighter-rouge">Backend</code>, <code class="language-plaintext highlighter-rouge">Backend Revision</code>, etc.</p> </li> <li> <p>GlutenPlanFallbackEvent, it contains the fallback information for each query execution. Note, if the query execution is in AQE, then Gluten will post it for each stage.</p> </li> </ul> <p>Developers can register <code class="language-plaintext highlighter-rouge">SparkListener</code> to handle these two Gluten events.</p> <h2 id="sql-tab"> <a href="#sql-tab" class="anchor-heading" aria-labelledby="sql-tab"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> SQL tab </h2> <p>Gluten provides a tab based on Spark UI, named <code class="language-plaintext highlighter-rouge">Gluten SQL / DataFrame</code></p> <p><img src="../image/gluten-ui.png" alt="Gluten-UI" /></p> <p>This tab contains two parts:</p> <ol> <li>The Gluten build information.</li> <li>SQL/Dataframe queries fallback information.</li> </ol> <p>If you want to disable Gluten UI, add a config when submitting <code class="language-plaintext highlighter-rouge">--conf spark.gluten.ui.enabled=false</code>.</p> <h2 id="history-server"> <a href="#history-server" class="anchor-heading" aria-labelledby="history-server"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> History server </h2> <p>Gluten UI also supports Spark history server. Add gluten-ui jar into the history server classpath, e.g., $SPARK_HOME/jars, then restart history server.</p> <h2 id="native-plan-string"> <a href="#native-plan-string" class="anchor-heading" aria-labelledby="native-plan-string"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Native plan string </h2> <p>Gluten supports inject native plan string into Spark explain with formatted mode by setting <code class="language-plaintext highlighter-rouge">--conf spark.gluten.sql.injectNativePlanStringToExplain=true</code>. Here is an example, how Gluten show the native plan string.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(9) WholeStageCodegenTransformer (2)
Input [6]: [c1#0L, c2#1L, c3#2L, c1#3L, c2#4L, c3#5L]
Arguments: false
Native Plan:
-- Project[expressions: (n3_6:BIGINT, "n0_0"), (n3_7:BIGINT, "n0_1"), (n3_8:BIGINT, "n0_2"), (n3_9:BIGINT, "n1_0"), (n3_10:BIGINT, "n1_1"), (n3_11:BIGINT, "n1_2")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT
  -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT
    -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT
    -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT
</code></pre></div></div> <h2 id="native-plan-with-stats"> <a href="#native-plan-with-stats" class="anchor-heading" aria-labelledby="native-plan-with-stats"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Native plan with stats </h2> <p>Gluten supports print native plan with stats to executor system output stream by setting <code class="language-plaintext highlighter-rouge">--conf spark.gluten.sql.debug=true</code>. Note that, the plan string with stats is task level which may cause executor log size big. Here is an example, how Gluten show the native plan string with stats.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I20231121 10:19:42.348845 90094332 WholeStageResultIterator.cc:220] Native Plan with stats for: [Stage: 1 TID: 16]
-- Project[expressions: (n3_6:BIGINT, "n0_0"), (n3_7:BIGINT, "n0_1"), (n3_8:BIGINT, "n0_2"), (n3_9:BIGINT, "n1_0"), (n3_10:BIGINT, "n1_1"), (n3_11:BIGINT, "n1_2")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT
   Output: 27 rows (3.56KB, 3 batches), Cpu time: 10.58us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1
      queuedWallNanos              sum: 2.00us, count: 1, min: 2.00us, max: 2.00us
      runningAddInputWallNanos     sum: 626ns, count: 1, min: 626ns, max: 626ns
      runningFinishWallNanos       sum: 0ns, count: 1, min: 0ns, max: 0ns
      runningGetOutputWallNanos    sum: 5.54us, count: 1, min: 5.54us, max: 5.54us
  -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT
     Output: 27 rows (3.56KB, 3 batches), Cpu time: 223.00us, Blocked wall time: 0ns, Peak memory: 93.12KB, Memory allocations: 15
     HashBuild: Input: 10 rows (960B, 10 batches), Output: 0 rows (0B, 0 batches), Cpu time: 185.67us, Blocked wall time: 0ns, Peak memory: 68.00KB, Memory allocations: 2, Threads: 1
        distinctKey0                 sum: 4, count: 1, min: 4, max: 4
        hashtable.capacity           sum: 4, count: 1, min: 4, max: 4
        hashtable.numDistinct        sum: 10, count: 1, min: 10, max: 10
        hashtable.numRehashes        sum: 1, count: 1, min: 1, max: 1
        queuedWallNanos              sum: 0ns, count: 1, min: 0ns, max: 0ns
        rangeKey0                    sum: 4, count: 1, min: 4, max: 4
        runningAddInputWallNanos     sum: 1.27ms, count: 1, min: 1.27ms, max: 1.27ms
        runningFinishWallNanos       sum: 0ns, count: 1, min: 0ns, max: 0ns
        runningGetOutputWallNanos    sum: 1.29us, count: 1, min: 1.29us, max: 1.29us
     H23/11/21 10:19:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 13) in 335 ms on 10.221.97.35 (executor driver) (1/10)
ashProbe: Input: 9 rows (864B, 3 batches), Output: 27 rows (3.56KB, 3 batches), Cpu time: 37.33us, Blocked wall time: 0ns, Peak memory: 25.12KB, Memory allocations: 13, Threads: 1
        dynamicFiltersProduced       sum: 1, count: 1, min: 1, max: 1
        queuedWallNanos              sum: 0ns, count: 1, min: 0ns, max: 0ns
        runningAddInputWallNanos     sum: 4.54us, count: 1, min: 4.54us, max: 4.54us
        runningFinishWallNanos       sum: 83ns, count: 1, min: 83ns, max: 83ns
        runningGetOutputWallNanos    sum: 29.08us, count: 1, min: 29.08us, max: 29.08us
    -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT
       Input: 9 rows (864B, 3 batches), Output: 9 rows (864B, 3 batches), Cpu time: 630.75us, Blocked wall time: 0ns, Peak memory: 2.44KB, Memory allocations: 63, Threads: 1, Splits: 3
          dataSourceWallNanos              sum: 102.00us, count: 1, min: 102.00us, max: 102.00us
          dynamicFiltersAccepted           sum: 1, count: 1, min: 1, max: 1
          flattenStringDictionaryValues    sum: 0, count: 1, min: 0, max: 0
          ioWaitNanos                      sum: 312.00us, count: 1, min: 312.00us, max: 312.00us
          localReadBytes                   sum: 0B, count: 1, min: 0B, max: 0B
          numLocalRead                     sum: 0, count: 1, min: 0, max: 0
          numPrefetch                      sum: 0, count: 1, min: 0, max: 0
          numRamRead                       sum: 0, count: 1, min: 0, max: 0
          numStorageRead                   sum: 6, count: 1, min: 6, max: 6
          overreadBytes                    sum: 0B, count: 1, min: 0B, max: 0B
          prefetchBytes                    sum: 0B, count: 1, min: 0B, max: 0B
          queryThreadIoLatency             sum: 12, count: 1, min: 12, max: 12
          ramReadBytes                     sum: 0B, count: 1, min: 0B, max: 0B
          runningAddInputWallNanos         sum: 0ns, count: 1, min: 0ns, max: 0ns
          runningFinishWallNanos           sum: 125ns, count: 1, min: 125ns, max: 125ns
          runningGetOutputWallNanos        sum: 1.07ms, count: 1, min: 1.07ms, max: 1.07ms
          skippedSplitBytes                sum: 0B, count: 1, min: 0B, max: 0B
          skippedSplits                    sum: 0, count: 1, min: 0, max: 0
          skippedStrides                   sum: 0, count: 1, min: 0, max: 0
          storageReadBytes                 sum: 3.44KB, count: 1, min: 3.44KB, max: 3.44KB
          totalScanTime                    sum: 0ns, count: 1, min: 0ns, max: 0ns
    -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT
       Input: 0 rows (0B, 0 batches), Output: 10 rows (960B, 10 batches), Cpu time: 1.03ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1
          runningAddInputWallNanos     sum: 0ns, count: 1, min: 0ns, max: 0ns
          runningFinishWallNanos       sum: 54.62us, count: 1, min: 54.62us, max: 54.62us
          runningGetOutputWallNanos    sum: 1.10ms, count: 1, min: 1.10ms, max: 1.10ms
</code></pre></div></div> <h1 id="gluten-implicits"> <a href="#gluten-implicits" class="anchor-heading" aria-labelledby="gluten-implicits"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gluten Implicits </h1> <p>Gluten provides a helper class to get the fallback summary from a Spark Dataset.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import org.apache.spark.sql.execution.GlutenImplicits._
val df = spark.sql("SELECT * FROM t")
df.fallbackSummary
</code></pre></div></div> <p>Note that, if AQE is enabled, but the query is not materialized, then it will re-plan the query execution with disabled AQE. It is a workaround to get the final plan, and it may cause the inconsistent results with a materialized query. However, we have no choice.</p> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <a href="https://incubator.apache.org/">Apache Incubator</a> <p class="text-small text-grey-dk-100 mb-0">Copyright © 2024 The Apache Software Foundation, Licensed under the Apache License, Version 2.0. Apache Gluten, Gluten, Apache, the Apache feather logo, and the Apache Gluten project logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.</p> <p class="text-small text-grey-dk-100 mb-0">Apache Gluten is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.</p> <p class="text-small text-grey-dk-100 mb-0"><a href="https://privacy.apache.org/policies/privacy-policy-public.html">Privacy Policy</a></p> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.9.1/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
