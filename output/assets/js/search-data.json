{"0": {
    "doc": "About",
    "title": "About",
    "content": "This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com . You can find the source code for Minima at GitHub: jekyll / minima . You can find the source code for Jekyll at GitHub: jekyll / jekyll . ",
    "url": "/about/",
    
    "relUrl": "/about/"
  },"1": {
    "doc": "Blog",
    "title": "Latest Posts",
    "content": ". | ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"2": {
    "doc": "Blog",
    "title": "Gluten 1.2.0 Release",
    "content": "On Sept. 3rd 2024, I am thrilled to share that Apache Gluten (incubating) has celebrated its 1st Apache official release, version 1.2.0, featuring the following highlights. | ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"3": {
    "doc": "Blog",
    "title": "Welcome to Jekyll!",
    "content": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. | . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"4": {
    "doc": "Blog",
    "title": "Blog",
    "content": " ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"5": {
    "doc": "Community",
    "title": "Community",
    "content": " ",
    "url": "/community/",
    
    "relUrl": "/community/"
  },"6": {
    "doc": "Community",
    "title": "Team",
    "content": "The project is mentored by several mentors during its Apache incubation process and is governed by a Podling Project Management Committee (PPMC). The project’s committers also have write access to the codebase. Mentors . | Name | Apache ID | GitHub ID | . | Felix Cheung | felixcheung | felixcheung | . | Kent Yao | yao | yaooqinn | . | Shao Feng Shi | shaofengshi | shaofengshi | . | Wenli Zhang | ovilia | Ovilia | . | Yu Li | liyu | carp84 | . PPMC Members . | Name | Apache ID | GitHub ID | . | Binwei Yang | felixybw | FelixYBW | . | Chang Chen | changchen | baibaichen | . | Hongbin Ma | mahongbin | binmahone | . | Hongze Zhang | hongze | zhztheplayer | . | Junqing Li | jackylee | jackylee-ch | . | Ke Jia | kejia | JkSelf | . | Keyong Zhou | zhouky | waitinfuture | . | Liang JiaBiao | lgbo | lgbo-ustc | . | Neng Liu | liuneng | liuneng1994 | . | Philo He | philo | PHILO-HE | . | Rong Ma | marong | marin-ma | . | Rui Mo | rui | rui-mo | . | Shuai Li | loneylee | loneylee | . | Taiyang Li | taiyangli | taiyang-li | . | Weiting Chen | weitingchen | weiting-chen | . | XiDuo You | ulyssesyou | ulysses-you | . | Yang Zhang | yangzy | Yohahaha | . | Yuan Zhou | yuanzhou | zhouyuan | . | Zhao Kuo | zhaokuo | kecookier | . | Zhen Li | zhli | zhli1142015 | . | ZhiBiao Zhang | zhanglistar | zhanglistar | . | Zhichao Zhang | zhangzc | zzcclp | . | Zuo Chunwei | zuochunwei | zuochunwei | . Committers . | Name | Apache ID | GitHub ID | . | Arnav Balyan | arnavbalyan | ArnavBalyan | . | Chengcheng Jin | chengchengjin | jinchengchenghh | . | Guangxin Wang | wangguangxin | WangGuangxin | . | Jiayi Liu | liujiayi771 | liujiayi771 | . | Kerwin Zhang | kerwinzhang | kerwin-zk | . | Mingliang Zhu | mingliang | zml1206 | . | Mingyong Xu | exmy | exmy | . | Qian Sun | qiansun | dcoliversun | . | Shuai Xu | shuaixu | shuai-xu | . | Wenzheng Liu | lwz9103 | lwz9103 | . | Yan Ma | yma | yma11 | . | Yangyang Gao | gaoyangxiaozhu | gaoyangxiaozhu | . | Yunhe Zou | kevinyhzou | KevinyhZou | . | Zhen Wang | wangzhen | wForget | . ",
    "url": "/community/#team",
    
    "relUrl": "/community/#team"
  },"7": {
    "doc": "Community",
    "title": "Contact Us",
    "content": "This plugin is still under active development now, and doesn’t have a stable release. Welcome to evaluate it. If you encounter any issues or have any suggestions, please submit to our issue list. We’d love to hear your feedback . Apache has meticulously set up dedicated mailing lists for every project, underscoring the pivotal role of these platforms in fostering communication within the Apache community. Mailing lists serve as the backbone for numerous aspects of community operation and maintenance. They facilitate technical discussions, idea sharing, project Q&amp;A, decision-making on new features or changes, release voting, and more. Any topic relevant to the project can be discussed, making mailing lists the primary platform for community engagement and collaboration. By subscribing to this mailing list, you’ll receive timely updates on the latest Linkis community developments, enabling you to stay ahead and engaged with the community’s progress. | Mailing List | Description | Subscribe | Unsubscribe | . | dev@gluten.apache.org | Community Activity | subscribe | unsubscribe | . | commits@gluten.apache.org | Code Repository Activity | subscribe | unsubscribe | . How to Subscribe to the Mailing Lists . To subscribe to the dev@gluten.apache.org mailing list as an example, please follow these steps: . | Send a blank email to dev-subscribe@gluten.apache.org | Check your inbox for an email titled confirm subscribe to dev@gluten.apache.org If not received within a reasonable time, ensure it’s not blocked by your email provider. If not blocked and no reply is received after a while, repeat step 1 | Reply directly to the confirmation email without altering the subject or adding content. | Await an email titled WELCOME to dev@linkis.apache.org | Upon receiving the welcome email, you’re successfully subscribed. To engage in discussions, email dev@gluten.apache.org, which will reach all subscribers. | . To unsubscribe to the dev@gluten.apache.org mailing list as an example, please follow these steps: . | Send a blank email to dev-unsubscribe@gluten.apache.org | Check your inbox for an email titled confirm unsubscribe from dev@gluten.apache.org | Reply directly to the confirmation email without altering the subject or adding content. | Await an email titled GOODBYE from dev@gluten.apache.org | Unsubsscribe success | . Issues and Discussions . We use github to track bugs, feature requests, and answer questions. File an issue for a bug or feature request. ",
    "url": "/community/#contact-us",
    
    "relUrl": "/community/#contact-us"
  },"8": {
    "doc": "Contributing to Gluten",
    "title": "How to become a committer",
    "content": "To initiate your contributions to Gluten, understand the contribution process—any individual can submit patches, documentation, and examples to the project. The PMC routinely incorporates fresh committers from the pool of active contributors, assessing their contributions to Gluten. The criteria for new committers encompass: . | The Scope of Contribution: To expand contributions beyond pull requests, individuals can engage in various activities including reviewing other pull requests, addressing bugs and documentation, triaging issues, responding to community inquiries, enhancing usability, minimizing technical debt, supporting continuous integration (CI), validating releases, troubleshooting in unique environments, and more. | Sustaining High-Quality Contributions: Committers must demonstrate an ongoing commitment to Gluten through substantial contributions across diverse project areas, including assuming leadership in at least one critical component. These contributions should uphold consistent quality standards and endure over an extended period, typically follow below rules: | 3+ months with light activity and involvement | 2+ months with medium activity and involvement | 1+ month with solid activity and involvement . | Cultivating Community Growth: This statement emphasizes the importance of active engagement and positive behavior within the community by committers. It highlights the role of committers in fostering a supportive environment, providing guidance to newcomers, and contributing to consensus-driven discussions, all essential elements of a thriving community. The reference to the Code of Conduct and the Apache Way underscores the commitment to shared values and collaborative decision-making processes | . The PMC of Apache projects appoints new members who must fulfill the PMC responsibilities outlined in Apache Guidance. These duties encompass voting on releases, upholding Apache project trademarks, handling legal and licensing matters, and ensuring compliance with project mechanics. Additionally, the PMC selectively includes committers who demonstrate comprehension and aid in executing these tasks. you can reach out to one of the PMC members or the dev@gluten.apache.org mailing list. ",
    "url": "/contributing/#how-to-become-a-committer",
    
    "relUrl": "/contributing/#how-to-become-a-committer"
  },"9": {
    "doc": "Contributing to Gluten",
    "title": "Contributing to Gluten",
    "content": " ",
    "url": "/contributing/",
    
    "relUrl": "/contributing/"
  },"10": {
    "doc": "Downloads",
    "title": "Download Apache Gluten (incubating)",
    "content": "Gluten is a plugin for Apache Spark to double SparkSQL’s performance. ",
    "url": "/downloads/#download-apache-gluten-incubating",
    
    "relUrl": "/downloads/#download-apache-gluten-incubating"
  },"11": {
    "doc": "Downloads",
    "title": "Instructions",
    "content": "Please select a version of Gluten to download from the list below; we recommend opting for the latest release. Please ensure the integrity of the release by verifying the corresponding SHA-512 hashes, signatures, and the project release KEYS. Instructions for verifying the SHA-512 hashes and signatures can be found in the provided guidelines. ",
    "url": "/downloads/#instructions",
    
    "relUrl": "/downloads/#instructions"
  },"12": {
    "doc": "Downloads",
    "title": "The Latest Apache Official Release",
    "content": "Please be aware that the binary file was compiled on CentOS7 using an Intel(r) Xeon(r) Gold 6252 processor with static linking. While it may operate on most operating systems and Intel SKYLAKE or later platforms, performance is not guaranteed. For the best performance and platform support, we recommend compiling from source code yourself. | Version | Date | Source | Binary | Release Notes | . | 1.4.0 | Jun. 19 2025 | source(sha512,signature) | spark32-binary(sha512,signature) spark33-binary(sha512,signature) spark34-binary(sha512,signature) spark35-binary(sha512,signature) | release notes | . | 1.3.0 | Jan. 24 2025 | source(sha512,signature) | spark32-binary(sha512,signature) spark33-binary(sha512,signature) spark34-binary(sha512,signature) spark35-binary(sha512,signature) | release notes | . | 1.2.1 | Dec. 12 2024 | source(sha512,signature) | spark32-binary(sha512,signature) spark33-binary(sha512,signature) spark34-binary(sha512,signature) spark35-binary(sha512,signature) | release notes | . ",
    "url": "/downloads/#the-latest-apache-official-release",
    
    "relUrl": "/downloads/#the-latest-apache-official-release"
  },"13": {
    "doc": "Downloads",
    "title": "Nightly Releases",
    "content": "Development versions of Gluten binaries are also built nightly and hosted at https://nightlies.apache.org/gluten/. Be aware that these binaries are unofficial and recommended for testing purpose only. ",
    "url": "/downloads/#nightly-releases",
    
    "relUrl": "/downloads/#nightly-releases"
  },"14": {
    "doc": "Downloads",
    "title": "All Archived Releases",
    "content": "| Version | Date | Source | Binary | Release Notes | . | 1.2.0 | Sept. 3 2024 | source(sha512,signature) | spark32-binary(sha512,signature) spark33-binary(sha512,signature) spark34-binary(sha512,signature) spark35-binary(sha512,signature) | release notes | . | 1.1.1(non-Apache release) | Mar. 2 2024 | source | binary location | release notes | . | 1.1.0(non-Apache release) | Nov. 30 2023 | source | binary location | release notes | . | 1.0.0(non-Apache release) | Jul. 13 2023 | source | binary location | release notes | . | 0.5.0(non-Apache release) | Apr. 7 2023 | source | binary location | release notes | . ",
    "url": "/downloads/#all-archived-releases",
    
    "relUrl": "/downloads/#all-archived-releases"
  },"15": {
    "doc": "Downloads",
    "title": "License",
    "content": "The software licensed under Apache License 2.0. ",
    "url": "/downloads/#license",
    
    "relUrl": "/downloads/#license"
  },"16": {
    "doc": "Downloads",
    "title": "Downloads",
    "content": " ",
    "url": "/downloads/",
    
    "relUrl": "/downloads/"
  },"17": {
    "doc": "Home",
    "title": "Overview",
    "content": "Apache Gluten(incubating) is a middle layer responsible for offloading JVM-based SQL engines’ execution to native engines. ",
    "url": "/#overview",
    
    "relUrl": "/#overview"
  },"18": {
    "doc": "Home",
    "title": "1 Introduction",
    "content": " ",
    "url": "/#1-introduction",
    
    "relUrl": "/#1-introduction"
  },"19": {
    "doc": "Home",
    "title": "1.1 Problem Statement",
    "content": "Apache Spark is a stable, mature project that has been developed for many years. It is one of the best frameworks to scale out for processing petabyte-scale datasets. However, the Spark community has had to address performance challenges that require various optimizations over time. As a key optimization in Spark 2.0, Whole Stage Code Generation is introduced to replace Volcano Model, which achieves 2x speedup. Henceforth, most optimizations are at query plan level. Single operator’s performance almost stops growing. On the other side, SQL engines have been researched for many years. There are a few libraries like Clickhouse, Arrow and Velox, etc. By using features like native implementation, columnar data format and vectorized data processing, these libraries can outperform Spark’s JVM based SQL engine. However, these libraries only support single node execution. ",
    "url": "/#11-problem-statement",
    
    "relUrl": "/#11-problem-statement"
  },"20": {
    "doc": "Home",
    "title": "1.2 Gluten’s Solution",
    "content": "“Gluten” is Latin for glue. The main goal of Gluten project is to “glue” native libraries with SparkSQL. Thus, we can benefit from high scalability of Spark SQL framework and high performance of native libraries. The basic rule of Gluten’s design is that we would reuse spark’s whole control flow and as many JVM code as possible but offload the compute-intensive data processing part to native code. Here is what Gluten does: . | Transform Spark’s whole stage physical plan to Substrait plan and send to native | Offload performance-critical data processing to native library | Define clear JNI interfaces for native libraries | Switch available native backends easily | Reuse Spark’s distributed control flow | Manage data sharing between JVM and native | Extensible to support more native accelerators | . ",
    "url": "/#12-glutens-solution",
    
    "relUrl": "/#12-glutens-solution"
  },"21": {
    "doc": "Home",
    "title": "1.3 Target User",
    "content": "Gluten’s target user is anyone who wants to accelerate SparkSQL fundamentally. As a plugin to Spark, Gluten doesn’t require any change for dataframe API or SQL query, but only requires user to make correct configuration. See Gluten configuration properties here. ",
    "url": "/#13-target-user",
    
    "relUrl": "/#13-target-user"
  },"22": {
    "doc": "Home",
    "title": "1.4 References",
    "content": "You can click below links for more related information. | Gluten References | . ",
    "url": "/#14-references",
    
    "relUrl": "/#14-references"
  },"23": {
    "doc": "Home",
    "title": "2 Architecture",
    "content": "The overview chart is like below. Substrait provides a well-defined cross-language specification for data compute operations (see more details here). Spark physical plan is transformed to Substrait plan. Then Substrait plan is passed to native through JNI call. On native side, the native operator chain will be built out and offloaded to native engine. Gluten will return Columnar Batch to Spark and Spark Columnar API (since Spark-3.0) will be used at execution time. Gluten uses Apache Arrow data format as its basic data format, so the returned data to Spark JVM is ArrowColumnarBatch. Currently, Gluten only supports Clickhouse backend &amp; Velox backend. Velox is a C++ database acceleration library which provides reusable, extensible and high-performance data processing components. More details can be found from https://github.com/facebookincubator/velox/. Gluten can also be extended to support more backends. There are several key components in Gluten: . | Query Plan Conversion: converts Spark’s physical plan to Substrait plan. | Unified Memory Management: controls native memory allocation. | Columnar Shuffle: shuffles Gluten columnar data. The shuffle service still reuses the one in Spark core. A kind of columnar exchange operator is implemented to support Gluten columnar data format. | Fallback Mechanism: supports falling back to Vanilla spark for unsupported operators. Gluten ColumnarToRow (C2R) and RowToColumnar (R2C) will convert Gluten columnar data and Spark’s internal row data if needed. Both C2R and R2C are implemented in native code as well | Metrics: collected from Gluten native engine to help identify bugs, performance bottlenecks, etc. The metrics are displayed in Spark UI. | Shim Layer: supports multiple Spark versions. We plan to only support Spark’s latest 2 or 3 releases. Currently, Spark-3.2, Spark-3.3 &amp; Spark-3.4 (experimental) are supported. | . ",
    "url": "/#2-architecture",
    
    "relUrl": "/#2-architecture"
  },"24": {
    "doc": "Home",
    "title": "3 How to Use",
    "content": "There are two methods for utilizing Gluten. The first is to use a pre-built JAR for a quick test of Gluten’s acceleration capabilities. The second is to compile the JAR yourself, ensuring it runs on your target platform and delivers the best possible performance. ",
    "url": "/#3-how-to-use",
    
    "relUrl": "/#3-how-to-use"
  },"25": {
    "doc": "Home",
    "title": "3.1 Use a pre-Built Jar",
    "content": "One way is to use released jar. Here is a simple example. Currently, only centos7/8 and ubuntu20.04/22.04 are well supported. You can find a pre-built gluten release jar in Apache Gluten Download. Please be aware that the pre-built JAR is compiled using static linking via vcpkg, based on the Intel® Xeon® Gold 6252 processor. It should be compatible with most operating systems, including CentOS and Ubuntu, although performance is not guaranteed. For the optimal performance experience, we recommend using the 3.2 Custom Build to compile Gluten from source tailored to your specific platform. spark-shell \\ --master yarn --deploy-mode client \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --jars gluten-velox-bundle-spark3.2_2.12-centos_7_x86_64-1.2.0.jar . ",
    "url": "/#31-use-a-pre-built-jar",
    
    "relUrl": "/#31-use-a-pre-built-jar"
  },"26": {
    "doc": "Home",
    "title": "3.2 Custom Build",
    "content": "To utilize Gluten with Spark, you can compile Gluten from the source and then configure it to enable the Gluten plugin. Below is a straightforward example. For more comprehensive instructions, please refer to the detailed guidance provided in the corresponding backend section. export gluten_jar = /PATH/TO/GLUTEN/backends-velox/target/&lt;gluten-jar&gt; spark-shell \\ --master yarn --deploy-mode client \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --conf spark.driver.extraClassPath=${gluten_jar} \\ --conf spark.executor.extraClassPath=${gluten_jar} \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager ... ",
    "url": "/#32-custom-build",
    
    "relUrl": "/#32-custom-build"
  },"27": {
    "doc": "Home",
    "title": "3.2.1 Build and install Gluten with Velox backend",
    "content": "If you want to use Gluten Velox backend, see Build with Velox to build and install the necessary libraries. You can also find more information in Gluten with Velox backend and velox. ",
    "url": "/#321-build-and-install-gluten-with-velox-backend",
    
    "relUrl": "/#321-build-and-install-gluten-with-velox-backend"
  },"28": {
    "doc": "Home",
    "title": "3.2.2 Build and install Gluten with ClickHouse backend",
    "content": "If you want to use Gluten ClickHouse backend, see Build with ClickHouse Backend. ClickHouse backend is developed by Kyligence, please visit Kyligence’s ClickHouse for more infomation. ",
    "url": "/#322-build-and-install-gluten-with-clickhouse-backend",
    
    "relUrl": "/#322-build-and-install-gluten-with-clickhouse-backend"
  },"29": {
    "doc": "Home",
    "title": "3.2.3 Build options",
    "content": "See Build Parameters. ",
    "url": "/#323-build-options",
    
    "relUrl": "/#323-build-options"
  },"30": {
    "doc": "Home",
    "title": "4 Join the Community",
    "content": " ",
    "url": "/#4-join-the-community",
    
    "relUrl": "/#4-join-the-community"
  },"31": {
    "doc": "Home",
    "title": "Contact Us",
    "content": "Gluten was initiated by Intel and Kyligence in 2022. Several companies are also actively participating in the development, such as BIGO, Meituan, Alibaba Cloud, NetEase, Baidu, Microsoft, etc. If you are interested in Gluten project, please contact and subscribe below mailing lists for further discussion. Please see contact us for more information. ",
    "url": "/#contact-us",
    
    "relUrl": "/#contact-us"
  },"32": {
    "doc": "Home",
    "title": "Source Code",
    "content": "Please see gluten source code for more information. ",
    "url": "/#source-code",
    
    "relUrl": "/#source-code"
  },"33": {
    "doc": "Home",
    "title": "How to Contribute to Gluten",
    "content": "Please see contributing guide about how to make contributions. ",
    "url": "/#how-to-contribute-to-gluten",
    
    "relUrl": "/#how-to-contribute-to-gluten"
  },"34": {
    "doc": "Home",
    "title": "Wechat Group",
    "content": "There is a Wechat group (in Chinese) which may be more friendly for PRC developers/users. Due to the limitation of wechat group, please mail to dev@gluten.apache.org. ",
    "url": "/#wechat-group",
    
    "relUrl": "/#wechat-group"
  },"35": {
    "doc": "Home",
    "title": "Slack channel",
    "content": "For Velox backend, we recommend to use velox community. ",
    "url": "/#slack-channel",
    
    "relUrl": "/#slack-channel"
  },"36": {
    "doc": "Home",
    "title": "5 Performance",
    "content": "We use TPCH-like and TPCDS-like as decison support benchmarks to evaluate Gluten’s performance. TPCH-like is a query set for modified from TPC-H benchmark and TPCDS-like is a query set for modified from TPC-DS benchmark. We use Parquet file format for Velox testing &amp; MergeTree file format for Clickhouse testing, compared to Parquet file format as baseline. See Decision Support Benchmark. ",
    "url": "/#5-performance",
    
    "relUrl": "/#5-performance"
  },"37": {
    "doc": "Home",
    "title": "Gluten + Velox backend Performance",
    "content": "The below test environment: single node with 3TB data on Intel® Xeon® Platinum 8592+; Spark-3.3.1 for both baseline and Gluten. The TPCH-like result (tested in Mar. 2024) shows an overall speedup of 3.34x and up to 23.45x speedup in a single query with Gluten + Velox backend used. The TPCDS-like result (tested in Mar. 2024) shows an overall speedup of 3.02x and up to 13.75x speedup in a single query with Gluten + Velox backend used. Notices &amp; Disclaimers . Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation. ",
    "url": "/#gluten--velox-backend-performance",
    
    "relUrl": "/#gluten--velox-backend-performance"
  },"38": {
    "doc": "Home",
    "title": "Gluten + ClickHouse backend Performance",
    "content": "The below testing environment: a 8-nodes AWS cluster with 1TB data; Spark-3.1.1 for both baseline and Gluten. The Decision Support Benchmark1 result shows an average speedup of 2.12x and up to 3.48x speedup with Gluten Clickhouse backend. ",
    "url": "/#gluten--clickhouse-backend-performance",
    
    "relUrl": "/#gluten--clickhouse-backend-performance"
  },"39": {
    "doc": "Home",
    "title": "6 Security",
    "content": "Gluten aims to provide secure software. If you discover a vulnerability, please report it promptly to the project’s PPMC or the ASF security team. We appreciate your effort to help keep the project secure. ",
    "url": "/#6-security",
    
    "relUrl": "/#6-security"
  },"40": {
    "doc": "Home",
    "title": "Thanks to our contributors",
    "content": ". ",
    "url": "/#thanks-to-our-contributors",
    
    "relUrl": "/#thanks-to-our-contributors"
  },"41": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"42": {
    "doc": "Powered by Gluten",
    "title": "Project and Product Name using Apache Gluten",
    "content": "Organizations developing products and projects for use with Apache Gluten (incubating), as well as related marketing materials, should ensure they respect the trademark of “Apache Gluten” and its logo. For comprehensive and authoritative guidance on the proper usage of ASF trademarks, please refer to the ASF Trademarks Guidance and the associated FAQ. ",
    "url": "/poweredby/#project-and-product-name-using-apache-gluten",
    
    "relUrl": "/poweredby/#project-and-product-name-using-apache-gluten"
  },"43": {
    "doc": "Powered by Gluten",
    "title": "Companies and Organizations",
    "content": "To add yourself to the list, please email dev@gluten.apache.org with the following information: your organization name, URL, a list of the Gluten components you are using, and a brief description of your use case. Sorted by Company Name . | Company Name | Description | Link | . | Alibaba | Alibaba has chosen Gluten as a part of their public cloud solution. |   | . | Baidu | Baidu has used gluten in multiple business scenarios. |   | . | Bytedance | EMR in Bytedance’s Volcengine | Link | . | BIGO | BIGO is a global technology company headquartered in Singapore, focusing on the development and operation of social media and live streaming platforms. It owns several well-known applications, such as Bigo Live and Likee. | Link | . | BONC | BONC is a high-tech software company specializing in big data, cloud computing, and other technological fields. | Link | . | IBM | Gluten and Velox at IBM | Link | . | Kyligence | Shanghai Kyligence Information Technology Co., Ltd. (Kyligence) was founded in 2016 by the original team behind Apache Kylin. The company is dedicated to developing the next-generation enterprise-level intelligent multidimensional database, aimed at simplifying multidimensional data analysis (OLAP) on data lakes for enterprises. | Link | . | Meituan | Meituan, a Chinese shopping platform, has chosen Gluten for their production-ready environment. | Link | . | Microsoft | Microsoft Fabric is an AI-powered analytics platform | Link | . | Google | Google Lightning engine | Link | . | Netease | Netease is a Chinese Internet technology company providing online services. | Link | . | Palantir | Palantir’s product implementation of native acceleration is built upon the Apache Gluten project. | Link | . | Pinterest | Gluten and Velox at Pinterest | Link | . | SF Express | Gluten and Velox has been used in SF Express since 2024. |   | . | Uber | Uber’s Spark Data Platform has been using Gluten since 2024 to reduce compute costs and improve query efficiency. |   | . | DataPelago | DataPelago Accelerator for Spark — the next frontier in Spark performance and efficiency | Link | . ",
    "url": "/poweredby/#companies-and-organizations",
    
    "relUrl": "/poweredby/#companies-and-organizations"
  },"44": {
    "doc": "Powered by Gluten",
    "title": "Powered by Gluten",
    "content": " ",
    "url": "/poweredby/",
    
    "relUrl": "/poweredby/"
  },"45": {
    "doc": "Gluten References",
    "title": "Gluten related Use Cases and Publications",
    "content": "For more information on Apache Gluten (incubating), including related articles and videos, please refer to the following links. | Title | Company | Publish Date | Reference | . | Gluten: A middle layer to offload Spark SQL to Native | Intel | Jul. 2022 | Link | . | Best Exploration of Columnar Shuffle Design | Intel | Jul. 2022 | Link | . | Gluten Introduction | Intel | Sept. 2022 | Link | . | Introducing Velox | Meta | Mar. 2023 | Link | . | Gluten in Kyligence(in Chinese) | Kyligence | Mar. 2023 | Link | . | Velox in Intel | Intel | Jun. 2023 | Link | . | Gluten + Celeborn(in Chinese) | Intel | Jul. 2023 | Link | . | Gluten: Modernizing Java-based Query Engines | Intel | Sept. 2023 | Link | . | Apache Spark with Native Engine | BIGO | Nov. 2023 | Link | . | BONC’s BEH with Gluten and AVX512 | BONC | Nov. 2023 | Link | . | Gluten: Double Spark Performance | Intel | Nov. 2023 | Link | . | Apache Spark Native Engine(in Chinese) | Netease | Nov. 2023 | Link | . | Apache Spark Native Engine | Netease | Dec. 2023 | Link | . | Apache Gluten Status Update | Intel | Apr. 2024 | Link | . | Accelearting Spark at Microsot using Gluten and Velox | Microsoft | Apr. 2024 | Link | . | Velox at IBM | IBM | Apr. 2024 | Link | . | Unlocking Data Query Performance | Pinterest | Apr. 2024 | Link | . | Native execution engine for Fabric Spark | Microsoft | May 2024 | Link | . | Best Practice of Gluten and Velox in Meituan | Meituan | Jun. 2024 | Link | . | EMR in Bytedance’s Volcengine(in Chinese) | ByteDance | Dec. 2024 | Link | . | State of the Union: Apache Gluten | IBM | Apr. 2025 | Link | . | Real World Applications of Velox and Apache Gluten in Dataproc’s NQE | Google | Apr. 2025 | Link | . | Apache Gluten: Revolutionizing Data Processing Efficiency | IBM | Apr. 2025 | Link | . | Gluten + ClickHouse Backend Performance Optimization Practise(in Chinese) | BIGO | Jun. 2025 | Link | . | The R&amp;D and Application of Gluten ClickHouse Backend at BIGO and Progress on Flink Native(in Chinese) | BIGO | May 2025 | Link | . ",
    "url": "/references/#gluten-related-use-cases-and-publications",
    
    "relUrl": "/references/#gluten-related-use-cases-and-publications"
  },"46": {
    "doc": "Gluten References",
    "title": "Gluten References",
    "content": " ",
    "url": "/references/",
    
    "relUrl": "/references/"
  },"47": {
    "doc": "Archives",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/#gluten-documents-by-version",
    
    "relUrl": "/archives/#gluten-documents-by-version"
  },"48": {
    "doc": "Archives",
    "title": "Archives",
    "content": " ",
    "url": "/archives/",
    
    "relUrl": "/archives/"
  },"49": {
    "doc": "Documentation(Latest)",
    "title": "Documentation(Latest)",
    "content": "  Click here to view the latest documentation . If you are not redirected, click here. ",
    "url": "/docs/latest.html",
    
    "relUrl": "/docs/latest.html"
  },"50": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Gluten CPP Core Guidelines",
    "content": "This is a set of CPP core guidelines for Gluten. The aim is to make the codebase simpler, more efficient, more maintainable by promoting consistency and according to best practices. ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#gluten-cpp-core-guidelines",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#gluten-cpp-core-guidelines"
  },"51": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Philosophy",
    "content": "Philosophical rules are generally not measurable. However, they are valuable. For Gluten CPP coding, there are a few Philosophical rules as the following. | Write in ISO Standard C++. | Standard API first, the CPP programming APIs are priority to system calls. | Write code consistently. It’s good for understanding and maintaining. | Keep simple, making code clear and easy to read. | Optimize code for reader, not for writer. Thus, more time will be spent reading code than writing it. | Make it work, and then make it better or faster. | Don’t import any complexity if possible. Collaborate with minimal knowledge consensus. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#philosophy",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#philosophy"
  },"52": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Code Formatting",
    "content": "Many aspects of C++ coding style will be covered by clang-format, such as spacing, line width, indentation and ordering (for includes, using directives and etc).  . | Always ensure your code is compatible with clang-format-12 for Velox backend. | dev/formatcppcode.sh is provided for formatting Velox CPP code. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#code-formatting",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#code-formatting"
  },"53": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Naming Conventions",
    "content": ". | Use PascalCase for types (class, struct, enum, type alias, type template parameter) and file name. | Use camelCase for function, member and local variable, and non-type template parameter. | Use camelCase_ for private and protected member variable. | Use snake_case for namespace name and build target. | Use UPPER_SNAKE_CASE for macro. | Use kPascalCase for static constant and enumerator. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#naming-conventions",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#naming-conventions"
  },"54": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Designs",
    "content": ". | No over design. | No negation of negation, isValid is better than isNotInvalid. | Avoid corner case, and common case first. | Express ideas directly, don’t let me think. | Make a check for the arguments in the interface between modules, and don’t make a check in the inner implementation, use assert in the private implementation instead of too much safe check. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#designs",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#designs"
  },"55": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Source File &amp; Header File",
    "content": ". | All header files must have a single-inclusion guard using #pragma once | Always use .h as header file suffix, not .hpp. | Always use .cc as source file suffix, neither .cpp nor .cxx. | One file should contain one main class, and the file name should be consistent with the main class name. | Obvious exception: files used for defining various misc functions. | . | If a header file has a corresponding source file, they should have the same file name with different suffix, such as a.h vs a.cc. | If a function is declared in the file a.h, ensure it’s defined in the corrosponding source file a.cc, do not define it in other files. | No deep source directory for CPP files, not do it as JAVA. | Include header files should satisfy the following rules. | Include the necessary header files, which means the source file (.cc) containing the only one line #include \"test.h\" can be compiled successfully without including any other header files. | Do not include any unnecessary header files, the more including, the slower compiling. | In one word, no more, no less, just as needed. | . | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#source-file--header-file",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#source-file--header-file"
  },"56": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Class",
    "content": ". | Base class name doesn’t end with Base, use Backend instead of BackendBase. | Ensure one class does one thing, and follows the single responsibility principle. | No big class, No huge class, No too much interfaces. | Distinguish interface from implementation, make implementations private. | When designing a class hierarchy, distinguish between interface inheritance and implementation inheritance. | Ensure that public inheritance represent the relation of is-a. | Ensure that private inheritance represent the relation of implements-with. | . | Don’t make a function virtual without reason. | Ensure the polymorphic base class has a virtual deconstructor. | Use override to make overriding explicit and to make the compiler work. | Use const to mark the member function read-only as far as possible. | When you try to define a copy constructor or a operator= for a class, remember the Rule of three/five/zero. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#class",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#class"
  },"57": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Function",
    "content": ". | Make functions short and simple. | Calling a meaningful function is more readable than writing too many statements in place, but the performance-sensitive code path is an exception. | Give the function a good name, how to check whether the function name is good or not. | When you read it loudly, you feel smooth. | The information can be represented by arguments should not be encoded into the function name. such as. use get(size_t index) instead of getByIndex. | . | A function should focus on a single logic operation. | A function should do as the name meaning. | do everything converd by the function name | don’t do anything not convered by the function name | . | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#function",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#function"
  },"58": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Variable",
    "content": ". | Make variable names simple and meaningful. | Don’t group all your variables at the top of the scope, it’s an outdated habit. | Declare variables as close to the usage point as possible. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#variable",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#variable"
  },"59": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Constant",
    "content": ". | Prefer const variables to using preprocessor (#define) to define constant values. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#constant",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#constant"
  },"60": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Macro",
    "content": ". | Macros downgrade readability, break mind, and affect debug. | Macros have side effects. | Use macros cautiously and carefully. | Consider using const variables or inline functions to replace macros. | Consider defining macros with the wrap of do {...} while (0) | Avoid using 3rd party library macros directly. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#macro",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#macro"
  },"61": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Namespace",
    "content": ". | Don’t using namespace xxx in header files. Instead, you can do this in source files. But it’s still not encouraged. | Place all Gluten CPP codes under namespace gluten because one level namespace is enough. No nested namespace. Nested namespaces bring mess. | The anonymous namespace is recommended for defining file level classes, functions and variables. It’s used to place file scoped static functions and variables. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#namespace",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#namespace"
  },"62": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Resource Management",
    "content": ". | Use handles and RAII to manage resources automatically. | Immediately give the result of an explicit resource allocation to a manager object. | Prefer scoped objects and stack objects. | Use raw pointers to denote individual objects. | Use pointer + size_t to denote array objects if you don’t want to use containers. | A raw pointer (a T*) is non-owning. | A raw reference (a T&amp;) is non-owning. | Understand the difference of unique_ptr, shared_ptr, weak_ptr. | unique_ptr represents ownership, but not share ownership. unique_ptr is equivalent to RAII, release the resource when the object is destructed. | shared_ptr represents shared ownership by use-count. It is more expensive that unqiue_ptr. | weak_ptr models temporary ownership. It is useful in breaking reference cycles formed by objects managed by shared_ptr. | . | Use unique_ptr or shared_ptr to represent ownership. | Prefer unique_ptr over shared_ptr unless you need to share ownership. | Use make_unique to make unique_ptrs. | Use make_shared to make shared_ptrs. | Take smart pointers as parameters only to explicitly express lifetime semantics. | For general use, take T* or T&amp; arguments rather than smart pointers. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#resource-management",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#resource-management"
  },"63": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Exception",
    "content": ". | The exception specifications are changing always. The difference between various CPP standards is big, so we should use exception cautiously in Gluten. | Prefer return code to throwing exceptions. | Prefer compile-time checking to run-time checking. | Encapsulate messy constructors, rather than spreading through the code. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#exception",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#exception"
  },"64": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "Code Comment",
    "content": ". | Add necessary comments. The comment is not the more the better, also not the less the better. | Good comment makes obscure code easily understood. It’s unnecessary to add comments for quite obvious code. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#code-comment",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#code-comment"
  },"65": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "References",
    "content": ". | CppCoreGuidelines | Velox CODING_STYLE | Thanks Gluten developers for their wise suggestions and helps. | . ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/#references",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/#references"
  },"66": {
    "doc": "v1.1.1/CPP Code Style",
    "title": "v1.1.1/CPP Code Style",
    "content": " ",
    "url": "/archives/v1.1.1/developers/cpp_code_style/",
    
    "relUrl": "/archives/v1.1.1/developers/cpp_code_style/"
  },"67": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "How to understand the key work of Gluten?",
    "content": "The Gluten worked as the role of bridge, it’s a middle layer between the Spark and the native execution library. The Gluten is responsibility for validating whether the operators of the Spark plan can be executed by the native engine or not. If yes, the Gluten transforms Spark plan to Substrait plan, and then send the Substrait plan to the native engine. The Gluten codes consist of two parts: the C++ codes and the Java/Scala codes. | All C++ codes are placed under the directory of gluten_home/cpp, the Java/Scala codes are placed under several directories, such as gluten_home/gluten-core gluten_home/gluten-data gluten_home/backends-velox. | The Java/Scala codes are responsibility for validating and transforming the execution plan. Source data should also be provided, the source data may come from files or other forms such as networks. | The C++ codes take the Substrait plan and the source data as inputs and transform the Substrait plan to the corresponding backend plan. If the backend is Velox, the Substrait plan will be transformed to the Velox plan, and then be executed. | . JNI is a programming technology of invoking C++ from Java. All JNI interfaces are defined in the file JniWrapper.cc under the directory jni. ",
    "url": "/archives/v1.1.1/developers/howto/#how-to-understand-the-key-work-of-gluten",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#how-to-understand-the-key-work-of-gluten"
  },"68": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "How to debug in Gluten?",
    "content": " ",
    "url": "/archives/v1.1.1/developers/howto/#how-to-debug-in-gluten",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#how-to-debug-in-gluten"
  },"69": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "1 How to debug C++",
    "content": "If you don’t concern about the Scala/Java codes and just want to debug the C++ codes executed in native engine, you may debug the C++ via benchmarks with GDB. To debug C++, you have to generate the example files, the example files consist of: . | A file contained Substrait plan in JSON format | One or more input data files in Parquet format | . You can generate the example files by the following steps: . | build Velox and Gluten CPP gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . | Compiling with --build_type=Debug is good for debugging. | The executable file generic_benchmark will be generated under the directory of gluten_home/cpp/build/velox/benchmarks/. | . | build Gluten and generate the example files cd gluten_home mvn clean package -Pspark-3.2 -Pbackends-velox -Prss mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . | After the above operations, the examples files are generated under gluten_home/backends-velox | You can check it by the command tree gluten_home/backends-velox/generated-native-benchmark/ | You may replace -Pspark-3.2 with -Pspark-3.3 if your spark’s version is 3.3 $ tree gluten_home/backends-velox/generated-native-benchmark/ gluten_home/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . | . | now, run benchmarks with GDB cd gluten_home/cpp/build/velox/benchmarks/ gdb generic_benchmark . | When GDB load generic_benchmark successfully, you can set breakpoint on the main function with command b main, and then run with command r, then the process generic_benchmark will start and stop at the main function. | You can check the variables’ state with command p variable_name, or execute the program line by line with command n, or step-in the function been called with command s. | Actually, you can debug generic_benchmark with any gdb commands as debugging normal C++ program, because the generic_benchmark is a pure C++ executable file in fact. | . | gdb-tui is a valuable feature and is worth trying. You can get more help from the online docs. gdb-tui . | you can start generic_benchmark with specific JSON plan and input files . | If you omit them, the example.json, example_lineitem + example_orders under the directory of gluten_home/backends-velox/generated-native-benchmark will be used as default. | You can also edit the file example.json to custom the Substrait plan or specify the inputs files placed in the other directory. | . | get more detail information about benchmarks from MicroBenchmarks | . ",
    "url": "/archives/v1.1.1/developers/howto/#1-how-to-debug-c",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#1-how-to-debug-c"
  },"70": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "2 How to debug plan validation process",
    "content": "Gluten will validate generated plan before execute it, and validation usually happens in native side, so we provide a utility to help debug validation process in native side. | Run query with conf spark.gluten.sql.debug=true, and you will find generated plan be printed in stderr with json format, save it as plan.json for example. | Compile cpp part with --build_benchmarks=ON, then check plan_validator_util executable file in gluten_home/cpp/build/velox/benchmarks/. | Run or debug with ./plan_validator_util &lt;path&gt;/plan.json | . ",
    "url": "/archives/v1.1.1/developers/howto/#2-how-to-debug-plan-validation-process",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#2-how-to-debug-plan-validation-process"
  },"71": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "3 How to debug Java/Scala",
    "content": "wait to add . ",
    "url": "/archives/v1.1.1/developers/howto/#3-how-to-debug-javascala",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#3-how-to-debug-javascala"
  },"72": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "4 How to debug with core-dump",
    "content": "wait to complete . cd the_directory_of_core_file_generated gdb gluten_home/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . | the core-Executor task l-2000883-1671542526 represents the core file name. | . ",
    "url": "/archives/v1.1.1/developers/howto/#4-how-to-debug-with-core-dump",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#4-how-to-debug-with-core-dump"
  },"73": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "How to run TPC-H on Velox backend",
    "content": "Now, both Parquet and DWRF format files are supported, related scripts and files are under the directory of gluten_home/backends-velox/workload/tpch. The file README.md under gluten_home/backends-velox/workload/tpch offers some useful help but it’s still not enough and exact. One way of run TPC-H test is to run velox-be by workflow, you can refer to velox_be.yml . Here will explain how to run TPC-H on Velox backend with the Parquet file format. | First step, prepare the datasets, you have two choices. | One way, generate Parquet datasets using the script under gluten_home/backends-velox/workload/tpch/gen_data/parquet_dataset, You can get help from the above mentioned README.md. | The other way, using the small dataset under gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox directly, If you just want to make simple TPC-H testing, this dataset is a good choice. | . | Second step, run TPC-H on Velox backend testing. | Modify gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.scala. | . | set var parquet_file_path to correct directory. If using the small dataset directly in the step one, then modify it as below var parquet_file_path = \"gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox\" . | set var gluten_root to correct directory. If gluten_home is the directory of /home/gluten, then modify it as below var gluten_root = \"/home/gluten\" . - Modify `gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.sh`. | Set GLUTEN_JAR correctly. Please refer to the section of Build Gluten with Velox Backend | Set SPARK_HOME correctly. | Set the memory configurations appropriately. - Execute tpch_parquet.sh using the below command. | cd gluten_home/backends-velox/workload/tpch/run_tpch/ | ./tpch_parquet.sh | . | . ",
    "url": "/archives/v1.1.1/developers/howto/#how-to-run-tpc-h-on-velox-backend",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#how-to-run-tpc-h-on-velox-backend"
  },"74": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "How to run TPC-DS",
    "content": "wait to add . ",
    "url": "/archives/v1.1.1/developers/howto/#how-to-run-tpc-ds",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#how-to-run-tpc-ds"
  },"75": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "How to track the memory exhaust problem",
    "content": "When your gluten spark jobs failed because of OOM, you can track the memory allocation’s call stack by configuring spark.gluten.backtrace.allocation = true. The above configuration will use BacktraceAllocationListener wrapping from SparkAllocationListener to create VeloxMemoryManager. BacktraceAllocationListener will check every allocation, if a single allocation bytes exceeds a fixed value or the accumulative allocation bytes exceeds 1/2/3…G, the call stack of memory allocation will be outputted to standard output, you can check the backtrace and get some valuable information about tracking the memory exhaust issues. You can also adjust the policy to decide when to backtrace, such as the fixed value. ",
    "url": "/archives/v1.1.1/developers/howto/#how-to-track-the-memory-exhaust-problem",
    
    "relUrl": "/archives/v1.1.1/developers/howto/#how-to-track-the-memory-exhaust-problem"
  },"76": {
    "doc": "v1.1.1/How To Use Gluten",
    "title": "v1.1.1/How To Use Gluten",
    "content": "There are some common questions about developing, debugging and testing been asked again and again. In order to help the developers to contribute to Gluten as soon as possible, we collected these frequently asked questions, and organized them in the form of Q&amp;A. It’s convenient for the developers to check and learn. when you encountered a new problem and then resolved it, please add a new item to this document if you think it may be helpful to the other developers. We use gluten_home to represent the home directory of Gluten in this document. ",
    "url": "/archives/v1.1.1/developers/howto/",
    
    "relUrl": "/archives/v1.1.1/developers/howto/"
  },"77": {
    "doc": "v1.1.1/How To Release",
    "title": "How to Release",
    "content": "This section outlines the steps for releasing Apache Gluten (incubating) according to the Apache release guidelines. All projects under the Apache umbrella must adhere to the Apache Release Policy. This guide is designed to assist you in comprehending the policy and navigating the process of releasing projects at Apache. ",
    "url": "/archives/v1.1.1/developers/howtorelease/#how-to-release",
    
    "relUrl": "/archives/v1.1.1/developers/howtorelease/#how-to-release"
  },"78": {
    "doc": "v1.1.1/How To Release",
    "title": "Release Process",
    "content": ". | Prepare the release artifacts. | Upload the release artifacts to the SVN repository. | Verify the release artifacts. | Initiate a release vote. | Announce the results and the release. | . Prepare the release artifacts. | Create a branch from the target git repository. | Tag a RC and draft the release notes. | Build and Sign the release artifacts (including source archives, binaries, …etc). | Generate checksums for the artifacts. | . How to Sign the release artifacts. | Create a GPG key . | Add the GPG key to the KEYS file . | Sign the release artifacts with the GPG key. | . # create a GPG key, after executing this command, select the first one RSA 和 RSA $ gpg --full-generate-key # list the GPG keys $ gpg --keyid-format SHORT --list-keys # upload the GPG key to the key server, xxx is the GPG key id # eg: pub rsa4096/4C21E346 2024-05-06 [SC], 4C21E346 is the GPG key id; $ gpg --keyserver keyserver.ubuntu.com --send-key xxx # append the GPG key to the KEYS file the svn repository # [IMPORTANT] Don't replace the KEYS file, just append the GPG key to the KEYS file. $ svn co https://dist.apache.org/repos/dist/release/incubator/gluten/ $ (gpg --list-sigs xxx@apache.org &amp;&amp; gpg --export --armor xxx@apache.org) &gt;&gt; KEYS $ svn ci -m \"add gpg key\" # sign the release artifacts, xxxx is xxx@apache.org $ for i in *.tar.gz; do echo $i; gpg --local-user xxxx --armor --output $i.asc --detach-sig $i ; done . How to Generate checksums for the release artifacts. # create the checksums $ for i in *.tar.gz; do echo $i; sha512sum $i &gt; $i.sha512 ; done . Upload the release artifacts to the SVN repository. | Create a project directory in the SVN repository (1st time only). https://dist.apache.org/repos/dist/dev/incubator/gluten/ . | Create a directory for the release artifacts in the SVN repository. https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} release-version format: apache-gluten-#.#.#-rc# . | Upload the release artifacts to the SVN repository. $ svn co https://dist.apache.org/repos/dist/dev/incubator/gluten/ $ cp /path/to/release/artifacts/* ./{release-version}/ $ svn add ./{release-version}/* $ svn commit -m \"add Apache Gluten release artifacts for {release-version}\" . | After the upload, please visit the link https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} to verify if the file upload is successful or not. The upload release artifacts should be include * apache-gluten-#.#.#-incubating-src.tar.gz * apache-gluten-#.#.#-incubating-src.tar.gz.asc * apache-gluten-#.#.#-incubating-src.tar.gz.sha512 . | . Verify the release artifacts. Please follow below steps to verify the release artifacts. | Check if the Download links are valid. | Check if the checksums and GPG signatures are valid. | Check if the release artifacts name is qualified and match with the current release. | Check if LICENSE and NOTICE files are correct. | Check if the License Headers are included in all files if necessary. | No unlicensed compiled archives bundled in source archive. | . How to Verify the Signatures . Please follow below steps to verify the signatures. # download KEYS $ curl https://dist.apache.org/repos/dist/release/incubator/gluten/KEYS &gt; KEYS # import KEYS and trust the key, please replace the email address with the one you want to trust. $ gpg --import KEYS $ gpg --edit-key xxx@apache.org gpg&gt; trust gpg&gt; 5 gpg&gt; y gpg&gt; quit # enter the directory where the release artifacts are located $ cd /path/to/release/artifacts # verify the signature $ for i in *.tar.gz; do echo $i; gpg --verify $i.asc $i ; done # if you see 'Good signature' in the output, it means the signature is valid. How to Verify the checksums . Please follow below steps to verify the checksums . # verify the checksums $ for i in *.tar.gz; do echo $i; sha512sum --check $i.sha512; done . Initiate a release vote. | Email a vote request to dev@gluten.apache.org, requiring at least 3 PPMC +1s. | Allow 72 hours or until enough votes are collected. | Share the vote outcome on the dev list. | If successful, request a vote on general@incubator.apache.org, needing 3 PMC +1s. | Wait 72 hours or for sufficient votes. | Announce the results on the general list. | . Vote Email Template . [VOTE] Release Apache Gluten (Incubating) {release-version} Hello, This is a call for vote to release Apache Gluten (Incubating) version {release-version}. The vote thread: https://lists.apache.org/thread/{id} Vote Result: https://lists.apache.org/thread/{id} The release candidates: https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version}/ Release notes: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Git tag for the release: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Git commit id for the release: https://github.com/apache/incubator-gluten/commit/{id} Keys to verify the Release Candidate: https://downloads.apache.org/incubator/gluten/KEYS The vote will be open for at least 72 hours or until the necessary number of votes are reached. Please vote accordingly: [ ] +1 approve [ ] +0 no opinion [ ] -1 disapprove with the reason Checklist for reference: [ ] Download links are valid. [ ] Checksums and PGP signatures are valid. [ ] Source code distributions have correct names matching the current release. [ ] LICENSE and NOTICE files are correct for each Apache Gluten repo. [ ] All files have license headers if necessary. [ ] No unlicensed compiled archives bundled in source archive. To compile from the source, please refer to: https://github.com/apache/incubator-gluten#building-from-source Thanks, &lt;YOUR NAME&gt; . Announce the results and the release. Announce Email Template . Hello everyone, The Apache Gluten (Incubating) {release-version} has been released! Apache Gluten is a Q&amp;A platform software for teams at any scale. Whether it's a community forum, help center, or knowledge management platform, you can always count on Apache Gluten. Download Links: https://downloads.apache.org/incubator/gluten/ Release Notes: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Website: https://gluten.apache.org/ Resources: - Issue: https://github.com/apache/incubator-gluten/issues - Mailing list: dev@gluten.apache.org Thanks, &lt;YOUR NAME&gt; . ",
    "url": "/archives/v1.1.1/developers/howtorelease/#release-process",
    
    "relUrl": "/archives/v1.1.1/developers/howtorelease/#release-process"
  },"79": {
    "doc": "v1.1.1/How To Release",
    "title": "v1.1.1/How To Release",
    "content": " ",
    "url": "/archives/v1.1.1/developers/howtorelease/",
    
    "relUrl": "/archives/v1.1.1/developers/howtorelease/"
  },"80": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "Generate Micro Benchmarks for Velox Backend",
    "content": "This document explains how to use the existing micro benchmark template in Gluten Cpp. A micro benchmark for Velox backend is provided in Gluten Cpp to simulate the execution of a first or middle stage in Spark. It serves as a more convenient alternative to debug in Gluten Cpp comparing with directly debugging in a Spark job. Developers can use it to create their own workloads, debug in native process, profile the hotspot and do optimizations. To simulate a first stage, you need to dump the Substrait plan and input split info into two JSON files. The input URIs of the splits should be exising file locations, which can be either local or HDFS paths. To simulate a middle stage, in addition to the JSON file, you also need to save the input data of this stage into Parquet files. The benchmark will load the data into Arrow format, then add Arrow2Velox to feed the data into Velox pipeline to reproduce the reducer stage. Shuffle exchange is not included. Please refer to the sections below to learn how to dump the Substrait plan and create the input data files. ",
    "url": "/archives/v1.1.1/developers/microbenchmark/#generate-micro-benchmarks-for-velox-backend",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/#generate-micro-benchmarks-for-velox-backend"
  },"81": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "Try the example",
    "content": "To run a micro benchmark, user should provide one file that contains the Substrait plan in JSON format, and optional one or more input data files in parquet format. The commands below help to generate example input files: . cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_tests=ON --build_benchmarks=ON # Run test to generate input data files. If you are using spark 3.3, replace -Pspark-3.2 with -Pspark-3.3 mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am \\ -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . The generated example files are placed in gluten/backends-velox: . $ tree gluten/backends-velox/generated-native-benchmark/ gluten/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . Run micro benchmark with the generated files as input. You need to specify the absolute path to the input files: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example.json \\ --data /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_orders/part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet,\\ /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_lineitem/part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet \\ --threads 1 --iterations 1 --noprint-result --benchmark_filter=InputFromBatchStream . The output should be like: . 2022-11-18T16:49:56+08:00 Running ./generic_benchmark Run on (192 X 3800 MHz CPU s) CPU Caches: L1 Data 48 KiB (x96) L1 Instruction 32 KiB (x96) L2 Unified 2048 KiB (x96) L3 Unified 99840 KiB (x2) Load Average: 0.28, 1.17, 1.59 ***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead. -- Project[expressions: (n3_0:BIGINT, ROW[\"n1_0\"]), (n3_1:VARCHAR, ROW[\"n1_1\"])] -&gt; n3_0:BIGINT, n3_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 36.33us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 queuedWallNanos sum: 2.00us, count: 2, min: 0ns, max: 2.00us -- HashJoin[RIGHT SEMI (FILTER) n0_0=n1_0] -&gt; n1_0:BIGINT, n1_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 191.56us, Blocked wall time: 0ns, Peak memory: 2.00MB, Memory allocations: 8 HashBuild: Input: 582 rows (16.45KB, 1 batches), Output: 0 rows (0B, 0 batches), Cpu time: 1.84us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 distinctKey0 sum: 583, count: 1, min: 583, max: 583 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 59748, count: 1, min: 59748, max: 59748 HashProbe: Input: 37897 rows (296.07KB, 1 batches), Output: 535 rows (65.81KB, 1 batches), Cpu time: 189.71us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 5, Threads: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns -- ArrowStream[] -&gt; n0_0:BIGINT Input: 0 rows (0B, 0 batches), Output: 37897 rows (296.07KB, 1 batches), Cpu time: 1.29ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 -- ArrowStream[] -&gt; n1_0:BIGINT, n1_1:VARCHAR Input: 0 rows (0B, 0 batches), Output: 582 rows (16.45KB, 1 batches), Cpu time: 894.22us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 ----------------------------------------------------------------------------------------------------------------------------- Benchmark Time CPU Iterations UserCounters... ----------------------------------------------------------------------------------------------------------------------------- InputFromBatchVector/iterations:1/process_time/real_time/threads:1 41304520 ns 23740340 ns 1 collect_batch_time=34.7812M elapsed_time=41.3113M . ",
    "url": "/archives/v1.1.1/developers/microbenchmark/#try-the-example",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/#try-the-example"
  },"82": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "Generate Substrait plan and input for any query",
    "content": "First, build Gluten with --build_benchmarks=ON. cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_benchmarks=ON # For debugging purpose, rebuild Gluten with build type `Debug`./dev/buildbundle-veloxbe.sh --build_benchmarks=ON --build_type=Debug . First, get the Stage Id from spark UI for the stage you want to simulate. And then re-run the query with below configurations to dump the inputs to micro benchmark. | Parameters | Description | Recommend Setting | . | spark.gluten.sql.benchmark_task.stageId | Spark task stage id | target stage id | . | spark.gluten.sql.benchmark_task.partitionId | Spark task partition id, default value -1 means all the partition of this stage | 0 | . | spark.gluten.sql.benchmark_task.taskId | If not specify partition id, use spark task attempt id, default value -1 means all the partition of this stage | target task attemp id | . | spark.gluten.saveDir | Directory to save the inputs to micro benchmark, should exist and be empty. | /path/to/saveDir | . Check the files in spark.gluten.saveDir. If the simulated stage is a first stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage, without input file splits. | Split file: JSON formatted, file name split_[stageId]_[partitionId]_[splitIndex].json. There can be more than one split file in a first stage task. Contains the substrait plan piece to the input file splits. | . Run benchmark. By default, the result will be printed to stdout. You can use --noprint-result to suppress this output. Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --split /absolut_path/to/split_[stageId]_[partitionId]_0.parquet,/absolut_path/to/split_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . If the simulated stage is a middle stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage. | Data file: Parquet formatted, file name data_[stageId]_[partitionId]_[iteratorIndex].json. There can be more than one input data file in a middle stage task. The input data files of a middle stage will be loaded as iterators to serve as the inputs for the pipeline: | . \"localFiles\": { \"items\": [ { \"uriFile\": \"iterator:0\" } ] } . Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --data /absolut_path/to/data_[stageId]_[partitionId]_0.parquet,/absolut_path/to/data_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . For some complex queries, stageId may cannot represent the Substrait plan input, please get the taskId from spark UI, and get your target parquet from saveDir. In this example, only one partition input with partition id 2, taskId is 36, iterator length is 2. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/complex_plan.json \\ --data /absolute_path/to/data_36_2_0.parquet,/absolute_path/to/data_36_2_1.parquet \\ --threads 1 --noprint-result . ",
    "url": "/archives/v1.1.1/developers/microbenchmark/#generate-substrait-plan-and-input-for-any-query",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/#generate-substrait-plan-and-input-for-any-query"
  },"83": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "Save ouput to parquet to analyze",
    "content": "You can save the output to a parquet file to analyze. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --data /absolute_path/to/data.parquet --threads 1 --noprint-result --write-file=/absolute_path/to/result.parquet . ",
    "url": "/archives/v1.1.1/developers/microbenchmark/#save-ouput-to-parquet-to-analyze",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/#save-ouput-to-parquet-to-analyze"
  },"84": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "Add shuffle write process",
    "content": "You can add the shuffle write process at the end of this stage. Note that this will ignore the --write-file option. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --split /absolute_path/to/split.json \\ --threads 1 --noprint-result --with-shuffle . By default, the compression codec for shuffle outputs is LZ4. You can switch to other codecs by adding one of the following argument flags to the command: . | –zstd: ZSTD codec, compression level 1 | –qat-gzip: QAT GZIP codec, compression level 1 | –qat-zstd: QAT ZSTD codec, compression level 1 | –iaa-gzip: IAA GZIP codec, compression level 1 | . Note using QAT or IAA codec requires Gluten cpp is built with these features. Please check the corresponding section in Velox document first for how to setup, build and enable these features in Gluten. For QAT support, please check Intel® QuickAssist Technology (QAT) support. For IAA support, please check Intel® In-memory Analytics Accelerator (IAA/IAX) support . ",
    "url": "/archives/v1.1.1/developers/microbenchmark/#add-shuffle-write-process",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/#add-shuffle-write-process"
  },"85": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "Simulate Spark with multiple processes and threads",
    "content": "You can use below command to launch several processes and threads to simulate parallel execution on Spark. Each thread in the same process will be pinned to the core number starting from --cpu. Suppose running on a baremetal machine with 48C, 2-socket, HT-on, launching below command will utilize all vcores. processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --threads $threads --cpu $((i*threads)) &amp; done . If you want to add the shuffle write process, you can specify multiple direcotries by setting environment variable GLUTEN_SPARK_LOCAL_DIRS to a comma-separated string for shuffle write to spread the I/O pressure to multiple disks. mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done . Run Examples . We also provide some example inputs in cpp/velox/benchmarks/data. E.g. generic_q5/q5_first_stage_0.json simulates a first-stage in TPCH Q5, which has the the most heaviest table scan. You can follow below steps to run this example. | Open generic_q5/q5_first_stage_0.json with file editor. Search for \"uriFile\": \"LINEITEM\" and replace LINEITEM with the URI to one partition file in lineitem. In the next line, replace the number in \"length\": \"...\" with the actual file length. Suppose you are using the provided small TPCH table in cpp/velox/benchmarks/data/tpch_sf10m, the replaced JSON should be like: | . { \"items\": [ { \"uriFile\": \"file:///path/to/gluten/cpp/velox/benchmarks/data/tpch_sf10m/lineitem/part-00000-6c374e0a-7d76-401b-8458-a8e31f8ab704-c000.snappy.parquet\", \"length\": \"1863237\", \"parquet\": {} } ] } . | Launch multiple processes and multiple threads. Set GLUTEN_SPARK_LOCAL_DIRS and add –with-shuffle to the command. | . mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0.json --split /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0_split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done &gt;stdout.log 2&gt;stderr.log . You can find the “elapsed_time” and other metrics in stdout.log. In below output, the “elapsed_time” is ~10.75s. If you run TPCH Q5 with Gluten on Spark, a single task in the same Spark stage should take about the same time. ------------------------------------------------------------------------------------------------------------------ Benchmark Time CPU Iterations UserCounters... ------------------------------------------------------------------------------------------------------------------ SkipInput/iterations:1/process_time/real_time/threads:8 1317255379 ns 10061941861 ns 8 collect_batch_time=0 elapsed_time=10.7563G shuffle_compress_time=4.19964G shuffle_spill_time=0 shuffle_split_time=0 shuffle_write_time=1.91651G . ",
    "url": "/archives/v1.1.1/developers/microbenchmark/#simulate-spark-with-multiple-processes-and-threads",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/#simulate-spark-with-multiple-processes-and-threads"
  },"86": {
    "doc": "v1.1.1/Micro Benchmarks for Velox Backend",
    "title": "v1.1.1/Micro Benchmarks for Velox Backend",
    "content": " ",
    "url": "/archives/v1.1.1/developers/microbenchmark/",
    
    "relUrl": "/archives/v1.1.1/developers/microbenchmark/"
  },"87": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Environment",
    "content": "Now gluten supports Ubuntu20.04, Ubuntu22.04, centos8, centos7 and macOS. ",
    "url": "/archives/v1.1.1/developers/newto/#environment",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#environment"
  },"88": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Openjdk8",
    "content": "Environment setting . For root user, the environment variables file is /etc/profile, it will make effect for all the users. For other user, you can set in ~/.bashrc. Guide for ubuntu . The default JDK version in ubuntu is java11, we need to set to java8. apt install openjdk-8-jdk update-alternatives --config java java -version . --config java to config java executable path, javac and other commands can also use this command to config. For some other uses, we suggest to set JAVA_HOME. export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # pay attention to $PATH double quote export PATH=\"$PATH:$JAVA_HOME/bin\" . Must set PATH with double quote in ubuntu. ",
    "url": "/archives/v1.1.1/developers/newto/#openjdk8",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#openjdk8"
  },"89": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Maven 3.6.3 or above",
    "content": "Maven Dowload Page And then set the environment setting. ",
    "url": "/archives/v1.1.1/developers/newto/#maven-363-or-above",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#maven-363-or-above"
  },"90": {
    "doc": "v1.1.1/New To Gluten",
    "title": "GCC 9.4 or above",
    "content": " ",
    "url": "/archives/v1.1.1/developers/newto/#gcc-94-or-above",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#gcc-94-or-above"
  },"91": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Compile gluten using debug mode",
    "content": "If you want to just debug java/scala code, there is no need to compile cpp code with debug mode. You can just refer to build-gluten-with-velox-backend. If you need to debug cpp code, please compile the backend code and gluten cpp code with debug mode. ## compile velox backend with benchmark and tests to debug gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . If you need to debug the tests in /gluten-ut, You need to compile java code with `-P spark-ut`. ",
    "url": "/archives/v1.1.1/developers/newto/#compile-gluten-using-debug-mode",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#compile-gluten-using-debug-mode"
  },"92": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Java/scala code development with Intellij",
    "content": " ",
    "url": "/archives/v1.1.1/developers/newto/#javascala-code-development-with-intellij",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#javascala-code-development-with-intellij"
  },"93": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Linux intellij local debug",
    "content": "Install the linux intellij version, and debug code locally. | Ask your linux maintainer to install the desktop, and then restart the server. | If you use Moba-XTerm to connect linux server, you don’t need to install x11 server, If not (e.g. putty), please follow this guide: X11 Forwarding: Setup Instructions for Linux and Mac . | Download intellij linux community version to linux server | Start Idea, bash &lt;idea_dir&gt;/idea.sh | . Notes: Sometimes, your desktop may stop accidently, left idea running. root@xx2:~bash idea-IC-221.5787.30/bin/idea.sh Already running root@xx2:~ps ux | grep intellij root@xx2:kill -9 &lt;pid&gt; . And then restart idea. ",
    "url": "/archives/v1.1.1/developers/newto/#linux-intellij-local-debug",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#linux-intellij-local-debug"
  },"94": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Windows/Mac intellij remote debug",
    "content": "If you have Ultimate intellij, you can try to debug remotely. ",
    "url": "/archives/v1.1.1/developers/newto/#windowsmac-intellij-remote-debug",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#windowsmac-intellij-remote-debug"
  },"95": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Set up gluten project",
    "content": ". | Make sure you have compiled gluten. | Load the gluten by File-&gt;Open, select &lt;gluten_home/pom.xml&gt;. | Activate your profiles such as , and Reload Maven Project, you will find all your need modules have been activated. | Create breakpoint and debug as you wish, maybe you can try CTRL+N to find TestOperator to start your test. | . ",
    "url": "/archives/v1.1.1/developers/newto/#set-up-gluten-project",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#set-up-gluten-project"
  },"96": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Java/Scala code style",
    "content": "Intellij IDE supports importing settings for Java/Scala code style. You can import intellij-codestyle.xml to your IDE. See Intellij guide. To generate a fix for Java/Scala code style, you can run one or more of the below commands according to the code modules involved in your PR. For Velox backend: . mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.3 -Pspark-ut -DskipTests . For Clickhouse backend: . mvn spotless:apply -Pbackends-clickhouse -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-clickhouse -Pspark-3.3 -Pspark-ut -DskipTests . ",
    "url": "/archives/v1.1.1/developers/newto/#javascala-code-style",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#javascala-code-style"
  },"97": {
    "doc": "v1.1.1/New To Gluten",
    "title": "CPP code development with Visual Studio Code",
    "content": "This guide is for remote debug. We will connect the remote linux server by SSH. Download the windows vscode software The important leftside bar is: . | Explorer (Project structure) | Search | Run and Debug | Extensions (Install C/C++ Extension Pack, Remote Development, GitLens at least, C++ Test Mate is also suggested) | Remote Explorer (Connect linux server by ssh command, click +, then input ssh user@10.1.7.003) | Manage (Settings) | . Input your password in the above pop-up window, it will take a few minutes to install linux vscode server in remote machine folder ~/.vscode-server If download failed, delete this folder and try again. ",
    "url": "/archives/v1.1.1/developers/newto/#cpp-code-development-with-visual-studio-code",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#cpp-code-development-with-visual-studio-code"
  },"98": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Usage",
    "content": "Set up project . File-&gt;Open Folder // select gluten folder Select cpp/CmakeList.txt as command prompt Select gcc version as command prompt . Settings . VSCode support 2 ways to set user setting. | Manage-&gt;Command Palette(Open settings.json, search by Preferences: Open Settings (JSON)) | Manage-&gt;Settings (Common setting) | . Build by vscode . VSCode will try to compile the debug version in /build. And we need to compile velox debug mode before, if you have compiled velox release mode, you just need to do. # Build the velox debug version in &lt;velox_home&gt;/_build/debug make debug EXTRA_CMAKE_FLAGS=\"-DVELOX_ENABLE_PARQUET=ON -DENABLE_HDFS=ON -DVELOX_BUILD_TESTING=OFF -DVELOX_ENABLE_DUCKDB=ON -DVELOX_BUILD_TEST_UTILS=ON\" . Then gluten will link velox debug library. Just click build in bottom bar, you will get intellisense search and link. Debug . The default compile command does not enable test and benchmark, so we cannot get any executable file Open the file in &lt;gluten_home&gt;/.vscode/settings.json (create if not exists) . { \"cmake.configureArgs\": [ \"-DBUILD_BENCHMARKS=ON\", \"-DBUILD_TESTS=ON\" ], \"C_Cpp.default.configurationProvider\": \"ms-vscode.cmake-tools\" } . Then we can get some executables, take velox_shuffle_writer_test as example . Click Run and Debug to create launch.json in &lt;gluten_home&gt;/.vscode/launch.json Click Add Configuration in the top of launch.json, select gdb launch or attach to exists program launch.json example . { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"velox shuffle writer test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/build/velox/tests/velox_shuffle_writer_test\", \"args\": [\"--gtest_filter=*TestSinglePartPartitioner*\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] }, { \"name\": \"benchmark test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/./generic_benchmark\", \"args\": [\"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/query.json\", \"--threads=1\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] } ] } . Change name, program, args to yours . Then you can create breakpoint and debug in Run and Debug section. Velox debug . For some velox tests such as ParquetReaderTest, tests need to read the parquet file in &lt;velox_home&gt;/velox/dwio/parquet/tests/examples, you should let the screen on ParquetReaderTest.cpp, then click Start Debuging, otherwise you will raise No such file or directory exception . ",
    "url": "/archives/v1.1.1/developers/newto/#usage",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#usage"
  },"99": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Usefule notes",
    "content": "Upgrade vscode . No need to upgrade vscode version, if upgraded, will download linux server again, switch update mode to off Search update in Manage-&gt;Settings to turn off update mode . Colour setting . \"workbench.colorTheme\": \"Quiet Light\", \"files.autoSave\": \"afterDelay\", \"workbench.colorCustomizations\": { \"editor.wordHighlightBackground\": \"#063ef7\", // \"editor.selectionBackground\": \"#d1d1c6\", // \"tab.activeBackground\": \"#b8b9988c\", \"editor.selectionHighlightBackground\": \"#c5293e\" }, . Clang format . Now gluten uses clang-format 12 to format source files. apt-get install clang-format-12 . Set config in settings.json . \"clang-format.executable\": \"clang-format-12\", \"editor.formatOnSave\": true, . If exists multiple clang-format version, formatOnSave may not take effect, specify the default formatter Search default formatter in Settings, select Clang-Format. If your formatOnSave still make no effect, you can use shortcut SHIFT+ALT+F to format one file mannually. ",
    "url": "/archives/v1.1.1/developers/newto/#usefule-notes",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#usefule-notes"
  },"100": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Debug cpp code with coredump",
    "content": "mkdir -p /mnt/DP_disk1/core sysctl -w kernel.core_pattern=/mnt/DP_disk1/core/core-%e-%p-%t cat /proc/sys/kernel/core_pattern # set the core file to unlimited size echo \"ulimit -c unlimited\" &gt;&gt; ~/.bashrc # then you will get the core file at `/mnt/DP_disk1/core` when the program crashes # gdb -c corefile # gdb &lt;gluten_home&gt;/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . ‘core-Executor task l-2000883-1671542526’ is the generated core file name. (gdb) bt (gdb) f7 (gdb) set print pretty on (gdb) p *this . | Get the backtrace | Switch to 7th stack | Print the variable in a more readable way | Print the variable fields | . Sometimes you only get the cpp exception message, you can generate core dump file by the following code: . char* p = nullptr; *p = 'a'; . or by the following commands: . | gcore &lt;pid&gt; | kill -s SIGSEGV &lt;pid&gt; | . ",
    "url": "/archives/v1.1.1/developers/newto/#debug-cpp-code-with-coredump",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#debug-cpp-code-with-coredump"
  },"101": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Debug cpp with gdb",
    "content": "You can use gdb to debug tests and benchmarks. And also you can debug jni call. Place the following code to your debug path. pid_t pid = getpid(); printf(\"----------------------------------pid: %lun\", pid); sleep(10); . You can also get the pid by java command or grep java program when executing unit test. jps 1375551 ScalaTestRunner ps ux | grep TestOperator . Execute gdb command to debug: . gdb attach &lt;pid&gt; . gdb attach 1375551 wait to attach.... (gdb) b &lt;velox_home&gt;/velox/substrait/SubstraitToVeloxPlan.cpp:577 (gdb) c . ",
    "url": "/archives/v1.1.1/developers/newto/#debug-cpp-with-gdb",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#debug-cpp-with-gdb"
  },"102": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Run TPC-H and TPC-DS",
    "content": "We supply &lt;gluten_home&gt;/tools/gluten-it to execute these queries Refer to velox_be.yml . ",
    "url": "/archives/v1.1.1/developers/newto/#run-tpc-h-and-tpc-ds",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#run-tpc-h-and-tpc-ds"
  },"103": {
    "doc": "v1.1.1/New To Gluten",
    "title": "Run gluten+velox on clean machine",
    "content": "We can run gluten + velox on clean machine by one command (supported OS: Ubuntu20.04/22.04, Centos 7/8, etc.). spark-shell --name run_gluten \\ --master yarn --deploy-mode client \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --jars https://github.com/oap-project/gluten/releases/download/v1.0.0/gluten-velox-bundle-spark3.2_2.12-ubuntu_20.04_x86_64-1.0.0.jar \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager . ",
    "url": "/archives/v1.1.1/developers/newto/#run-glutenvelox-on-clean-machine",
    
    "relUrl": "/archives/v1.1.1/developers/newto/#run-glutenvelox-on-clean-machine"
  },"104": {
    "doc": "v1.1.1/New To Gluten",
    "title": "v1.1.1/New To Gluten",
    "content": "Help users to debug and test with gluten. For intel internal developer, you could refer to internal wiki New Employee Guide to get more information such as proxy settings, Gluten has cpp code and java/scala code, we can use some useful IDE to read and debug. ",
    "url": "/archives/v1.1.1/developers/newto/",
    
    "relUrl": "/archives/v1.1.1/developers/newto/"
  },"105": {
    "doc": "v1.1.1/Substrait Modifications",
    "title": "Substrait Modifications in Gluten",
    "content": "Substrait is a project aiming to create a well-defined, cross-language specification for data compute operations. Since it is still under active development, there are some lacking representations for Gluten needed computing operations. At the same time, some existing representations need to be modified a bit to satisfy the needs of computing. In Gluten, the base version of Substrait is v0.23.0. This page records all the Gluten changes to Substrait proto files for reference. It is preferred to upstream these changes to Substrait, but for those cannot be upstreamed, alternatives like AdvancedExtension could be considered. ",
    "url": "/archives/v1.1.1/developers/substrait/#substrait-modifications-in-gluten",
    
    "relUrl": "/archives/v1.1.1/developers/substrait/#substrait-modifications-in-gluten"
  },"106": {
    "doc": "v1.1.1/Substrait Modifications",
    "title": "Modifications to algebra.proto",
    "content": ". | Added JsonReadOptions and TextReadOptions in FileOrFiles(#1584). | Changed join type JOIN_TYPE_SEMI to JOIN_TYPE_LEFT_SEMI and JOIN_TYPE_RIGHT_SEMI(#408). | Added WindowRel, added column_name and window_type in WindowFunction, changed Unbounded in WindowFunction into Unbounded_Preceding and Unbounded_Following, and added WindowType(#485). | Added output_schema in RelRoot(#1901). | Added ExpandRel(#1361). | Added GenerateRel(#574). | Added PartitionColumn in LocalFiles(#2405). | Added WriteRel (#3690). | . ",
    "url": "/archives/v1.1.1/developers/substrait/#modifications-to-algebraproto",
    
    "relUrl": "/archives/v1.1.1/developers/substrait/#modifications-to-algebraproto"
  },"107": {
    "doc": "v1.1.1/Substrait Modifications",
    "title": "Modifications to type.proto",
    "content": ". | Added Nothing in Type(#791). | Added names in Struct(#1878). | Added PartitionColumns in NamedStruct(#320). | Remove PartitionColumns and add column_types in NamedStruct(#2405). | . ",
    "url": "/archives/v1.1.1/developers/substrait/#modifications-to-typeproto",
    
    "relUrl": "/archives/v1.1.1/developers/substrait/#modifications-to-typeproto"
  },"108": {
    "doc": "v1.1.1/Substrait Modifications",
    "title": "v1.1.1/Substrait Modifications",
    "content": " ",
    "url": "/archives/v1.1.1/developers/substrait/",
    
    "relUrl": "/archives/v1.1.1/developers/substrait/"
  },"109": {
    "doc": "v1.1.1/Docker script for CentOS 7",
    "title": "v1.1.1/Docker script for CentOS 7",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on CentOS 7: . Run on host as root user: . docker pull centos:7 docker run -itd --name gluten centos:7 /bin/bash docker attach gluten . Run in docker: . yum -y install epel-release centos-release-scl yum -y install \\ git \\ dnf \\ cmake3 \\ devtoolset-9 \\ java-1.8.0-openjdk \\ java-1.8.0-openjdk-devel \\ ninja-build \\ wget \\ sudo # gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin # cmake 3.x is required ln -s /usr/bin/cmake3 /usr/local/bin/cmake # enable gcc 9 . /opt/rh/devtoolset-9/enable || exit 1 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # If you have the same error with issue-3283, you need to add the parameter `--compile_arrow_java=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/archives/v1.1.1/developers/docker_centos7/",
    
    "relUrl": "/archives/v1.1.1/developers/docker_centos7/"
  },"110": {
    "doc": "v1.1.1/Docker script for CentOS 8",
    "title": "v1.1.1/Docker script for CentOS 8",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on Centos8: . Run on host as root user: . docker pull centos:8 docker run -itd --name gluten centos:8 /bin/bash docker attach gluten . Run in docker: . #update mirror sed -i -e \"s|mirrorlist=|#mirrorlist=|g\" /etc/yum.repos.d/CentOS-* sed -i -e \"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\" /etc/yum.repos.d/CentOS-* dnf install -y epel-release sudo yum install -y dnf-plugins-core yum config-manager --set-enabled powertools dnf --enablerepo=powertools install -y ninja-build dnf --enablerepo=powertools install -y libdwarf-devel dnf install -y --setopt=install_weak_deps=False ccache gcc-toolset-9 git wget which libevent-devel \\ openssl-devel re2-devel libzstd-devel lz4-devel double-conversion-devel \\ curl-devel cmake libicu-devel source /opt/rh/gcc-toolset-9/enable || exit 1 yum install -y java-1.8.0-openjdk-devel patch export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH #gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/archives/v1.1.1/developers/docker_centos8",
    
    "relUrl": "/archives/v1.1.1/developers/docker_centos8"
  },"111": {
    "doc": "v1.1.1/Docker script for Ubuntu 22.04/20.04",
    "title": "v1.1.1/Docker script for Ubuntu 22.04/20.04",
    "content": "To the first build, it’s suggested to build Gluten in a clean docker image. Otherwise it’s easy to run into library version conflict issues. Here is a docker script we verified to build Gluten+Velox backend on Ubuntu22.04/20.04: . Run on host as root user: . docker pull ubuntu:22.04 docker run -itd --network host --name gluten ubuntu:22.04 /bin/bash docker attach gluten . Run in docker: . apt-get update #install gcc and libraries to build arrow apt install software-properties-common apt install maven build-essential cmake libssl-dev libre2-dev libcurl4-openssl-dev clang lldb lld libz-dev git ninja-build uuid-dev autoconf-archive curl zip unzip tar pkg-config bison libtool flex vim #velox script needs sudo to install dependency libraries apt install sudo # make sure jemalloc is uninstalled, jemalloc will be build in vcpkg, which conflicts with the default jemalloc in system apt purge libjemalloc-dev libjemalloc2 librust-jemalloc-sys-dev #make sure jdk8 is used. New version of jdk is not supported apt install -y openjdk-8-jdk apt install -y default-jdk export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH #manually install tzdata to avoid the interactive timezone config ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata dpkg --configure -a #setup proxy on necessary #export http_proxy=xxxx #export https_proxy=xxxx #clone gluten git clone https://github.com/oap-project/gluten.git cd gluten/ #config maven proxy #mkdir ~/.m2/ #vim ~/.m2/settings.xml # the script download velox &amp; arrow and compile all dependency library automatically # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # It's suggested to build using static link, enabled by `--enable_vcpkg=ON` # For developer, it's suggested to enable Debug info, by --build_type=RelWithDebInfo. Note RelWithDebInfo uses -o2, release uses -o3 ./dev/buildbundle-veloxbe.sh --enable_vcpkg=ON --build_type=RelWithDebInfo . ",
    "url": "/archives/v1.1.1/developers/docker_ubuntu/",
    
    "relUrl": "/archives/v1.1.1/developers/docker_ubuntu/"
  },"112": {
    "doc": "v1.1.1/Developers",
    "title": "Gluten Developers",
    "content": "This document provides a developer overview of the project and covers the following topics: . ",
    "url": "/archives/v1.1.1/developers/#gluten-developers",
    
    "relUrl": "/archives/v1.1.1/developers/#gluten-developers"
  },"113": {
    "doc": "v1.1.1/Developers",
    "title": "v1.1.1/Developers",
    "content": " ",
    "url": "/archives/v1.1.1/developers/",
    
    "relUrl": "/archives/v1.1.1/developers/"
  },"114": {
    "doc": "v1.1.1/Velox Function Development",
    "title": "Developer Guide for Implementing Spark Built-in SQL Functions in Velox",
    "content": "In velox, two folders prestosql &amp; sparksql are holding most sql functions, respective for presto and spark. Gluten will ask velox to firstly register prestosql functions, then sparksql functions. So if prestosql and sparksql share same signature for a function, the sparksql function will overwrite the corresponding prestosql function. If the required function is lacking in both folders (exceptions are some common functions defined outside, like cast), we need to implement the missing function in sparksql folder. It is possible that a prestosql function has some semantic difference with the corresponding spark function, even though they share the same name and function signature. If so, we also need to do an implementation in sparksql folder, generally based on the original impl. for prestosql. There are a few spark functions that can behave differently for some special cases, depending on ANSI on or off. Currently, gluten does NOT support ANSI mode. So only ANSI off needs to be considered in implementing spark built-in functions in velox. Take BitwiseAndFunction as example: . template &lt;typename T&gt; struct BitwiseAndFunction { template &lt;typename TInput&gt; // For void return type, it indicates null result will never be obtained for non-null input. // For bool return type, it indicates null result can be obtained for non-null input (false for null). FOLLY_ALWAYS_INLINE void call(TInput&amp; result, TInput a, TInput b) { result = a &amp; b; } }; . It is templated, as well as the call function, to allow multiple types. In the above impl., the result will be null for null input. Please use callNullable if you need different behavior for null input, e.g., get a non-null result for null input. Also see callNullFree in velox document. It is used for fast evaluation in the case that any input has null. The below code will register the implemented function for all kinds of integer types. The specified name bitwise_and will be actually used in calling this function. registerBinaryIntegral&lt;BitwiseAndFunction&gt;({prefix + \"bitwise_and\"}); . Functions for complex types have similar implementations. See ArrayAverageFunction in velox/functions/prestosql/ArrayFunctions.h. Reference: . Velox’s official developer guide: . | velox/docs/develop/scalar-functions.rst | velox/examples/SimpleFunctions.cpp | . ",
    "url": "/archives/v1.1.1/developers/velox_function_development/#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox",
    
    "relUrl": "/archives/v1.1.1/developers/velox_function_development/#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox"
  },"115": {
    "doc": "v1.1.1/Velox Function Development",
    "title": "v1.1.1/Velox Function Development",
    "content": " ",
    "url": "/archives/v1.1.1/developers/velox_function_development/",
    
    "relUrl": "/archives/v1.1.1/developers/velox_function_development/"
  },"116": {
    "doc": "v1.1.1/ClickHouse Backend",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.1.1/docs/clickhouse/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.1.1/docs/clickhouse/#gluten-documents-by-version"
  },"117": {
    "doc": "v1.1.1/ClickHouse Backend",
    "title": "v1.1.1/ClickHouse Backend",
    "content": " ",
    "url": "/archives/v1.1.1/docs/clickhouse/",
    
    "relUrl": "/archives/v1.1.1/docs/clickhouse/"
  },"118": {
    "doc": "v1.1.1/Getting Started with ClickHouse Backend",
    "title": "ClickHouse Backend",
    "content": "ClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP), which supports best in the industry query performance, while significantly reducing storage requirements through its innovative use of columnar storage and compression. We port ClickHouse ( based on version 23.1 ) as a library, called ‘libch.so’, and Gluten loads this library through JNI as the native engine. In this way, we don’t need to deploy a standalone ClickHouse Cluster, Spark uses Gluten as SparkPlugin to read and write ClickHouse MergeTree data. Architecture . The architecture of the ClickHouse backend is shown below: . | On Spark driver, Spark uses Gluten SparkPlugin to transform the physical plan to the Substrait plan, and then pass the Substrait plan to ClickHouse backend through JNI call on executors. | Based on Spark DataSource V2 interface, implementing a ClickHouse Catalog to support operating the ClickHouse tables, and then using Delta to save some metadata about ClickHouse like the MergeTree parts information, and also provide ACID transactions. | When querying from a ClickHouse table, it will fetch MergeTree parts information from Delta metadata and assign these parts into Spark partitions according to some strategies. | When writing data into a ClickHouse table, it will use ClickHouse library to write MergeTree parts data and collect these MergeTree parts information after writing successfully, and then save these MergeTree parts information into Delta metadata. ( The feature of writing MergeTree parts is coming soon. ) | On Spark executors, each executor will load the ‘libch.so’ through JNI when starting, and then call the operators according to the Substrait plan which is passed from Spark Driver, like reading data from the MergeTree parts, writing the MergeTree parts, filtering data, aggregating data and so on. | Currently, the ClickHouse backend only supports reading the MergeTree parts from local storage, it needs to use a high-performance shared file system to share a root bucket on every node of the cluster from the object storage, like JuiceFS. | . Development environment setup . In general, we use IDEA for Gluten development and CLion for ClickHouse backend development on Ubuntu 20. Prerequisites . Install the software required for compilation, run sudo ./ep/build-clickhouse/src/install_ubuntu.sh. Under the hood, it will install the following software: . | Clang 16.0 | cmake 3.20 or higher version | ninja-build 1.8.2 | . You can also refer to How-to-Build-ClickHouse-on-Linux. You need to install the following software manually: . | Java 8 | Maven 3.6.3 or higher version | Spark 3.2.2 or Spark 3.3.1 | . Then, get Gluten code: . git clone https://github.com/oap-project/gluten.git . Setup ClickHouse backend development environment . If you don’t care about development environment, you can skip this part. Otherwise, do: . | clone Kyligence/ClickHouse repo cd /to/some/place/ git clone --recursive --shallow-submodules -b clickhouse_backend https://github.com/Kyligence/ClickHouse.git . | Configure cpp-ch ${GLUTEN_SOURCE}/cpp-ch can be treated as an add-on of Kyligence/Clickhouse . First, initialize some configuration for this add-on: . export GLUTEN_SOURCE=/path/to/gluten export CH_SOURCE_DIR=/path/to/ClickHouse cmake -G Ninja -S ${GLUTEN_SOURCE}/cpp-ch -B ${GLUTEN_SOURCE}/cpp-ch/build_ch -DCH_SOURCE_DIR=${CH_SOURCE_DIR} \"-DCMAKE_C_COMPILER=$(command -v clang-16)\" \"-DCMAKE_CXX_COMPILER=$(command -v clang++-16)\" \"-DCMAKE_BUILD_TYPE=RelWithDebInfo\" . Next, you need to compile Kyligence/Clickhouse. There are two options: . | (Option 1) Use CLion . | Open ClickHouse repo | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; Toolchains, and then choose Bundled CMake, clang-16 as C Compiler, clang++-16 as C++ Compiler: . | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; CMake: . And then add these options into CMake options: . -DENABLE_PROTOBUF=ON -DENABLE_TESTS=OFF -DENABLE_JEMALLOC=ON -DENABLE_MULTITARGET_CODE=ON -DENABLE_EXTERN_LOCAL_ENGINE=ON . | Build ‘ch’ target on ClickHouse Project with Debug mode or Release mode: . If it builds with Release mode successfully, there is a library file called ‘libch.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-release/utils/extern-local-engine/’. If it builds with Debug mode successfully, there is a library file called ‘libchd.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-debug/utils/extern-local-engine/’. | . | (Option 2) Use command line cmake --build ${GLUTEN_SOURCE}/cpp-ch/build_ch --target build_ch . If it builds successfully, there is a library file called ‘libch.so’ in path ‘${GLUTEN_SOURCE}/cpp-ch/build/utils/extern-local-engine/’. | . Directly Compile ClickHouse backend . In case you don’t want a develop environment, you can use the following command to compile ClickHouse backend directly: . git clone https://github.com/oap-project/gluten.git cd gluten bash ./ep/build-clickhouse/src/build_clickhouse.sh . This will download Clickhouse for you and build everything. The target file is /path/to/gluten/cpp-ch/build/utils/extern-local-engine/libch.so. Compile Gluten . The prerequisites are the same as the one mentioned above. Compile Gluten with ClickHouse backend through maven: . | for Spark 3.2.2 | . git clone https://github.com/oap-project/gluten.git cd gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.2 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.2-jar-with-dependencies.jar . | for Spark 3.3.1 | . git clone https://github.com/oap-project/gluten.git cd gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.3 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.3-jar-with-dependencies.jar . Gluten in local Spark Thrift Server . Prepare working directory . | for Spark 3.2.2 | . tar zxf spark-3.2.2-bin-hadoop2.7.tgz cd spark-3.2.2-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.0.1.jar and delta-storage-2.0.1.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.0.1/delta-core_2.12-2.0.1.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.0.1/delta-storage-2.0.1.jar -P ./jars cp gluten-XXXXX-spark-3.2-jar-with-dependencies.jar jars/ . | for Spark 3.3.1 | . tar zxf spark-3.3.1-bin-hadoop2.7.tgz cd spark-3.3.1-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.2.0.jar and delta-storage-2.2.0.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar -P ./jars cp gluten-XXXXX-spark-3.3-jar-with-dependencies.jar jars/ . Query local data . Start Spark Thriftserver on local . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master local[3] \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 #connect to Spark Thriftserver by beeline bin/beeline -u jdbc:hive2://localhost:10000/ -n root . Query local MergeTree files . | Prepare data | . Currently, the feature of writing ClickHouse MergeTree parts by Spark is developing, so you need to use command ‘clickhouse-local’ to generate MergeTree parts data manually. We provide a python script to call the command ‘clickhouse-local’ to convert parquet data to MergeTree parts: . #install ClickHouse community version sudo apt-get install -y apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754 echo \"deb https://packages.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt install -y --allow-downgrades clickhouse-server=22.5.1.2079 clickhouse-client=22.5.1.2079 clickhouse-common-static=22.5.1.2079 #generate MergeTree parts mkdir -p /path_clickhouse_database/table_path/ python3 /path_to_clickhouse_backend_src/utils/local-engine/tool/parquet_to_mergetree.py --path=/tmp --source=/path_to_parquet_data/tpch-data-sf100/lineitem --dst=/path_clickhouse_database/table_path/lineitem . This python script will convert one parquet data file to one MergeTree parts. | Create a TPC-H lineitem table using ClickHouse DataSource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION '/path_clickhouse_database/table_path/lineitem'; . | TPC-H Q6 test | . SELECT sum(l_extendedprice * l_discount) AS revenue FROM lineitem WHERE l_shipdate &gt;= date'1994-01-01' AND l_shipdate &lt; date'1994-01-01' + interval 1 year AND l_discount BETWEEN 0.06 - 0.01 AND 0.06 + 0.01 AND l_quantity &lt; 24; . | Result . The DAG is shown on Spark UI as below: . | . Query local Parquet files . You can query local parquet files directly. -- query on a single file select * from parquet.`/your_data_root_dir/1.parquet`; -- query on a directly which has multiple files select * from parquet.`/your_data_roo_dir/`; . You can also create a TEMPORARY VIEW for parquet files. create or replace temporary view your_table_name using org.apache.spark.sql.parquet options( path \"/your_data_root_dir/\" ) . Query Parquet files in S3 . If you have parquet files in S3(either AWS S3 or S3 compatible storages like MINIO), you can query them directly. You need to add these additional configs to spark: . --config spark.hadoop.fs.s3a.endpoint=S3_ENDPOINT --config spark.hadoop.fs.s3a.path.style.access=true --config spark.hadoop.fs.s3a.access.key=YOUR_ACCESS_KEY --config spark.hadoop.fs.s3a.secret.key=YOUR_SECRET_KEY . where S3_ENDPOINT must follow the format of https://s3.region-code.amazonaws.com, e.g. https://s3.us-east-1.amazonaws.com (or `http://hostname:39090 for MINIO) . When you query the parquet files in S3, you need to add the prefix s3a:// to the path, e.g. s3a://your_bucket_name/path_to_your_parquet. Additionally, you can add these configs to enable local caching of S3 data. Each spark executor will have its own cache. Cache stealing between executors is not supported yet. --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.enabled=true --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.cache_path=/executor_local_folder_for_cache . Use beeline to execute queries . After start a spark thriftserver, we can use the beeline to connect to this server. # run a file /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -f &lt;your_sql_file&gt; # run a query /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -e '&lt;your_sql&gt;' # enter a interactive mode /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 . Query Hive tables in HDFS . Suppose that you have set up hive and hdfs, you can query the data on hive directly. | Copy hive-site.xml into /path_to_spark/conf/ | Copy hdfs-site.xml into /path_to_spark/conf/, and edit spark-env.sh | . # add this line into spark-env.sh export HADOOP_CONF_DIR=/path_to_spark/conf . | Start spark thriftserver with hdfs configurations | . hdfs_conf_file=/your_local_path/hdfs-site.xml cd spark-3.2.2-bin-hadoop2.7 # add a new option: spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf ./sbin/start-thriftserver.sh \\ --master local[3] \\ --files $hdfs_conf_file \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=./hdfs-site.xml . For example, you have a table demo_database.demo_table on the hive, you can run queries as below. select * from demo_database.demo_talbe; . Gluten in YARN cluster . We can to run a Spark SQL task by gluten on a yarn cluster as following . #!/bin/bash # The file contains the sql you want to run sql_file=/path/to/spark/sql/file export SPARK_HOME=/path/to/spark/home spark_cmd=$SPARK_HOME/bin/spark-sql # Define the path to libch.so ch_lib=/path/to/libch.so export LD_PRELOAD=$ch_lib # copy gluten jar file to $SPARK_HOME/jar gluten_jar=/path/to/gluten/jar/file cp $gluten_jar $SPARK_HOME/jar batchsize=20480 hdfs_conf=/path/to/hdfs-site.xml $spark_cmd \\ --name gluten_on_yarn --master yarn \\ --deploy-mode client \\ --files $ch_lib \\ --executor-cores 1 \\ --num-executors 2 \\ --executor-memory 10g \\ --conf spark.default.parallelism=4 \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=7g \\ --conf spark.driver.maxResultSize=2g \\ --conf spark.sql.autoBroadcastJoinThreshold=-1 \\ --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} \\ --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} \\ --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} \\ --conf spark.sql.broadcastTimeout=4800 \\ --conf spark.task.maxFailures=1 \\ --conf spark.excludeOnFailure.enabled=false \\ --conf spark.driver.maxResultSize=4g \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.dynamicAllocation.executorIdleTimeout=0s \\ --conf spark.sql.shuffle.partitions=112 \\ --conf spark.sql.sources.useV1SourceList=avro \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.gluten.sql.columnar.columnartorow=true \\ --conf spark.gluten.sql.columnar.loadnative=true \\ --conf spark.gluten.sql.columnar.libpath=$ch_lib \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=$hdfs_conf \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.logger.level=debug \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.executorEnv.LD_PRELOAD=$LD_PRELOAD \\ --conf spark.hadoop.input.connect.timeout=600000 \\ --conf spark.hadoop.input.read.timeout=600000 \\ --conf spark.hadoop.input.write.timeout=600000 \\ --conf spark.hadoop.dfs.client.log.severity=\"DEBUG2\" \\ --files $ch_lib \\ -f $sql_file . We also can use spark-submit to run a task. Benchmark with TPC-H 100 Q6 on Gluten with ClickHouse backend . This benchmark is tested on AWS EC2 cluster, there are 7 EC2 instances: . | Node Role | EC2 Type | Instances Count | Resources | AMI | . | Master | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . | Worker | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . Deploy on Cloud . | Tested on Spark Standalone cluster, its resources are shown below: . |   | CPU cores | Memory | Instances Count | . | Spark Worker | 15 | 60G | 6 | . | Prepare jars . Refer to Deploy Spark 3.2.2 . | Deploy gluten-core-XXXXX-jar-with-dependencies.jar . | . #deploy 'gluten-core-XXXXX-jar-with-dependencies.jar' to every node, and then cp gluten-core-XXXXX-jar-with-dependencies.jar /path_to_spark/jars/ . | Deploy ClickHouse library . Deploy ClickHouse library ‘libch.so’ to every worker node. | . Deploy JuiceFS . | JuiceFS uses Redis to save metadata, install redis firstly: | . wget https://download.redis.io/releases/redis-6.0.14.tar.gz sudo apt install build-essential tar -zxvf redis-6.0.14.tar.gz cd redis-6.0.14 make make install PREFIX=/home/ubuntu/redis6 cd .. rm -rf redis-6.0.14 #start redis server /home/ubuntu/redis6/bin/redis-server /home/ubuntu/redis6/redis.conf . | Use JuiceFS to format a S3 bucket and mount a volumn on every node . Please refer to The-JuiceFS-Command-Reference . | . wget https://github.com/juicedata/juicefs/releases/download/v0.17.5/juicefs-0.17.5-linux-amd64.tar.gz tar -zxvf juicefs-0.17.5-linux-amd64.tar.gz ./juicefs format --block-size 4096 --storage s3 --bucket https://s3.cn-northwest-1.amazonaws.com.cn/s3-gluten-tpch100/ --access-key \"XXXXXXXX\" --secret-key \"XXXXXXXX\" redis://:123456@master-ip:6379/1 gluten-tables #mount a volumn on every node ./juicefs mount -d --no-usage-report --no-syslog --attr-cache 7200 --entry-cache 7200 --dir-entry-cache 7200 --buffer-size 500 --prefetch 1 --open-cache 86400 --log /home/ubuntu/juicefs-logs/mount1.log --cache-dir /home/ubuntu/juicefs-cache/ --cache-size 102400 redis://:123456@master-ip:6379/1 /home/ubuntu/gluten/gluten_table #create a directory for lineitem table path mkdir -p /home/ubuntu/gluten/gluten_table/lineitem . Preparation . Please refer to Data-preparation to generate MergeTree parts data to the lineitem table path: /home/ubuntu/gluten/gluten_table/lineitem. Run Spark Thriftserver . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master spark://master-ip:7070 --deploy-mode client \\ --driver-memory 16g --driver-cores 4 \\ --total-executor-cores 90 --executor-memory 60g --executor-cores 15 \\ --conf spark.driver.memoryOverhead=8G \\ --conf spark.default.parallelism=90 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=536870912 \\ --conf spark.sql.parquet.filterPushdown=true \\ --conf spark.sql.parquet.enableVectorizedReader=true \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=42949672960 \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 . Test TPC-H Q6 with JMeter . | Create a lineitem table using clickhouse datasource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION 'file:///home/ubuntu/gluten/gluten_table/lineitem'; . | Run TPC-H Q6 test with JMeter . | Run TPC-H Q6 test 100 times in the first round; | Run TPC-H Q6 test 1000 times in the second round; | . | . Performance . The performance of Gluten + ClickHouse backend increases by about 1/3. |   | 70% | 80% | 90% | 99% | Avg | . | Spark + Parquet | 590ms | 592ms | 597ms | 609ms | 588ms | . | Spark + Gluten + ClickHouse backend | 402ms | 405ms | 409ms | 425ms | 399ms | . New CI System . https://opencicd.kyligence.com/job/Gluten/job/gluten-ci/ public read-only account：gluten/hN2xX3uQ4m . Celeborn support . Gluten with clickhouse backend has not yet supportted Celeborn natively as remote shuffle service using columar shuffle. However, you can still use Celeborn with row shuffle, which means a ColumarBatch will be converted to a row during shuffle. Below introduction is used to enable this feature: . First refer to this URL(https://github.com/apache/incubator-celeborn) to setup a celeborn cluster. Then add the Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-0.3.0-incubating.jar | . Currently to use Celeborn following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.celeborn.SparkShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/incubator-celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . Celeborn Columnar Shuffle Support . The native Celeborn support can be enabled by the following configuration . spark.shuffle.manager=org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager . quickly start a celeborn cluster . wget https://dlcdn.apache.org/incubator/celeborn/celeborn-0.3.0-incubating/apache-celeborn-0.3.0-incubating-bin.tgz &amp;&amp; \\ tar -zxvf apache-celeborn-0.3.0-incubating-bin.tgz &amp;&amp; \\ mv apache-celeborn-0.3.0-incubating-bin/conf/celeborn-defaults.conf.template apache-celeborn-0.3.0-incubating-bin/conf/celeborn-defaults.conf &amp;&amp; \\ mv apache-celeborn-0.3.0-incubating-bin/conf/log4j2.xml.template apache-celeborn-0.3.0-incubating-bin/conf/log4j2.xml &amp;&amp; \\ mkdir /opt/hadoop &amp;&amp; chmod 777 /opt/hadoop &amp;&amp; \\ echo -e \"celeborn.worker.flusher.threads 4\\nceleborn.worker.storage.dirs /tmp\\nceleborn.worker.monitor.disk.enabled false\" &gt; apache-celeborn-0.3.0-incubating-bin/conf/celeborn-defaults.conf &amp;&amp; \\ bash apache-celeborn-0.3.0-incubating-bin/sbin/start-master.sh &amp;&amp; bash apache-celeborn-0.3.0-incubating-bin/sbin/start-worker.sh . Columnar shuffle mode . We have two modes of columnar shuffle . | prefer cache | prefer spill | . Switch through the configuration spark.gluten.sql.columnar.backend.ch.shuffle.preferSpill, the default is false, enable prefer cache shuffle. In the prefer cache mode, as much memory as possible will be used to cache the shuffle data. When the memory is insufficient, spark will actively trigger the memory spill. You can also specify the threshold size through spark.gluten.sql.columnar.backend.ch.spillThreshold to Limit memory usage. The default value is 0MB, which means no limit on memory usage. ",
    "url": "/archives/v1.1.1/docs/clickhouse/getting-started/#clickhouse-backend",
    
    "relUrl": "/archives/v1.1.1/docs/clickhouse/getting-started/#clickhouse-backend"
  },"119": {
    "doc": "v1.1.1/Getting Started with ClickHouse Backend",
    "title": "v1.1.1/Getting Started with ClickHouse Backend",
    "content": " ",
    "url": "/archives/v1.1.1/docs/clickhouse/getting-started/",
    
    "relUrl": "/archives/v1.1.1/docs/clickhouse/getting-started/"
  },"120": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Supported Version",
    "content": "| Type | Version |——-|——————————| Spark | 3.2.2, 3.3.1 | OS | Ubuntu20.04/22.04, Centos7/8 | jdk | openjdk8 | scala | 2.12 . Spark3.4.0 support is still WIP. TPCH/DS can pass, UT is not yet passed. There are pending PRs for jdk11 support. Currently, the mvn script can automatically fetch and build all dependency libraries incluing Velox. Our nightly build still use Velox under oap-project. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#supported-version",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#supported-version"
  },"121": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Prerequisite",
    "content": "Currently, Gluten+Velox backend is only tested on Ubuntu20.04/Ubuntu22.04/Centos8. Other kinds of OS support are still in progress. The long term goal is to support several common OS and conda env deployment. Gluten builds with Spark3.2.x and Spark3.3.x now but only fully tested in CI with 3.2.2 and 3.3.1. We will add/update supported/tested versions according to the upstream changes. we need to set up the JAVA_HOME env. Currently, java 8 is required and the support for java 11/17 is not ready. For x86_64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH . For aarch64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64 export PATH=$JAVA_HOME/bin:$PATH . Get gluten . ## config maven, like proxy in ~/.m2/settings.xml ## fetch gluten code git clone https://github.com/oap-project/gluten.git . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#prerequisite",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#prerequisite"
  },"122": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Build Gluten with Velox Backend",
    "content": "It’s recommended to use buildbundle-veloxbe.sh to build gluten in one script. Gluten build guide listed the parameters and their default value of build command for your reference. For x86_64 build . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh ## After a complete build, if you need to re-build the project and only some gluten code is changed, ## you can use the following command to skip building velox and protobuf. # ./dev/buildbundle-veloxbe.sh --enable_ep_cache=ON --build_protobuf=OFF ## If you have the same error with issue-3283, you need to add the parameter `--compile_arrow_java=ON` . For aarch64 build: . export CPU_TARGET=\"aarch64\" cd /path/to/gluten ./dev/builddeps-veloxbe.sh . Build Velox separately . Scripts under /path/to/gluten/ep/build-velox/src provide get_velox.sh and build_velox.sh to build Velox separately, you could use these scripts with custom repo/branch/location. Velox provides arrow/parquet lib. Gluten cpp module need a required VELOX_HOME parsed by –velox_home, if you specify custom ep location, make sure these variables be passed correctly. ## fetch Velox and compile cd /path/to/gluten/ep/build-velox/src/ ## you could use custom ep location by --velox_home=custom_path, make sure specify --velox_home in build_velox.sh too./get_velox.sh ## make sure specify --velox_home if you have specified it in get_velox.sh./build_velox.sh ## compile Gluten cpp module cd /path/to/gluten/cpp ## if you use custom velox_home, make sure specified here by --velox_home ./compile.sh --build_velox_backend=ON ## compile Gluten java module and create package jar cd /path/to/gluten # For spark3.2.x mvn clean package -Pbackends-velox -Prss -Pspark-3.2 -DskipTests # For spark3.3.x mvn clean package -Pbackends-velox -Prss -Pspark-3.3 -DskipTests . notes：The compilation of Velox using the script of build_velox.sh may fail caused by oom, you can prevent this failure by using the user command of export NUM_THREADS=4 before executing the above scripts. Once building successfully, the Jar file will be generated in the directory: package/target/&lt;gluten-jar&gt; for Spark 3.2.x/Spark 3.3.x. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-velox-backend"
  },"123": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Dependency library deployment",
    "content": "With config enable_vcpkg=ON, the dependency libraries will be built and statically linked into libvelox.so and libgluten.so, which is packed into the gluten-jar. In this way, only the gluten-jar is needed to add to spark.&lt;driver|executor&gt;.extraClassPath and spark will deploy the jar to each worker node. It’s better to build the static version using a clean docker image without any extra libraries installed. On host with some libraries like jemalloc installed, the script may crash with odd message. You may need to uninstall those libraries to get a clean host. With config enable_vcpkg=OFF, the dependency libraries won’t be statically linked, instead the script will install the libraries to system then pack the dependency libraries into another jar named gluten-package-${Maven-artifact-version}.jar. Then you need to add the jar to extraClassPath then set spark.gluten.loadLibFromJar=true. Or you already manually deployed the dependency libraries on each worker node. You may find the libraries list from the gluten-package jar. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#dependency-library-deployment",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#dependency-library-deployment"
  },"124": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "HDFS support",
    "content": "Hadoop hdfs support is ready via the libhdfs3 library. The libhdfs3 provides native API for Hadoop I/O without the drawbacks of JNI. It also provides advanced authentication like Kerberos based. Please note this library has several dependencies which may require extra installations on Driver and Worker node. Build with HDFS support . To build Gluten with HDFS support, below command is suggested: . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_hdfs=ON . Configuration about HDFS support . HDFS uris (hdfs://host:port) will be extracted from a valid hdfs file path to initialize hdfs client, you do not need to specify it explicitly. libhdfs3 need a configuration file and example here, this file is a bit different from hdfs-site.xml and core-site.xml. Download that example config file to local and do some needed modifications to support HA or else, then set env variable like below to use it, or upload it to HDFS to use, more details here. // Spark local mode export LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode --conf spark.executorEnv.LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode and upload hdfs config file cp /path/to/hdfs-client.xml hdfs-client.xml --files hdfs-client.xml . One typical deployment on Spark/HDFS cluster is to enable short-circuit reading. Short-circuit reads provide a substantial performance boost to many applications. By default libhdfs3 does not set the default hdfs domain socket path to support HDFS short-circuit read. If this feature is required in HDFS setup, users may need to setup the domain socket path correctly by patching the libhdfs3 source code or by setting the correct config environment. In Gluten the short-circuit domain socket path is set to “/var/lib/hadoop-hdfs/dn_socket” in build_velox.sh So we need to make sure the folder existed and user has write access as below script. sudo mkdir -p /var/lib/hadoop-hdfs/ sudo chown &lt;sparkuser&gt;:&lt;sparkuser&gt; /var/lib/hadoop-hdfs/ . You also need to add configuration to the “hdfs-site.xml” as below: . &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt; . Kerberos support . Here are two steps to enable kerberos. | Make sure the hdfs-client.xml contains | . &lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;/property&gt; . | Specify the environment variable KRB5CCNAME and upload the kerberos ticket cache file | . --conf spark.executorEnv.KRB5CCNAME=krb5cc_0000 --files /tmp/krb5cc_0000 . The ticket cache file can be found by klist. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#hdfs-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#hdfs-support"
  },"125": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "AWS S3 support",
    "content": "Velox supports S3 with the open source AWS C++ SDK and Gluten uses Velox S3 connector to connect with S3. A new build option for S3(enable_s3) is added. Below command is used to enable this feature . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_s3=ON . Currently there are several ways to asscess S3 in Spark. Please refer Velox S3 part for more detailed configurations . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#aws-s3-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#aws-s3-support"
  },"126": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Celeborn support",
    "content": "Gluten with velox backend supports Celeborn as remote shuffle service. Below introduction is used to enable this feature . First refer to this URL(https://github.com/apache/incubator-celeborn) to setup a celeborn cluster. When compiling the Gluten Java module, it’s required to enable rss profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Prss -DskipTests . Then add the Gluten and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-0.3.0-incubating.jar | Gluten: gluten-velox-bundle-spark3.x_2.12-xx_xx_xx-SNAPSHOT.jar, gluten-thirdparty-lib-xx-xx.jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/incubator-celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#celeborn-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#celeborn-support"
  },"127": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "DeltaLake Support",
    "content": "Gluten with velox backend supports DeltaLake table. How to use . First of all, compile gluten-delta module by a delta profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Pdelta -DskipTests . Then, put the additional gluten-delta jar to the class path (usually it’s $SPARK_HOME/jars). The gluten-delta jar is in gluten-delta/target directory. After the two steps, you can query delta table by gluten/velox without scan’s fallback. Gluten with velox backends also support the column mapping of delta tables. About column mapping, see more here. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#deltalake-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#deltalake-support"
  },"128": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Iceberg Support",
    "content": "Gluten with velox backend supports Iceberg table. Currently, only reading COW (Copy-On-Write) tables is supported. How to use . First of all, compile gluten-iceberg module by a iceberg profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Piceberg -DskipTests . Then, put the additional gluten-iceberg jar to the class path (usually it’s $SPARK_HOME/jars). The gluten-iceberg jar is in gluten-iceberg/target directory. After the two steps, you can query iceberg table by gluten/velox without scan’s fallback. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#iceberg-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#iceberg-support"
  },"129": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Coverage",
    "content": "Spark3.3 has 387 functions in total. ~240 are commonly used. Velox’s functions have two category, Presto and Spark. Presto has 124 functions implemented. Spark has 62 functions. Spark functions are verified to have the same result as Vanilla Spark. Some Presto functions have the same result as Vanilla Spark but some others have different. Gluten prefer to use Spark functions firstly. If it’s not in Spark’s list but implemented in Presto, we currently offload to Presto one until we noted some result mismatch, then we need to reimplement the function in Spark category. Gluten currently offloads 94 functions and 14 operators, more details refer to Velox Backend’s Supported Operators &amp; Functions. Velox doesn’t support ANSI mode), so as Gluten. Once ANSI mode is enabled in Spark config, Gluten will fallback to Vanilla Spark. To identify what can be offloaded in a query and detailed fallback reasons, user can follow below steps to retrieve corresponding logs. 1) Enable Gluten by proper [configuration](https://github.com/oap-project/gluten/blob/main/docs/Configuration.md). 2) Disable Spark AQE to trigger plan validation in Gluten spark.sql.adaptive.enabled = false 3) Check physical plan sparkSession.sql(\"your_sql\").explain() . With above steps, you will get a physical plan output like: . == Physical Plan == -Execute InsertIntoHiveTable (7) +- Coalesce (6) +- VeloxColumnarToRowExec (5) +- ^ ProjectExecTransformer (3) +- GlutenRowToArrowColumnar (2) +- Scan hive default.extracted_db_pins (1) . GlutenRowToArrowColumnar/VeloxColumnarToRowExec indicates there is a fallback operator before or after it. And you may find fallback reason like below in logs. native validation failed due to: in ProjectRel, Scalar function name not registered: get_struct_field, called with arguments: (ROW&lt;col_0:INTEGER,col_1:BIGINT,col_2:BIGINT&gt;, INTEGER). In the above, the symbol ^ indicates a plan is offloaded to Velox in a stage. In Spark DAG, all such pipelined plans (consecutive plans marked with ^) are plotted inside an umbrella node named WholeStageCodegenTransformer (It’s not codegen node. The naming is just for making it well plotted like Spark Whole Stage Codegen). ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#coverage",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#coverage"
  },"130": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Spill (Experimental)",
    "content": "Velox backend supports spilling-to-disk. Using the following configuration options to customize spilling: . | Name | Default Value | Description | . | spark.gluten.sql.columnar.backend.velox.spillStrategy | auto | none: Disable spill on Velox backend; auto: Let Spark memory manager manage Velox’s spilling | . | spark.gluten.sql.columnar.backend.velox.spillFileSystem | local | The filesystem used to store spill data. local: The local file system. heap-over-local: Write files to JVM heap if having extra heap space. Otherwise write to local file system. | . | spark.gluten.sql.columnar.backend.velox.aggregationSpillEnabled | true | Whether spill is enabled on aggregations | . | spark.gluten.sql.columnar.backend.velox.joinSpillEnabled | true | Whether spill is enabled on joins | . | spark.gluten.sql.columnar.backend.velox.orderBySpillEnabled | true | Whether spill is enabled on sorts | . | spark.gluten.sql.columnar.backend.velox.aggregationSpillMemoryThreshold | 0 | Memory limit before spilling to disk for aggregations, per Spark task. Unit: byte | . | spark.gluten.sql.columnar.backend.velox.joinSpillMemoryThreshold | 0 | Memory limit before spilling to disk for joins, per Spark task. Unit: byte | . | spark.gluten.sql.columnar.backend.velox.orderBySpillMemoryThreshold | 0 | Memory limit before spilling to disk for sorts, per Spark task. Unit: byte | . | spark.gluten.sql.columnar.backend.velox.maxSpillLevel | 4 | The max allowed spilling level with zero being the initial spilling level | . | spark.gluten.sql.columnar.backend.velox.maxSpillFileSize | 20MB | The max allowed spill file size. If it is zero, then there is no limit | . | spark.gluten.sql.columnar.backend.velox.minSpillRunSize | 268435456 | The min spill run size limit used to select partitions for spilling | . | spark.gluten.sql.columnar.backend.velox.spillStartPartitionBit | 29 | The start partition bit which is used with ‘spillPartitionBits’ together to calculate the spilling partition number | . | spark.gluten.sql.columnar.backend.velox.spillPartitionBits | 2 | The number of bits used to calculate the spilling partition number. The number of spilling partitions will be power of two | . | spark.gluten.sql.columnar.backend.velox.spillableReservationGrowthPct | 25 | The spillable memory reservation growth percentage of the previous memory reservation size | . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#spill-experimental",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#spill-experimental"
  },"131": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Velox User-Defined Functions (UDF)",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#velox-user-defined-functions-udf",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#velox-user-defined-functions-udf"
  },"132": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Introduction",
    "content": "Velox backend supports User-Defined Functions (UDF). Users can create their own functions using the UDF interface provided in Velox backend and build libraries for these functions. At runtime, the UDF are registered at the start of applications. Once registered, Gluten will be able to parse and offload these UDF into Velox during execution. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#introduction",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#introduction"
  },"133": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Creating a UDF library",
    "content": "The following steps demonstrate how to set up a UDF library project: . | Include the UDF Interface Header: First, include the UDF interface header file Udf.h in the project file. The header file defines the UdfEntry struct, along with the macros for declaring the necessary functions to integrate the UDF into Gluten and Velox. | Implement the UDF: Implement UDF. These functions should be able to register to Velox. | Implement the Interface Functions: Implement the following interface functions that integrate UDF into Project Gluten: . | getNumUdf(): This function should return the number of UDF in the library. This is used to allocating udfEntries array as the argument for the next function getUdfEntries. | getUdfEntries(gluten::UdfEntry* udfEntries): This function should populate the provided udfEntries array with the details of the UDF, including function names and return types. | registerUdf(): This function is called to register the UDF to Velox function registry. This is where users should register functions by calling facebook::velox::exec::registerVecotorFunction or other Velox APIs. | The interface functions are mapping to marcos in Udf.h. Here’s an example of how to implement these functions: . | . // Filename MyUDF.cpp #include &lt;velox/expression/VectorFunction.h&gt; #include &lt;velox/udf/Udf.h&gt; const int kNumMyUdf = 1; gluten::UdfEntry myUdf[kNumMyUdf] = myudf1; class MyUdf : public facebook::velox::exec::VectorFunction { ... // Omit concrete implementation } static std::vector&lt;std::shared_ptr&lt;exec::FunctionSignature&gt;&gt; myUdfSignatures() { return {facebook::velox::exec::FunctionSignatureBuilder() .returnType(myUdf[0].dataType) .argumentType(\"integer\") .build()}; } DEFINE_GET_NUM_UDF { return kNumMyUdf; } DEFINE_GET_UDF_ENTRIES { udfEntries[0] = myUdf[0]; } DEFINE_REGISTER_UDF { facebook::velox::exec::registerVectorFunction( myUdf[0].name, myUdfSignatures(), std::make_unique&lt;MyUdf&gt;()); } . | . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#creating-a-udf-library",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#creating-a-udf-library"
  },"134": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Building the UDF library",
    "content": "To build the UDF library, users need to compile the C++ code and link to libvelox.so. It’s recommended to create a CMakeLists.txt for the project. Here’s an example: . project(myudf) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(GLUTEN_HOME /path/to/gluten) add_library(myudf SHARED \"MyUDF.cpp\") find_library(VELOX_LIBRARY REQUIRED NAMES velox HINTS ${GLUTEN_HOME}/cpp/build/releases NO_DEFAULT_PATH) target_include_directories(myudf PRIVATE ${GLUTEN_HOME}/cpp ${GLUTEN_HOME}/ep/build-velox/build/velox_ep) target_link_libraries(myudf PRIVATE ${VELOX_LIBRARY}) . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#building-the-udf-library",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#building-the-udf-library"
  },"135": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Using UDF in Gluten",
    "content": "Gluten loads the UDF libraries at runtime. You can upload UDF libraries via --files or --archives, and configure the libray paths using the provided Spark configuration, which accepts comma separated list of library paths. Note if running on Yarn client mode, the uploaded files are not reachable on driver side. Users should copy those files to somewhere reachable for driver and set spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths. This configuration is also useful when the udfLibraryPaths is different between driver side and executor side. | Use --files --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/libmyudf.so . | Use --archives --archives /path/to/udf_archives.zip#udf_archives --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=udf_archives # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/udf_archives.zip . | Specify URI | . You can also specify the local or HDFS URIs to the UDF libraries or archives. Local URIs should exist on driver and every worker nodes. --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=hdfs://path/to/library_or_archive . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#using-udf-in-gluten",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#using-udf-in-gluten"
  },"136": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Try the example",
    "content": "We provided an Velox UDF example file MyUDF.cpp. After building gluten cpp, you can find the example library at /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so . Start spark-shell or spark-sql with below configuration . --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so . Run query. The functions myudf1 and myudf2 increment the input value by a constant of 5 . select myudf1(1), myudf2(100L) . The output from spark-shell will be like . +----------------+------------------+ |udfexpression(1)|udfexpression(100)| +----------------+------------------+ | 6| 105| +----------------+------------------+ . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#try-the-example",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#try-the-example"
  },"137": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "High-Bandwidth Memory (HBM) support",
    "content": "Gluten supports allocating memory on HBM. This feature is optional and is disabled by default. It is implemented on top of Memkind library. You can refer to memkind’s readme for more details. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#high-bandwidth-memory-hbm-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#high-bandwidth-memory-hbm-support"
  },"138": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Build Gluten with HBM",
    "content": "Gluten will internally build and link to a specific version of Memkind library and hwloc. Other dependencies should be installed on Driver and Worker node first: . sudo apt install -y autoconf automake g++ libnuma-dev libtool numactl unzip libdaxctl-dev . After the set-up, you can now build Gluten with HBM. Below command is used to enable this feature . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh --enable_hbm=ON . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-hbm",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-hbm"
  },"139": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Configure and enable HBM in Spark Application",
    "content": "At runtime, MEMKIND_HBW_NODES enviroment variable is detected for configuring HBM NUMA nodes. For the explaination to this variable, please refer to memkind’s manual page. This can be set for all executors through spark conf, e.g. --conf spark.executorEnv.MEMKIND_HBW_NODES=8-15. Note that memory allocation fallback is also supported and cannot be turned off. If HBM is unavailable or fills up, the allocator will use default(DDR) memory. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#configure-and-enable-hbm-in-spark-application",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#configure-and-enable-hbm-in-spark-application"
  },"140": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Intel® QuickAssist Technology (QAT) support",
    "content": "Gluten supports using Intel® QuickAssist Technology (QAT) for data compression during Spark Shuffle. It benefits from QAT Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on QAT driver library and QATzip library. Please manually download QAT driver for your system, and follow its README to build and install on all Driver and Worker node: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#intel-quickassist-technology-qat-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#intel-quickassist-technology-qat-support"
  },"141": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Software Requirements",
    "content": ". | Download QAT driver for your system, and follow its README to build and install on all Driver and Worker nodes: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. | Below compression libraries need to be installed on all Driver and Worker nodes: . | Zlib* library of version 1.2.7 or higher | ZSTD* library of version 1.5.4 or higher | LZ4* library | . | . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#software-requirements",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#software-requirements"
  },"142": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Build Gluten with QAT",
    "content": ". | Setup ICP_ROOT environment variable to the directory where QAT driver is extracted. This environment variable is required during building Gluten and running Spark applications. It’s recommended to put it in .bashrc on Driver and Worker nodes. | . echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc source ~/.bashrc # Also set for root if running as non-root user sudo su - echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc exit . | This step is required if your application is running as Non-root user. The users must be added to the ‘qat’ group after QAT drvier is installed. And change the amount of max locked memory for the username that is included in the group name. This can be done by specifying the limit in /etc/security/limits.conf. | . sudo su - usermod -aG qat username # need relogin to take effect # To set 500MB add a line like this in /etc/security/limits.conf echo \"@qat - memlock 500000\" &gt;&gt; /etc/security/limits.conf exit . | Enable huge page. This step is required to execute each time after system reboot. We recommend using systemctl to manage at system startup. You change the values for “max_huge_pages” and “max_huge_pages_per_process” to make sure there are enough resources for your workload. As for Spark applications, one process matches one executor. Within the executor, every task is allocated a maximum of 5 huge pages. | . sudo su - cat &lt;&lt; EOF &gt; /usr/local/bin/qat_startup.sh #!/bin/bash echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages rmmod usdm_drv insmod $ICP_ROOT/build/usdm_drv.ko max_huge_pages=1024 max_huge_pages_per_process=32 EOF chmod +x /usr/local/bin/qat_startup.sh cat &lt;&lt; EOF &gt; /etc/systemd/system/qat_startup.service [Unit] Description=Configure QAT [Service] ExecStart=/usr/local/bin/qat_startup.sh [Install] WantedBy=multi-user.target EOF systemctl enable qat_startup.service systemctl start qat_startup.service # setup immediately systemctl status qat_startup.service exit . | After the setup, you are now ready to build Gluten with QAT. Use the command below to enable this feature: | . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh --enable_qat=ON . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-qat",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-qat"
  },"143": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Enable QAT with Gzip/Zstd for shuffle compression",
    "content": ". | To offload shuffle compression into QAT, first make sure you have the right QAT configuration file at /etc/4xxx_devX.conf. We provide a example configuration file. This configuration sets up to 4 processes that can bind to 1 QAT, and each process can use up to 16 QAT DC instances. | . ## run as root ## Overwrite QAT configuration file. cd /etc for i in {0..7}; do echo \"4xxx_dev$i.conf\"; done | xargs -i cp -f /path/to/gluten/docs/qat/4x16.conf {} ## Restart QAT after updating configuration files. adf_ctl restart . | Check QAT status and make sure the status is up | . adf_ctl status . The output should be like: . Checking status of all devices. There is 8 QAT acceleration device(s) in the system: qat_dev0 - type: 4xxx, inst_id: 0, node_id: 0, bsf: 0000:6b:00.0, #accel: 1 #engines: 9 state: up qat_dev1 - type: 4xxx, inst_id: 1, node_id: 1, bsf: 0000:70:00.0, #accel: 1 #engines: 9 state: up qat_dev2 - type: 4xxx, inst_id: 2, node_id: 2, bsf: 0000:75:00.0, #accel: 1 #engines: 9 state: up qat_dev3 - type: 4xxx, inst_id: 3, node_id: 3, bsf: 0000:7a:00.0, #accel: 1 #engines: 9 state: up qat_dev4 - type: 4xxx, inst_id: 4, node_id: 4, bsf: 0000:e8:00.0, #accel: 1 #engines: 9 state: up qat_dev5 - type: 4xxx, inst_id: 5, node_id: 5, bsf: 0000:ed:00.0, #accel: 1 #engines: 9 state: up qat_dev6 - type: 4xxx, inst_id: 6, node_id: 6, bsf: 0000:f2:00.0, #accel: 1 #engines: 9 state: up qat_dev7 - type: 4xxx, inst_id: 7, node_id: 7, bsf: 0000:f7:00.0, #accel: 1 #engines: 9 state: up . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip # Valid options are gzip and zstd --conf spark.gluten.sql.columnar.shuffle.codecBackend=qat . | You can use below command to check whether QAT is working normally at run-time. The value of fw_counters should continue to increase during shuffle. | . while :; do cat /sys/kernel/debug/qat_4xxx_0000:6b:00.0/fw_counters; sleep 1; done . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#enable-qat-with-gzipzstd-for-shuffle-compression",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#enable-qat-with-gzipzstd-for-shuffle-compression"
  },"144": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "QAT driver references",
    "content": "Documentation . README Text Files (README_QAT20.L.1.0.0-00021.txt) . Release Notes . Check out the Intel® QuickAssist Technology Software for Linux* - Release Notes for the latest changes in this release. Getting Started Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Getting Started Guide for detailed installation instructions. Programmer’s Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Programmer’s Guide for software usage guidelines. For more Intel® QuickAssist Technology resources go to Intel® QuickAssist Technology (Intel® QAT) . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#qat-driver-references",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#qat-driver-references"
  },"145": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Intel® In-memory Analytics Accelerator (IAA/IAX) support",
    "content": "Similar to Intel® QAT, Gluten supports using Intel® In-memory Analytics Accelerator (IAA, also called IAX) for data compression during Spark Shuffle. It benefits from IAA Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on Intel® QPL. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#intel-in-memory-analytics-accelerator-iaaiax-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#intel-in-memory-analytics-accelerator-iaaiax-support"
  },"146": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Build Gluten with IAA",
    "content": "Gluten will internally build and link to a specific version of QPL library, but extra environment setup is still required. Please refer to QPL Installation Guide to install dependencies and configure accelerators. This step is required if your application is running as Non-root user. Create a group for the users who have privilege to use IAA, and grant group iaa read/write access to the IAA Work-Queues. sudo groupadd iaa sudo usermod -aG iaa username # need to relogin sudo chgrp -R iaa /dev/iax sudo chmod -R g+rw /dev/iax . After the set-up, you can now build Gluten with QAT. Below command is used to enable this feature . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh --enable_iaa=ON . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-iaa",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#build-gluten-with-iaa"
  },"147": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Enable IAA with Gzip Compression for shuffle compression",
    "content": ". | To enable QAT at run-time, first make sure you have configured the IAA Work-Queues correctly, and the file permissions of /dev/iax/wqX.0 are correct. | . sudo ls -l /dev/iax . The output should be like: . total 0 crw-rw---- 1 root iaa 509, 0 Apr 5 18:54 wq1.0 crw-rw---- 1 root iaa 509, 5 Apr 5 18:54 wq11.0 crw-rw---- 1 root iaa 509, 6 Apr 5 18:54 wq13.0 crw-rw---- 1 root iaa 509, 7 Apr 5 18:54 wq15.0 crw-rw---- 1 root iaa 509, 1 Apr 5 18:54 wq3.0 crw-rw---- 1 root iaa 509, 2 Apr 5 18:54 wq5.0 crw-rw---- 1 root iaa 509, 3 Apr 5 18:54 wq7.0 crw-rw---- 1 root iaa 509, 4 Apr 5 18:54 wq9.0 . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip --conf spark.gluten.sql.columnar.shuffle.codecBackend=iaa . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#enable-iaa-with-gzip-compression-for-shuffle-compression",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#enable-iaa-with-gzip-compression-for-shuffle-compression"
  },"148": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "IAA references",
    "content": "Intel® IAA Enabling Guide . Check out the Intel® In-Memory Analytics Accelerator (Intel® IAA) Enabling Guide . Intel® QPL Documentation . Check out the Intel® Query Processing Library (Intel® QPL) Documentation . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#iaa-references",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#iaa-references"
  },"149": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Test TPC-H or TPC-DS on Gluten with Velox backend",
    "content": "All TPC-H and TPC-DS queries are supported in Gluten Velox backend. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend"
  },"150": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Data preparation",
    "content": "The data generation scripts are TPC-H dategen script and TPC-DS dategen script. The used TPC-H and TPC-DS queries are the original ones, and can be accessed from TPC-DS queries and TPC-H queries. Some other versions of TPC-DS queries are also provided, but are not recommended for testing, including: . | the modified TPC-DS queries with “Decimal-to-Double”: TPC-DS non-decimal queries (outdated). | . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#data-preparation",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#data-preparation"
  },"151": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Submit the Spark SQL job",
    "content": "Submit test script from spark-shell. You can find the scala code to Run TPC-H as an example. Please remember to modify the location of TPC-H files as well as TPC-H queries before you run the testing. var parquet_file_path = \"/PATH/TO/TPCH_PARQUET_PATH\" var gluten_root = \"/PATH/TO/GLUTEN\" . Below script shows an example about how to run the testing, you should modify the parameters such as executor cores, memory, offHeap size based on your environment. export GLUTEN_JAR = /PATH/TO/GLUTEN/package/target/&lt;gluten-jar&gt; cat tpch_parquet.scala | spark-shell --name tpch_powertest_velox \\ --master yarn --deploy-mode client \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.driver.extraClassPath=${GLUTEN_JAR} \\ --conf spark.executor.extraClassPath=${GLUTEN_JAR} \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --num-executors 6 \\ --executor-cores 6 \\ --driver-memory 20g \\ --executor-memory 25g \\ --conf spark.executor.memoryOverhead=5g \\ --conf spark.driver.maxResultSize=32g . Refer to Gluten configuration for more details. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#submit-the-spark-sql-job",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#submit-the-spark-sql-job"
  },"152": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Result",
    "content": "wholestagetransformer indicates that the offload works. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#result",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#result"
  },"153": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Performance",
    "content": "Below table shows the TPC-H Q1 and Q6 Performance in a multiple-thread test (–num-executors 6 –executor-cores 6) for Velox and vanilla Spark. Both Parquet and ORC datasets are sf1024. | Query Performance (s) | Velox (ORC) | Vanilla Spark (Parquet) | Vanilla Spark (ORC) | . | TPC-H Q6 | 13.6 | 21.6 | 34.9 | . | TPC-H Q1 | 26.1 | 76.7 | 84.9 | . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#performance",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#performance"
  },"154": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "External reference setup",
    "content": "TO ease your first-hand experience of using Gluten, we have set up an external reference cluster. If you are interested, please contact Weiting.Chen@intel.com. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#external-reference-setup",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#external-reference-setup"
  },"155": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Gluten UI",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#gluten-ui",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#gluten-ui"
  },"156": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Gluten event",
    "content": "Gluten provides two events GlutenBuildInfoEvent and GlutenPlanFallbackEvent: . | GlutenBuildInfoEvent, it contains the Gluten build information so that we are able to be aware of the environment when doing some debug. It includes Java Version, Scala Version, GCC Version, Gluten Version, Spark Version, Hadoop Version, Gluten Revision, Backend, Backend Revision, etc. | GlutenPlanFallbackEvent, it contains the fallback information for each query execution. Note, if the query execution is in AQE, then Gluten will post it for each stage. | . Developers can register SparkListener to handle these two Gluten events. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#gluten-event",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#gluten-event"
  },"157": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "SQL tab",
    "content": "Gluten provides a tab based on Spark UI, named Gluten SQL / DataFrame . This tab contains two parts: . | The Gluten build information. | SQL/Dataframe queries fallback information. | . If you want to disable Gluten UI, add a config when submitting --conf spark.gluten.ui.enabled=false. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#sql-tab",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#sql-tab"
  },"158": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "History server",
    "content": "Gluten UI also supports Spark history server. Add gluten-ui jar into the history server classpath, e.g., $SPARK_HOME/jars, then restart history server. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#history-server",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#history-server"
  },"159": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Native plan string",
    "content": "Gluten supports inject native plan string into Spark explain with formatted mode by setting --conf spark.gluten.sql.injectNativePlanStringToExplain=true. Here is an example, how Gluten show the native plan string. (9) WholeStageCodegenTransformer (2) Input [6]: [c1#0L, c2#1L, c3#2L, c1#3L, c2#4L, c3#5L] Arguments: false Native Plan: -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#native-plan-string",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#native-plan-string"
  },"160": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Native plan with stats",
    "content": "Gluten supports print native plan with stats to executor system output stream by setting --conf spark.gluten.sql.debug=true. Note that, the plan string with stats is task level which may cause executor log size big. Here is an example, how Gluten show the native plan string with stats. I20231121 10:19:42.348845 90094332 WholeStageResultIterator.cc:220] Native Plan with stats for: [Stage: 1 TID: 16] -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 10.58us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 queuedWallNanos sum: 2.00us, count: 1, min: 2.00us, max: 2.00us runningAddInputWallNanos sum: 626ns, count: 1, min: 626ns, max: 626ns runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 5.54us, count: 1, min: 5.54us, max: 5.54us -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 223.00us, Blocked wall time: 0ns, Peak memory: 93.12KB, Memory allocations: 15 HashBuild: Input: 10 rows (960B, 10 batches), Output: 0 rows (0B, 0 batches), Cpu time: 185.67us, Blocked wall time: 0ns, Peak memory: 68.00KB, Memory allocations: 2, Threads: 1 distinctKey0 sum: 4, count: 1, min: 4, max: 4 hashtable.capacity sum: 4, count: 1, min: 4, max: 4 hashtable.numDistinct sum: 10, count: 1, min: 10, max: 10 hashtable.numRehashes sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 4, count: 1, min: 4, max: 4 runningAddInputWallNanos sum: 1.27ms, count: 1, min: 1.27ms, max: 1.27ms runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 1.29us, count: 1, min: 1.29us, max: 1.29us H23/11/21 10:19:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 13) in 335 ms on 10.221.97.35 (executor driver) (1/10) ashProbe: Input: 9 rows (864B, 3 batches), Output: 27 rows (3.56KB, 3 batches), Cpu time: 37.33us, Blocked wall time: 0ns, Peak memory: 25.12KB, Memory allocations: 13, Threads: 1 dynamicFiltersProduced sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningAddInputWallNanos sum: 4.54us, count: 1, min: 4.54us, max: 4.54us runningFinishWallNanos sum: 83ns, count: 1, min: 83ns, max: 83ns runningGetOutputWallNanos sum: 29.08us, count: 1, min: 29.08us, max: 29.08us -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT Input: 9 rows (864B, 3 batches), Output: 9 rows (864B, 3 batches), Cpu time: 630.75us, Blocked wall time: 0ns, Peak memory: 2.44KB, Memory allocations: 63, Threads: 1, Splits: 3 dataSourceWallNanos sum: 102.00us, count: 1, min: 102.00us, max: 102.00us dynamicFiltersAccepted sum: 1, count: 1, min: 1, max: 1 flattenStringDictionaryValues sum: 0, count: 1, min: 0, max: 0 ioWaitNanos sum: 312.00us, count: 1, min: 312.00us, max: 312.00us localReadBytes sum: 0B, count: 1, min: 0B, max: 0B numLocalRead sum: 0, count: 1, min: 0, max: 0 numPrefetch sum: 0, count: 1, min: 0, max: 0 numRamRead sum: 0, count: 1, min: 0, max: 0 numStorageRead sum: 6, count: 1, min: 6, max: 6 overreadBytes sum: 0B, count: 1, min: 0B, max: 0B prefetchBytes sum: 0B, count: 1, min: 0B, max: 0B queryThreadIoLatency sum: 12, count: 1, min: 12, max: 12 ramReadBytes sum: 0B, count: 1, min: 0B, max: 0B runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 125ns, count: 1, min: 125ns, max: 125ns runningGetOutputWallNanos sum: 1.07ms, count: 1, min: 1.07ms, max: 1.07ms skippedSplitBytes sum: 0B, count: 1, min: 0B, max: 0B skippedSplits sum: 0, count: 1, min: 0, max: 0 skippedStrides sum: 0, count: 1, min: 0, max: 0 storageReadBytes sum: 3.44KB, count: 1, min: 3.44KB, max: 3.44KB totalScanTime sum: 0ns, count: 1, min: 0ns, max: 0ns -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Input: 0 rows (0B, 0 batches), Output: 10 rows (960B, 10 batches), Cpu time: 1.03ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 54.62us, count: 1, min: 54.62us, max: 54.62us runningGetOutputWallNanos sum: 1.10ms, count: 1, min: 1.10ms, max: 1.10ms . ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#native-plan-with-stats",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#native-plan-with-stats"
  },"161": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "Gluten Implicits",
    "content": "Gluten provides a helper class to get the fallback summary from a Spark Dataset. import org.apache.spark.sql.execution.GlutenImplicits._ val df = spark.sql(\"SELECT * FROM t\") df.fallbackSummary . Note that, if AQE is enabled, but the query is not materialized, then it will re-plan the query execution with disabled AQE. It is a workaround to get the final plan, and it may cause the inconsistent results with a materialized query. However, we have no choice. ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/#gluten-implicits",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/#gluten-implicits"
  },"162": {
    "doc": "v1.1.1/Getting-Started with Velox Backend",
    "title": "v1.1.1/Getting-Started with Velox Backend",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/getting-started/",
    
    "relUrl": "/archives/v1.1.1/docs/velox/getting-started/"
  },"163": {
    "doc": "v1.1.1/Velox Backend",
    "title": "Gluten Velox Backend Documents",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/#gluten-velox-backend-documents",
    
    "relUrl": "/archives/v1.1.1/docs/velox/#gluten-velox-backend-documents"
  },"164": {
    "doc": "v1.1.1/Velox Backend",
    "title": "v1.1.1/Velox Backend",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/",
    
    "relUrl": "/archives/v1.1.1/docs/velox/"
  },"165": {
    "doc": "v1.1.1/Build Parameters for Velox Backend",
    "title": "Build Parameters",
    "content": "Native build parameters for buildbundle-veloxbe.sh or builddeps-veloxbe.sh . Please set them via --, e.g. --build_type=Release. | Parameters | Description | Default | . | build_type | Build type for Velox &amp; gluten cpp, CMAKE_BUILD_TYPE. | Release | . | build_tests | Build gluten cpp tests. | OFF | . | build_examples | Build udf example. | OFF | . | build_benchmarks | Build gluten cpp benchmarks. | OFF | . | build_jemalloc | Build with jemalloc. | ON | . | build_protobuf | Build protobuf lib. | ON | . | enable_qat | Enable QAT for shuffle data de/compression. | OFF | . | enable_iaa | Enable IAA for shuffle data de/compression. | OFF | . | enable_hbm | Enable HBM allocator. | OFF | . | enable_s3 | Build with S3 support. | OFF | . | enable_gcs | Build with GCs support. | OFF | . | enable_hdfs | Build with HDFS support. | OFF | . | enable_abfs | Build with ABFS support. | OFF | . | enable_ep_cache | Enable caching for external project build (Velox). | OFF | . | enable_vcpkg | Enable vcpkg for static build. | OFF | . | run_setup_script | Run setup script to install Velox dependencies. | ON | . | velox_repo | Specify your own Velox repo to build. | ”” | . | velox_branch | Specify your own Velox branch to build. | ”” | . | velox_home | Specify your own Velox source path to build. | ”” | . | build_velox_tests | Build Velox tests. | OFF | . | build_velox_benchmarks | Build Velox benchmarks (velox_tests and connectors will be disabled if ON) | OFF | . | compile_arrow_java | Compile arrow java for gluten build to use to fix invalid pointer issues. | OFF | . Velox build parameters for build_velox.sh . Please set them via --, e.g., --velox_home=/YOUR/PATH. | Parameters | Description | Default | . | velox_home | Specify Velox source path to build. | GLUTEN_SRC/ep/build-velox/build/velox_ep | . | build_type | Velox build type, i.e., CMAKE_BUILD_TYPE. | Release | . | enable_s3 | Build Velox with S3 support. | OFF | . | enable_gcs | Build Velox with GCS support. | OFF | . | enable_hdfs | Build Velox with HDFS support. | OFF | . | enable_abfs | Build Velox with ABFS support. | OFF | . | run_setup_script | Run setup script to install Velox dependencies before build. | ON | . | enable_ep_cache | Enable and reuse cache of Velox build. | OFF | . | build_test_utils | Build Velox with cmake arg -DVELOX_BUILD_TEST_UTILS=ON if ON. | OFF | . | build_tests | Build Velox test. | OFF | . | build_benchmarks | Build Velox benchmarks. | OFF | . | compile_arrow_java | Build arrow java for gluten build to use to fix invalid pointer issues. | OFF | . Maven build parameters . The below parameters can be set via -P for mvn. | Parameters | Description | Default state | . | backends-velox | Build Gluten Velox backend. | disabled | . | backends-clickhouse | Build Gluten ClickHouse backend. | disabled | . | rss | Build Gluten with Remote Shuffle Service, only applicable for Velox backend. | disabled | . | delta | Build Gluten with Delta Lake support. | disabled | . | iceberg | Build Gluten with Iceberg support. | disabled | . | spark-3.2 | Build Gluten for Spark 3.2. | enabled | . | spark-3.3 | Build Gluten for Spark 3.3. | disabled | . | spark-3.4 | Build Gluten for Spark 3.4. | disabled | . ",
    "url": "/archives/v1.1.1/docs/velox/build_parameters/#build-parameters",
    
    "relUrl": "/archives/v1.1.1/docs/velox/build_parameters/#build-parameters"
  },"166": {
    "doc": "v1.1.1/Build Parameters for Velox Backend",
    "title": "Gluten Jar for Deployment",
    "content": "The gluten jar built out is under GLUTEN_SRC/package/target/. It’s name pattern is gluten-&lt;backend_type&gt;-bundle-spark&lt;spark.bundle.version&gt;_&lt;scala.binary.version&gt;-&lt;os.detected.release&gt;_&lt;os.detected.release.version&gt;-&lt;project.version&gt;.jar. | Spark Version | spark.bundle.version | scala.binary.version | . | 3.2.2 | 3.2 | 2.12 | . | 3.3.1 | 3.3 | 2.12 | . | 3.4.2 | 3.4 | 2.12 | . ",
    "url": "/archives/v1.1.1/docs/velox/build_parameters/#gluten-jar-for-deployment",
    
    "relUrl": "/archives/v1.1.1/docs/velox/build_parameters/#gluten-jar-for-deployment"
  },"167": {
    "doc": "v1.1.1/Build Parameters for Velox Backend",
    "title": "v1.1.1/Build Parameters for Velox Backend",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/build_parameters/",
    
    "relUrl": "/archives/v1.1.1/docs/velox/build_parameters/"
  },"168": {
    "doc": "v1.1.1/Using GCS with Gluten",
    "title": "Working with GCS",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/gcs/#working-with-gcs",
    
    "relUrl": "/archives/v1.1.1/docs/velox/gcs/#working-with-gcs"
  },"169": {
    "doc": "v1.1.1/Using GCS with Gluten",
    "title": "Installing the gcloud CLI",
    "content": "To access GCS Objects using Gluten and Velox, first you have to [download an install the gcloud CLI] (https://cloud.google.com/sdk/docs/install). ",
    "url": "/archives/v1.1.1/docs/velox/gcs/#installing-the-gcloud-cli",
    
    "relUrl": "/archives/v1.1.1/docs/velox/gcs/#installing-the-gcloud-cli"
  },"170": {
    "doc": "v1.1.1/Using GCS with Gluten",
    "title": "Configuring GCS using a user account",
    "content": "This is recommended for regular users, follow the instructions to authorize a user account. After these steps, no specific configuration is required for Gluten, since the authorization was handled entirely by the gcloud tool. ",
    "url": "/archives/v1.1.1/docs/velox/gcs/#configuring-gcs-using-a-user-account",
    
    "relUrl": "/archives/v1.1.1/docs/velox/gcs/#configuring-gcs-using-a-user-account"
  },"171": {
    "doc": "v1.1.1/Using GCS with Gluten",
    "title": "Configuring GCS using a credential file",
    "content": "For workloads that need to be fully automated, manually authorizing can be problematic. For such cases it is better to use a json file with the credentials. This is described in the [instructions to configure a service account]https://cloud.google.com/sdk/docs/authorizing#service-account. Such json file with the credetials can be passed to Gluten: . spark.hadoop.fs.gs.auth.type SERVICE_ACCOUNT_JSON_KEYFILE spark.hadoop.fs.gs.auth.service.account.json.keyfile // path to the json file with the credentials. ",
    "url": "/archives/v1.1.1/docs/velox/gcs/#configuring-gcs-using-a-credential-file",
    
    "relUrl": "/archives/v1.1.1/docs/velox/gcs/#configuring-gcs-using-a-credential-file"
  },"172": {
    "doc": "v1.1.1/Using GCS with Gluten",
    "title": "Configuring GCS endpoints",
    "content": "For cases when a GCS mock is used, an optional endpoint can be provided: . spark.hadoop.fs.gs.storage.root.url // url to the mock gcs service including starting with http or https . ",
    "url": "/archives/v1.1.1/docs/velox/gcs/#configuring-gcs-endpoints",
    
    "relUrl": "/archives/v1.1.1/docs/velox/gcs/#configuring-gcs-endpoints"
  },"173": {
    "doc": "v1.1.1/Using GCS with Gluten",
    "title": "v1.1.1/Using GCS with Gluten",
    "content": "Object stores offered by CSPs such as GCS are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use a GCS endpoint as your data source, please ensure you are using the following GCS configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to GCS with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/archives/v1.1.1/docs/velox/gcs/",
    
    "relUrl": "/archives/v1.1.1/docs/velox/gcs/"
  },"174": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Working with S3",
    "content": " ",
    "url": "/archives/v1.1.1/docs/velox/s3/#working-with-s3",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#working-with-s3"
  },"175": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Configuring S3 endpoint",
    "content": "S3 provides the endpoint based method to access the files, here’s the example configuration. Users may need to modify some values based on real setup. spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider spark.hadoop.fs.s3a.access.key XXXXXXXXX spark.hadoop.fs.s3a.secret.key XXXXXXXXX spark.hadoop.fs.s3a.endpoint https://s3.us-west-1.amazonaws.com spark.hadoop.fs.s3a.connection.ssl.enabled true spark.hadoop.fs.s3a.path.style.access false . ",
    "url": "/archives/v1.1.1/docs/velox/s3/#configuring-s3-endpoint",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#configuring-s3-endpoint"
  },"176": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Configuring S3 instance credentials",
    "content": "S3 also provides other methods for accessing, you can also use instance credentials by setting the following config . spark.hadoop.fs.s3a.use.instance.credentials true . Note that in this case, “spark.hadoop.fs.s3a.endpoint” won’t take affect as Gluten will use the endpoint set during instance creation. ",
    "url": "/archives/v1.1.1/docs/velox/s3/#configuring-s3-instance-credentials",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#configuring-s3-instance-credentials"
  },"177": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Configuring S3 IAM roles",
    "content": "You can also use iam role credentials by setting the following configurations. Instance credentials have higher priority than iam credentials. spark.hadoop.fs.s3a.iam.role xxxx spark.hadoop.fs.s3a.iam.role.session.name xxxx . Note that spark.hadoop.fs.s3a.iam.role.session.name is optional. ",
    "url": "/archives/v1.1.1/docs/velox/s3/#configuring-s3-iam-roles",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#configuring-s3-iam-roles"
  },"178": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Other authentatication methods are not supported yet",
    "content": ". | AWS temporary credential | . ",
    "url": "/archives/v1.1.1/docs/velox/s3/#other-authentatication-methods-are-not-supported-yet",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#other-authentatication-methods-are-not-supported-yet"
  },"179": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Log granularity of AWS C++ SDK in velox",
    "content": "You can change log granularity of AWS C++ SDK by setting the spark.gluten.velox.awsSdkLogLevel configuration. The Allowed values are: . | OFF | FATAL | ERROR | WARN | INFO | DEBUG | TRACE | . ",
    "url": "/archives/v1.1.1/docs/velox/s3/#log-granularity-of-aws-c-sdk-in-velox",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#log-granularity-of-aws-c-sdk-in-velox"
  },"180": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "Local Caching support",
    "content": "Velox supports a local cache when reading data from HDFS/S3. The feature is very useful if remote storage is slow, e.g., reading from a public S3 bucket and stronger performance is desired. With this feature, Velox can asynchronously cache the data on local disk when reading from remote storage, and the future reading requests on already cached blocks will be serviced from local cache files. To enable the local caching feature, below configurations are required: . spark.gluten.sql.columnar.backend.velox.cacheEnabled // enable or disable velox cache, default false. spark.gluten.sql.columnar.backend.velox.memCacheSize // the total size of in-mem cache, default is 128MB. spark.gluten.sql.columnar.backend.velox.ssdCachePath // the folder to store the cache files, default is \"/tmp\". spark.gluten.sql.columnar.backend.velox.ssdCacheSize // the total size of the SSD cache, default is 128MB. Velox will do in-mem cache only if this value is 0. spark.gluten.sql.columnar.backend.velox.ssdCacheShards // the shards of the SSD cache, default is 1. spark.gluten.sql.columnar.backend.velox.ssdCacheIOThreads // the IO threads for cache promoting, default is 1. Velox will try to do \"read-ahead\" if this value is bigger than 1 spark.gluten.sql.columnar.backend.velox.ssdODirect // enable or disable O_DIRECT on cache write, default false. It’s recommended to mount SSDs to the cache path to get the best performance of local caching. On the start up of Spark context, the cache files will be allocated under “spark.gluten.sql.columnar.backend.velox.cachePath”, with UUID based suffix, e.g. “/tmp/cache.13e8ab65-3af4-46ac-8d28-ff99b2a9ec9b0”. Gluten is not able to reuse older caches for now, and the old cache files are left there after Spark context shutdown. ",
    "url": "/archives/v1.1.1/docs/velox/s3/#local-caching-support",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/#local-caching-support"
  },"181": {
    "doc": "v1.1.1/Using S3 with Gluten",
    "title": "v1.1.1/Using S3 with Gluten",
    "content": "Object stores offered by CSPs such as AWS S3 are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use an S3 endpoint as your data source, please ensure you are using the following S3 configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to S3 with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/archives/v1.1.1/docs/velox/s3/",
    
    "relUrl": "/archives/v1.1.1/docs/velox/s3/"
  },"182": {
    "doc": "v1.1.1/Documentation",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.1.1/docs/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.1.1/docs/#gluten-documents-by-version"
  },"183": {
    "doc": "v1.1.1/Documentation",
    "title": "v1.1.1/Documentation",
    "content": " ",
    "url": "/archives/v1.1.1/docs/",
    
    "relUrl": "/archives/v1.1.1/docs/"
  },"184": {
    "doc": "v1.1.1",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.1.1/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.1.1/#gluten-documents-by-version"
  },"185": {
    "doc": "v1.1.1",
    "title": "v1.1.1",
    "content": " ",
    "url": "/archives/v1.1.1/",
    
    "relUrl": "/archives/v1.1.1/"
  },"186": {
    "doc": "Configuration(v1.2.1)",
    "title": "Spark Configurations for Gluten Plugin",
    "content": "There are many configurations could impact the Gluten Plugin performance and can be fine-tuned in Spark. You can add these configurations into spark-defaults.conf to enable or disable the setting. ",
    "url": "/archives/v1.2.1/docs/configuration/#spark-configurations-for-gluten-plugin",
    
    "relUrl": "/archives/v1.2.1/docs/configuration/#spark-configurations-for-gluten-plugin"
  },"187": {
    "doc": "Configuration(v1.2.1)",
    "title": "Spark parameters",
    "content": "| Parameters | Description | Recommend Setting | . | spark.driver.extraClassPath | To add Gluten Plugin jar file in Spark Driver | /path/to/jar_file | . | spark.executor.extraClassPath | To add Gluten Plugin jar file in Spark Executor | /path/to/jar_file | . | spark.executor.memory | To set up how much memory to be used for Spark Executor. |   | . | spark.memory.offHeap.size | To set up how much memory to be used for Java OffHeap. Please notice Gluten Plugin will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Gluten Plugin | 30G | . | spark.sql.sources.useV1SourceList | Choose to use V1 source | avro | . | spark.sql.join.preferSortMergeJoin | To turn off preferSortMergeJoin in Spark | false | . | spark.plugins | To load Gluten’s components by Spark’s plug-in loader | org.apache.gluten.GlutenPlugin | . | spark.shuffle.manager | To turn on Gluten Columnar Shuffle Plugin | org.apache.spark.shuffle.sort.ColumnarShuffleManager | . | spark.gluten.enabled | Enable Gluten, default is true. Just an experimental property. Recommend to enable/disable Gluten through the setting for spark.plugins. | true | . | spark.gluten.memory.isolation | (Experimental) Enable isolated memory mode. If true, Gluten controls the maximum off-heap memory can be used by each task to X, X = executor memory / max task slots. It’s recommended to set true if Gluten serves concurrent queries within a single session, since not all memory Gluten allocated is guaranteed to be spillable. In the case, the feature should be enabled to avoid OOM. Note when true, setting spark.memory.storageFraction to a lower value is suggested since storage memory is considered non-usable by Gluten. | false | . | spark.gluten.ras.enabled | Experimental: Enables RAS (relation algebra selector) during physical planning to generate more efficient query plan. Note, this feature is still in development and may not bring performance profits. | false | . | spark.gluten.sql.columnar.maxBatchSize | Number of rows to be processed in each batch. Default value is 4096. | 4096 | . | spark.gluten.sql.columnar.scanOnly | When enabled, this config will overwrite all other operators’ enabling, and only Scan and Filter pushdown will be offloaded to native. | false | . | spark.gluten.sql.columnar.batchscan | Enable or Disable Columnar BatchScan, default is true | true | . | spark.gluten.sql.columnar.hashagg | Enable or Disable Columnar Hash Aggregate, default is true | true | . | spark.gluten.sql.columnar.project | Enable or Disable Columnar Project, default is true | true | . | spark.gluten.sql.columnar.filter | Enable or Disable Columnar Filter, default is true | true | . | spark.gluten.sql.columnar.sort | Enable or Disable Columnar Sort, default is true | true | . | spark.gluten.sql.columnar.window | Enable or Disable Columnar Window, default is true | true | . | spark.gluten.sql.columnar.shuffledHashJoin | Enable or Disable ShuffledHashJoin, default is true | true | . | spark.gluten.sql.columnar.forceShuffledHashJoin | Force to use ShuffledHashJoin over SortMergeJoin, default is true. For queries that can benefit from storaged patitioned join, please set it to false. | true | . | spark.gluten.sql.columnar.sortMergeJoin | Enable or Disable Columnar Sort Merge Join, default is true | true | . | spark.gluten.sql.columnar.union | Enable or Disable Columnar Union, default is true | true | . | spark.gluten.sql.columnar.expand | Enable or Disable Columnar Expand, default is true | true | . | spark.gluten.sql.columnar.generate | Enable or Disable Columnar Generate, default is true | true | . | spark.gluten.sql.columnar.limit | Enable or Disable Columnar Limit, default is true | true | . | spark.gluten.sql.columnar.tableCache | Enable or Disable Columnar Table Cache, default is false | true | . | spark.gluten.sql.columnar.broadcastExchange | Enable or Disable Columnar Broadcast Exchange, default is true | true | . | spark.gluten.sql.columnar.broadcastJoin | Enable or Disable Columnar BroadcastHashJoin, default is true | true | . | spark.gluten.sql.columnar.shuffle.sort.threshold | The threshold to determine whether to use sort-based columnar shuffle. Sort-based shuffle will be used if the number of partitions is greater than this threshold. | 100000 | . | spark.gluten.sql.columnar.shuffle.codec | Set up the codec to be used for Columnar Shuffle. If this configuration is not set, will check the value of spark.io.compression.codec. By default, Gluten use software compression. Valid options for software compression are lz4, zstd. Valid options for QAT and IAA is gzip. | lz4 | . | spark.gluten.sql.columnar.shuffle.codecBackend | Enable using hardware accelerators for shuffle de/compression. Valid options are QAT and IAA. |   | . | spark.gluten.sql.columnar.shuffle.compressionMode | Setting different compression mode in shuffle, Valid options are buffer and rowvector, buffer option compress each buffer of RowVector individually into one pre-allocated large buffer, rowvector option first copies each buffer of RowVector to a large buffer and then compress the entire buffer in one go. | buffer | . | spark.gluten.sql.columnar.shuffle.compression.threshold | If number of rows in a batch falls below this threshold, will copy all buffers into one buffer to compress. | 100 | . | spark.gluten.sql.columnar.shuffle.realloc.threshold | Set the threshold to dynamically adjust the size of shuffle split buffers. The size of each split buffer is recalculated for each incoming batch of data. If the new size deviates from the current partition buffer size by a factor outside the range of [1 - threshold, 1 + threshold], the split buffer will be re-allocated using the newly calculated size | 0.25 | . | spark.gluten.sql.columnar.shuffle.merge.threshold | Set the threshold control the minimum merged size. When a partition buffer is full, and the number of rows is below (threshold * spark.gluten.sql.columnar.maxBatchSize), it will be saved for merging. | 0.25 | . | spark.gluten.sql.columnar.numaBinding | Set up NUMABinding, default is false | true | . | spark.gluten.sql.columnar.coreRange | Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system. Use 72 cores as an example. | 0-17,36-53 |18-35,54-71 | . | spark.gluten.sql.columnar.wholeStage.fallback.threshold | Configure the threshold for whether whole stage will fall back in AQE supported case by counting the number of ColumnarToRow &amp; vanilla leaf node | &gt;= 1 | . | spark.gluten.sql.columnar.query.fallback.threshold | Configure the threshold for whether query will fall back by counting the number of ColumnarToRow &amp; vanilla leaf node | &gt;= 1 | . | spark.gluten.sql.columnar.fallback.ignoreRowToColumnar | When true, the fallback policy ignores the RowToColumnar when counting fallback number. | true | . | spark.gluten.sql.columnar.fallback.preferColumnar | When true, the fallback policy prefers to use Gluten plan rather than vanilla Spark plan if the both of them contains ColumnarToRow and the vanilla Spark plan ColumnarToRow number is not smaller than Gluten plan. | true | . | spark.gluten.sql.columnar.force.hashagg | Force to use hash agg to replace sort agg. | true | . | spark.gluten.sql.columnar.vanillaReaders | Enable vanilla spark’s vectorized reader. Please note it may bring perf. overhead due to extra data transition. We recommend to disable it if most queries can be fully offloaded to gluten. | false | . | spark.gluten.sql.native.bloomFilter | Enable or Disable native runtime bloom filter. | true | . | spark.gluten.sql.native.arrow.reader.enabled | Enable or Disable native arrow read CSV file format | false | . | spark.gluten.shuffleWriter.bufferSize | Set the number of buffer rows for the shuffle writer | value of spark.gluten.sql.columnar.maxBatchSize | . | spark.gluten.loadLibFromJar | Controls whether to load dynamic link library from a packed jar for gluten/cpp. Not applicable to static build and clickhouse backend. | false | . | spark.gluten.loadLibOS | When spark.gluten.loadLibFromJar is true. Manually specify the system os to load library, e.g., CentOS |   | . | spark.gluten.loadLibOSVersion | Manually specify the system os version to load library, e.g., if spark.gluten.loadLibOS is CentOS, this config can be 7 |   | . | spark.gluten.expression.blacklist | A black list of expression to skip transform, multiple values separated by commas. |   | . | spark.gluten.sql.columnar.fallback.expressions.threshold | Fall back filter/project if the height of expression tree reaches this threshold, considering Spark codegen can bring better performance for such case. | 50 | . | spark.gluten.sql.cartesianProductTransformerEnabled | Config to enable CartesianProductExecTransformer. | true | . | spark.gluten.sql.broadcastNestedLoopJoinTransformerEnabled | Config to enable BroadcastNestedLoopJoinExecTransformer. | true | . | spark.gluten.sql.cacheWholeStageTransformerContext | When true, WholeStageTransformer will cache the WholeStageTransformerContext when executing. It is used to get substrait plan node and native plan string. | false | . | spark.gluten.sql.injectNativePlanStringToExplain | When true, Gluten will inject native plan tree to explain string inside WholeStageTransformerContext. | false | . | spark.gluten.sql.fallbackRegexpExpressions | When true, Gluten will fall back all regexp expressions to avoid any incompatibility risk. | false | . ",
    "url": "/archives/v1.2.1/docs/configuration/#spark-parameters",
    
    "relUrl": "/archives/v1.2.1/docs/configuration/#spark-parameters"
  },"188": {
    "doc": "Configuration(v1.2.1)",
    "title": "Velox Parameters",
    "content": "The following configurations are related to Velox settings. | Parameters | Description | Recommend Setting | . | spark.gluten.sql.columnar.backend.velox.bloomFilter.expectedNumItems | The default number of expected items for the velox bloomfilter. | 1000000L | . | spark.gluten.sql.columnar.backend.velox.bloomFilter.numBits | The default number of bits to use for the velox bloom filter. | 8388608L | . | spark.gluten.sql.columnar.backend.velox.bloomFilter.maxNumBits | The max number of bits to use for the velox bloom filter. | 4194304L | . | spark.gluten.sql.columnar.backend.velox.fileHandleCacheEnabled | Disables caching if false. File handle cache should be disabled if files are mutable, i.e. file content may change while file path stays the same. |   | . | spark.gluten.sql.columnar.backend.velox.directorySizeGuess | Set the directory size guess for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.filePreloadThreshold | Set the file preload threshold for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.prefetchRowGroups | Set the prefetch row groups for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.loadQuantum | Set the load quantum for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.maxCoalescedDistanceBytes | Set the max coalesced distance bytes for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.maxCoalescedBytes | Set the max coalesced bytes for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.cachePrefetchMinPct | Set prefetch cache min pct for velox file scan. |   | . | spark.gluten.velox.awsSdkLogLevel | Log granularity of AWS C++ SDK in velox. | FATAL | . | spark.gluten.velox.fs.s3a.retry.mode | Retry mode for AWS s3 connection error, can be “legacy”, “standard” and “adaptive”. | legacy | . | spark.gluten.velox.fs.s3a.connect.timeout | Timeout for AWS s3 connection. | 1s | . | spark.gluten.sql.columnar.backend.velox.orc.scan.enabled | Enable velox orc scan. If disabled, vanilla spark orc scan will be used. | true | . | spark.gluten.sql.complexType.scan.fallback.enabled | Force fallback for complex type scan, including struct, map, array. | true | . Additionally, you can control the configurations of gluten at thread level by local property. | Parameters | Description | Recommend Setting | . | gluten.enabledForCurrentThread | Control the usage of gluten at thread level. | true | . Below is an example of developing an application using scala to set local properties. // Before executing the query, set local properties. sparkContext.setLocalProperty(key, value) spark.sql(\"select * from demo_tables\").show() . ",
    "url": "/archives/v1.2.1/docs/configuration/#velox-parameters",
    
    "relUrl": "/archives/v1.2.1/docs/configuration/#velox-parameters"
  },"189": {
    "doc": "Configuration(v1.2.1)",
    "title": "Configuration(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/configuration/",
    
    "relUrl": "/archives/v1.2.1/docs/configuration/"
  },"190": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Gluten CPP Core Guidelines",
    "content": "This is a set of CPP core guidelines for Gluten. The aim is to make the codebase simpler, more efficient, more maintainable by promoting consistency and according to best practices. ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#gluten-cpp-core-guidelines",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#gluten-cpp-core-guidelines"
  },"191": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Philosophy",
    "content": "Philosophical rules are generally not measurable. However, they are valuable. For Gluten CPP coding, there are a few Philosophical rules as the following. | Write in ISO Standard C++. | Standard API first, the CPP programming APIs are priority to system calls. | Write code consistently. It’s good for understanding and maintaining. | Keep simple, making code clear and easy to read. | Optimize code for reader, not for writer. Thus, more time will be spent reading code than writing it. | Make it work, and then make it better or faster. | Don’t import any complexity if possible. Collaborate with minimal knowledge consensus. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#philosophy",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#philosophy"
  },"192": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Code Formatting",
    "content": "Many aspects of C++ coding style will be covered by clang-format, such as spacing, line width, indentation and ordering (for includes, using directives and etc).  . | Always ensure your code is compatible with clang-format-12 for Velox backend. | dev/formatcppcode.sh is provided for formatting Velox CPP code. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#code-formatting",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#code-formatting"
  },"193": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Naming Conventions",
    "content": ". | Use PascalCase for types (class, struct, enum, type alias, type template parameter) and file name. | Use camelCase for function, member and local variable, and non-type template parameter. | Use camelCase_ for private and protected member variable. | Use snake_case for namespace name and build target. | Use UPPER_SNAKE_CASE for macro. | Use kPascalCase for static constant and enumerator. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#naming-conventions",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#naming-conventions"
  },"194": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Designs",
    "content": ". | No over design. | No negation of negation, isValid is better than isNotInvalid. | Avoid corner case, and common case first. | Express ideas directly, don’t let me think. | Make a check for the arguments in the interface between modules, and don’t make a check in the inner implementation, use assert in the private implementation instead of too much safe check. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#designs",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#designs"
  },"195": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Source File &amp; Header File",
    "content": ". | All header files must have a single-inclusion guard using #pragma once | Always use .h as header file suffix, not .hpp. | Always use .cc as source file suffix, neither .cpp nor .cxx. | One file should contain one main class, and the file name should be consistent with the main class name. | Obvious exception: files used for defining various misc functions. | . | If a header file has a corresponding source file, they should have the same file name with different suffix, such as a.h vs a.cc. | If a function is declared in the file a.h, ensure it’s defined in the corrosponding source file a.cc, do not define it in other files. | No deep source directory for CPP files, not do it as JAVA. | Include header files should satisfy the following rules. | Include the necessary header files, which means the source file (.cc) containing the only one line #include \"test.h\" can be compiled successfully without including any other header files. | Do not include any unnecessary header files, the more including, the slower compiling. | In one word, no more, no less, just as needed. | . | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#source-file--header-file",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#source-file--header-file"
  },"196": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Class",
    "content": ". | Base class name doesn’t end with Base, use Backend instead of BackendBase. | Ensure one class does one thing, and follows the single responsibility principle. | No big class, No huge class, No too much interfaces. | Distinguish interface from implementation, make implementations private. | When designing a class hierarchy, distinguish between interface inheritance and implementation inheritance. | Ensure that public inheritance represent the relation of is-a. | Ensure that private inheritance represent the relation of implements-with. | . | Don’t make a function virtual without reason. | Ensure the polymorphic base class has a virtual deconstructor. | Use override to make overriding explicit and to make the compiler work. | Use const to mark the member function read-only as far as possible. | When you try to define a copy constructor or a operator= for a class, remember the Rule of three/five/zero. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#class",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#class"
  },"197": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Function",
    "content": ". | Make functions short and simple. | Calling a meaningful function is more readable than writing too many statements in place, but the performance-sensitive code path is an exception. | Give the function a good name, how to check whether the function name is good or not. | When you read it loudly, you feel smooth. | The information can be represented by arguments should not be encoded into the function name. such as. use get(size_t index) instead of getByIndex. | . | A function should focus on a single logic operation. | A function should do as the name meaning. | do everything converd by the function name | don’t do anything not convered by the function name | . | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#function",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#function"
  },"198": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Variable",
    "content": ". | Make variable names simple and meaningful. | Don’t group all your variables at the top of the scope, it’s an outdated habit. | Declare variables as close to the usage point as possible. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#variable",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#variable"
  },"199": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Constant",
    "content": ". | Prefer const variables to using preprocessor (#define) to define constant values. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#constant",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#constant"
  },"200": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Macro",
    "content": ". | Macros downgrade readability, break mind, and affect debug. | Macros have side effects. | Use macros cautiously and carefully. | Consider using const variables or inline functions to replace macros. | Consider defining macros with the wrap of do {...} while (0) | Avoid using 3rd party library macros directly. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#macro",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#macro"
  },"201": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Namespace",
    "content": ". | Don’t using namespace xxx in header files. Instead, you can do this in source files. But it’s still not encouraged. | Place all Gluten CPP codes under namespace gluten because one level namespace is enough. No nested namespace. Nested namespaces bring mess. | The anonymous namespace is recommended for defining file level classes, functions and variables. It’s used to place file scoped static functions and variables. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#namespace",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#namespace"
  },"202": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Resource Management",
    "content": ". | Use handles and RAII to manage resources automatically. | Immediately give the result of an explicit resource allocation to a manager object. | Prefer scoped objects and stack objects. | Use raw pointers to denote individual objects. | Use pointer + size_t to denote array objects if you don’t want to use containers. | A raw pointer (a T*) is non-owning. | A raw reference (a T&amp;) is non-owning. | Understand the difference of unique_ptr, shared_ptr, weak_ptr. | unique_ptr represents ownership, but not share ownership. unique_ptr is equivalent to RAII, release the resource when the object is destructed. | shared_ptr represents shared ownership by use-count. It is more expensive that unqiue_ptr. | weak_ptr models temporary ownership. It is useful in breaking reference cycles formed by objects managed by shared_ptr. | . | Use unique_ptr or shared_ptr to represent ownership. | Prefer unique_ptr over shared_ptr unless you need to share ownership. | Use make_unique to make unique_ptrs. | Use make_shared to make shared_ptrs. | Take smart pointers as parameters only to explicitly express lifetime semantics. | For general use, take T* or T&amp; arguments rather than smart pointers. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#resource-management",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#resource-management"
  },"203": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Exception",
    "content": ". | The exception specifications are changing always. The difference between various CPP standards is big, so we should use exception cautiously in Gluten. | Prefer return code to throwing exceptions. | Prefer compile-time checking to run-time checking. | Encapsulate messy constructors, rather than spreading through the code. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#exception",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#exception"
  },"204": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "Code Comment",
    "content": ". | Add necessary comments. The comment is not the more the better, also not the less the better. | Good comment makes obscure code easily understood. It’s unnecessary to add comments for quite obvious code. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#code-comment",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#code-comment"
  },"205": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "References",
    "content": ". | CppCoreGuidelines | Velox CODING_STYLE | Thanks Gluten developers for their wise suggestions and helps. | . ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/#references",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/#references"
  },"206": {
    "doc": "CPP Code Style(v1.2.1)",
    "title": "CPP Code Style(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/cpp-code-style/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/cpp-code-style/"
  },"207": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "How to understand the key work of Gluten?",
    "content": "The Gluten worked as the role of bridge, it’s a middle layer between the Spark and the native execution library. The Gluten is responsibility for validating whether the operators of the Spark plan can be executed by the native engine or not. If yes, the Gluten transforms Spark plan to Substrait plan, and then send the Substrait plan to the native engine. The Gluten codes consist of two parts: the C++ codes and the Java/Scala codes. | All C++ codes are placed under the directory of gluten_home/cpp, the Java/Scala codes are placed under several directories, such as gluten_home/gluten-core gluten_home/gluten-data gluten_home/backends-velox. | The Java/Scala codes are responsibility for validating and transforming the execution plan. Source data should also be provided, the source data may come from files or other forms such as networks. | The C++ codes take the Substrait plan and the source data as inputs and transform the Substrait plan to the corresponding backend plan. If the backend is Velox, the Substrait plan will be transformed to the Velox plan, and then be executed. | . JNI is a programming technology of invoking C++ from Java. All JNI interfaces are defined in the file JniWrapper.cc under the directory jni. ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-understand-the-key-work-of-gluten",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-understand-the-key-work-of-gluten"
  },"208": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "How to debug in Gluten?",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-debug-in-gluten",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-debug-in-gluten"
  },"209": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "1 How to debug C++",
    "content": "If you don’t concern about the Scala/Java codes and just want to debug the C++ codes executed in native engine, you may debug the C++ via benchmarks with GDB. To debug C++, you have to generate the example files, the example files consist of: . | A file contained Substrait plan in JSON format | One or more input data files in Parquet format | . You can generate the example files by the following steps: . | build Velox and Gluten CPP gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . | Compiling with --build_type=Debug is good for debugging. | The executable file generic_benchmark will be generated under the directory of gluten_home/cpp/build/velox/benchmarks/. | . | build Gluten and generate the example files cd gluten_home mvn clean package -Pspark-3.2 -Pbackends-velox -Prss mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . | After the above operations, the examples files are generated under gluten_home/backends-velox | You can check it by the command tree gluten_home/backends-velox/generated-native-benchmark/ | You may replace -Pspark-3.2 with -Pspark-3.3 if your spark’s version is 3.3 $ tree gluten_home/backends-velox/generated-native-benchmark/ gluten_home/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . | . | now, run benchmarks with GDB cd gluten_home/cpp/build/velox/benchmarks/ gdb generic_benchmark . | When GDB load generic_benchmark successfully, you can set breakpoint on the main function with command b main, and then run with command r, then the process generic_benchmark will start and stop at the main function. | You can check the variables’ state with command p variable_name, or execute the program line by line with command n, or step-in the function been called with command s. | Actually, you can debug generic_benchmark with any gdb commands as debugging normal C++ program, because the generic_benchmark is a pure C++ executable file in fact. | . | gdb-tui is a valuable feature and is worth trying. You can get more help from the online docs. gdb-tui . | you can start generic_benchmark with specific JSON plan and input files . | If you omit them, the example.json, example_lineitem + example_orders under the directory of gluten_home/backends-velox/generated-native-benchmark will be used as default. | You can also edit the file example.json to custom the Substrait plan or specify the inputs files placed in the other directory. | . | get more detail information about benchmarks from MicroBenchmarks | . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#1-how-to-debug-c",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#1-how-to-debug-c"
  },"210": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "2 How to debug plan validation process",
    "content": "Gluten will validate generated plan before execute it, and validation usually happens in native side, so we provide a utility to help debug validation process in native side. | Run query with conf spark.gluten.sql.debug=true, and you will find generated plan be printed in stderr with json format, save it as plan.json for example. | Compile cpp part with --build_benchmarks=ON, then check plan_validator_util executable file in gluten_home/cpp/build/velox/benchmarks/. | Run or debug with ./plan_validator_util &lt;path&gt;/plan.json | . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#2-how-to-debug-plan-validation-process",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#2-how-to-debug-plan-validation-process"
  },"211": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "3 How to debug Java/Scala",
    "content": "wait to add . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#3-how-to-debug-javascala",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#3-how-to-debug-javascala"
  },"212": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "4 How to debug with core-dump",
    "content": "wait to complete . cd the_directory_of_core_file_generated gdb gluten_home/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . | the core-Executor task l-2000883-1671542526 represents the core file name. | . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#4-how-to-debug-with-core-dump",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#4-how-to-debug-with-core-dump"
  },"213": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "How to run TPC-H on Velox backend",
    "content": "Now, both Parquet and DWRF format files are supported, related scripts and files are under the directory of gluten_home/backends-velox/workload/tpch. The file README.md under gluten_home/backends-velox/workload/tpch offers some useful help but it’s still not enough and exact. One way of run TPC-H test is to run velox with docker by workflow, you can refer to velox_docker.yml . Here will explain how to run TPC-H on Velox backend with the Parquet file format. | First step, prepare the datasets, you have two choices. | One way, generate Parquet datasets using the script under gluten_home/backends-velox/workload/tpch/gen_data/parquet_dataset, You can get help from the above mentioned README.md. | The other way, using the small dataset under gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox directly, If you just want to make simple TPC-H testing, this dataset is a good choice. | . | Second step, run TPC-H on Velox backend testing. | Modify gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.scala. | . | set var parquet_file_path to correct directory. If using the small dataset directly in the step one, then modify it as below var parquet_file_path = \"gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox\" . | set var gluten_root to correct directory. If gluten_home is the directory of /home/gluten, then modify it as below var gluten_root = \"/home/gluten\" . - Modify `gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.sh`. | Set GLUTEN_JAR correctly. Please refer to the section of Build Gluten with Velox Backend | Set SPARK_HOME correctly. | Set the memory configurations appropriately. - Execute tpch_parquet.sh using the below command. | cd gluten_home/backends-velox/workload/tpch/run_tpch/ | ./tpch_parquet.sh | . | . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-run-tpc-h-on-velox-backend",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-run-tpc-h-on-velox-backend"
  },"214": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "How to run TPC-DS",
    "content": "wait to add . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-run-tpc-ds",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-run-tpc-ds"
  },"215": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "How to track the memory exhaust problem",
    "content": "When your gluten spark jobs failed because of OOM, you can track the memory allocation’s call stack by configuring spark.gluten.backtrace.allocation = true. The above configuration will use BacktraceAllocationListener wrapping from SparkAllocationListener to create VeloxMemoryManager. BacktraceAllocationListener will check every allocation, if a single allocation bytes exceeds a fixed value or the accumulative allocation bytes exceeds 1/2/3…G, the call stack of memory allocation will be outputted to standard output, you can check the backtrace and get some valuable information about tracking the memory exhaust issues. You can also adjust the policy to decide when to backtrace, such as the fixed value. ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-track-the-memory-exhaust-problem",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/#how-to-track-the-memory-exhaust-problem"
  },"216": {
    "doc": "How To Use Gluten(v1.2.1)",
    "title": "How To Use Gluten(v1.2.1)",
    "content": "There are some common questions about developing, debugging and testing been asked again and again. In order to help the developers to contribute to Gluten as soon as possible, we collected these frequently asked questions, and organized them in the form of Q&amp;A. It’s convenient for the developers to check and learn. when you encountered a new problem and then resolved it, please add a new item to this document if you think it may be helpful to the other developers. We use gluten_home to represent the home directory of Gluten in this document. ",
    "url": "/archives/v1.2.1/docs/developers/how-to-use/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-use/"
  },"217": {
    "doc": "How To Release(v1.2.1)",
    "title": "How to Release",
    "content": "This section outlines the steps for releasing Apache Gluten (incubating) according to the Apache release guidelines. All projects under the Apache umbrella must adhere to the Apache Release Policy. This guide is designed to assist you in comprehending the policy and navigating the process of releasing projects at Apache. ",
    "url": "/archives/v1.2.1/docs/developers/how-to-release/#how-to-release",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-release/#how-to-release"
  },"218": {
    "doc": "How To Release(v1.2.1)",
    "title": "Release Process",
    "content": ". | Prepare the release artifacts. | Upload the release artifacts to the SVN repository. | Verify the release artifacts. | Initiate a release vote. | Announce the results and the release. | . Prepare the release artifacts. | Create a branch from the target git repository. | Tag a RC and draft the release notes. | Build and Sign the release artifacts (including source archives, binaries, …etc). | Generate checksums for the artifacts. | . How to Sign the release artifacts. | Create a GPG key . | Add the GPG key to the KEYS file . | Sign the release artifacts with the GPG key. | . # create a GPG key, after executing this command, select the first one RSA 和 RSA $ gpg --full-generate-key # list the GPG keys $ gpg --keyid-format SHORT --list-keys # upload the GPG key to the key server, xxx is the GPG key id # eg: pub rsa4096/4C21E346 2024-05-06 [SC], 4C21E346 is the GPG key id; $ gpg --keyserver keyserver.ubuntu.com --send-key xxx # append the GPG key to the KEYS file the svn repository # [IMPORTANT] Don't replace the KEYS file, just append the GPG key to the KEYS file. $ svn co https://dist.apache.org/repos/dist/release/incubator/gluten/ $ (gpg --list-sigs xxx@apache.org &amp;&amp; gpg --export --armor xxx@apache.org) &gt;&gt; KEYS $ svn ci -m \"add gpg key\" # sign the release artifacts, xxxx is xxx@apache.org $ for i in *.tar.gz; do echo $i; gpg --local-user xxxx --armor --output $i.asc --detach-sig $i ; done . How to Generate checksums for the release artifacts. # create the checksums $ for i in *.tar.gz; do echo $i; sha512sum $i &gt; $i.sha512 ; done . Upload the release artifacts to the SVN repository. | Create a project directory in the SVN repository (1st time only). https://dist.apache.org/repos/dist/dev/incubator/gluten/ . | Create a directory for the release artifacts in the SVN repository. https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} release-version format: apache-gluten-#.#.#-rc# . | Upload the release artifacts to the SVN repository. $ svn co https://dist.apache.org/repos/dist/dev/incubator/gluten/ $ cp /path/to/release/artifacts/* ./{release-version}/ $ svn add ./{release-version}/* $ svn commit -m \"add Apache Gluten release artifacts for {release-version}\" . | After the upload, please visit the link https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} to verify if the file upload is successful or not. The upload release artifacts should be include * apache-gluten-#.#.#-incubating-src.tar.gz * apache-gluten-#.#.#-incubating-src.tar.gz.asc * apache-gluten-#.#.#-incubating-src.tar.gz.sha512 . | . Verify the release artifacts. Please follow below steps to verify the release artifacts. | Check if the Download links are valid. | Check if the checksums and GPG signatures are valid. | Check if the release artifacts name is qualified and match with the current release. | Check if LICENSE and NOTICE files are correct. | Check if the License Headers are included in all files if necessary. | No unlicensed compiled archives bundled in source archive. | . How to Verify the Signatures . Please follow below steps to verify the signatures. # download KEYS $ curl https://dist.apache.org/repos/dist/release/incubator/gluten/KEYS &gt; KEYS # import KEYS and trust the key, please replace the email address with the one you want to trust. $ gpg --import KEYS $ gpg --edit-key xxx@apache.org gpg&gt; trust gpg&gt; 5 gpg&gt; y gpg&gt; quit # enter the directory where the release artifacts are located $ cd /path/to/release/artifacts # verify the signature $ for i in *.tar.gz; do echo $i; gpg --verify $i.asc $i ; done # if you see 'Good signature' in the output, it means the signature is valid. How to Verify the checksums . Please follow below steps to verify the checksums . # verify the checksums $ for i in *.tar.gz; do echo $i; sha512sum --check $i.sha512; done . Initiate a release vote. | Email a vote request to dev@gluten.apache.org, requiring at least 3 PPMC +1s. | Allow 72 hours or until enough votes are collected. | Share the vote outcome on the dev list. | If successful, request a vote on general@incubator.apache.org, needing 3 PMC +1s. | Wait 72 hours or for sufficient votes. | Announce the results on the general list. | . Vote Email Template . [VOTE] Release Apache Gluten (Incubating) {release-version} Hello, This is a call for vote to release Apache Gluten (Incubating) version {release-version}. The vote thread: https://lists.apache.org/thread/{id} Vote Result: https://lists.apache.org/thread/{id} The release candidates: https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version}/ Release notes: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Git tag for the release: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Git commit id for the release: https://github.com/apache/incubator-gluten/commit/{id} Keys to verify the Release Candidate: https://downloads.apache.org/incubator/gluten/KEYS The vote will be open for at least 72 hours or until the necessary number of votes are reached. Please vote accordingly: [ ] +1 approve [ ] +0 no opinion [ ] -1 disapprove with the reason Checklist for reference: [ ] Download links are valid. [ ] Checksums and PGP signatures are valid. [ ] Source code distributions have correct names matching the current release. [ ] LICENSE and NOTICE files are correct for each Apache Gluten repo. [ ] All files have license headers if necessary. [ ] No unlicensed compiled archives bundled in source archive. To compile from the source, please refer to: https://github.com/apache/incubator-gluten#building-from-source Thanks, &lt;YOUR NAME&gt; . Announce the results and the release. Announce Email Template . Hello everyone, The Apache Gluten (Incubating) {release-version} has been released! Apache Gluten is a Q&amp;A platform software for teams at any scale. Whether it's a community forum, help center, or knowledge management platform, you can always count on Apache Gluten. Download Links: https://downloads.apache.org/incubator/gluten/ Release Notes: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Website: https://gluten.apache.org/ Resources: - Issue: https://github.com/apache/incubator-gluten/issues - Mailing list: dev@gluten.apache.org Thanks, &lt;YOUR NAME&gt; . Migrate candidate to the release Apache SVN . After the vote has passed, you need to migrate the RC build release to an official release by moving the artifacts from Apache SVN’s dev directory to the release directory. Please follow the steps below to upload the artifacts: . $ svn mv https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} https://dist.apache.org/repos/dist/release/incubator/gluten/{release-version} -m \"transfer packages for gluten {release-version}\" . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-release/#release-process",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-release/#release-process"
  },"219": {
    "doc": "How To Release(v1.2.1)",
    "title": "How To Release(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/how-to-release/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-release/"
  },"220": {
    "doc": "How To Scan Security issues(v1.2.1)",
    "title": "How to scan the security issues",
    "content": "This section outlines the steps to use tools to scan Apache Gluten (incubating) source code and make sure no vulnerability issues in the code. All projects under the Apache umbrella must adhere to the Apache Release Policy. This guide is designed to assist you in comprehending the policy and navigating the process of releasing projects at Apache. ",
    "url": "/archives/v1.2.1/docs/developers/how-to-security-scan/#how-to-scan-the-security-issues",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-security-scan/#how-to-scan-the-security-issues"
  },"221": {
    "doc": "How To Scan Security issues(v1.2.1)",
    "title": "Scan Security Process",
    "content": "Before every Apache Gluten (incubating) release, we need to ensure there is no vulnerability issue in the source code. We use Trivy as the tool to scan all the security issues. | Install Trivy, please follow the steps to install Trivy: Trivy Installation . | Configuring Trivy, please follow the guide to configure Trivy for specific operation: Trivy Configuration . | Run Trivy File System Scan with the source code. Below is an example about how we run Trivy scan with Apache Gluten (incubating) source code. You can use your own tpl file as a template. | . trivy fs --list-all-pkgs --format template --template \"@/PATH/TO/csv.tpl\" --output ./trivy-report.csv /PATH/TO/GLUTEN_LOCATION/ . | Open the report file and check if there is any vulnerability issue highlighted. We must guarantee all the vulnerability issue has been solved before an official release. | . ",
    "url": "/archives/v1.2.1/docs/developers/how-to-security-scan/#scan-security-process",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-security-scan/#scan-security-process"
  },"222": {
    "doc": "How To Scan Security issues(v1.2.1)",
    "title": "How To Scan Security issues(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/how-to-security-scan/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/how-to-security-scan/"
  },"223": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Generate Micro Benchmarks for Velox Backend",
    "content": "This document explains how to use the existing micro benchmark template in Gluten Cpp. A micro benchmark for Velox backend is provided in Gluten Cpp to simulate the execution of a first or middle stage in Spark. It serves as a more convenient alternative to debug in Gluten Cpp comparing with directly debugging in a Spark job. Developers can use it to create their own workloads, debug in native process, profile the hotspot and do optimizations. To simulate a first stage, you need to dump the Substrait plan and input split info into two JSON files. The input URIs of the splits should be exising file locations, which can be either local or HDFS paths. To simulate a middle stage, in addition to the JSON file, you also need to save the input data of this stage into Parquet files. The benchmark will load the data into Arrow format, then add Arrow2Velox to feed the data into Velox pipeline to reproduce the reducer stage. Shuffle exchange is not included. Please refer to the sections below to learn how to dump the Substrait plan and create the input data files. ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/#generate-micro-benchmarks-for-velox-backend",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/#generate-micro-benchmarks-for-velox-backend"
  },"224": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Try the example",
    "content": "To run a micro benchmark, user should provide one file that contains the Substrait plan in JSON format, and optional one or more input data files in parquet format. The commands below help to generate example input files: . cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_tests=ON --build_benchmarks=ON # Run test to generate input data files. If you are using spark 3.3, replace -Pspark-3.2 with -Pspark-3.3 mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am \\ -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . The generated example files are placed in gluten/backends-velox: . $ tree gluten/backends-velox/generated-native-benchmark/ gluten/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . Run micro benchmark with the generated files as input. You need to specify the absolute path to the input files: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example.json \\ --data /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_orders/part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet,\\ /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_lineitem/part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet \\ --threads 1 --iterations 1 --noprint-result --benchmark_filter=InputFromBatchStream . The output should be like: . 2022-11-18T16:49:56+08:00 Running ./generic_benchmark Run on (192 X 3800 MHz CPU s) CPU Caches: L1 Data 48 KiB (x96) L1 Instruction 32 KiB (x96) L2 Unified 2048 KiB (x96) L3 Unified 99840 KiB (x2) Load Average: 0.28, 1.17, 1.59 ***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead. -- Project[expressions: (n3_0:BIGINT, ROW[\"n1_0\"]), (n3_1:VARCHAR, ROW[\"n1_1\"])] -&gt; n3_0:BIGINT, n3_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 36.33us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 queuedWallNanos sum: 2.00us, count: 2, min: 0ns, max: 2.00us -- HashJoin[RIGHT SEMI (FILTER) n0_0=n1_0] -&gt; n1_0:BIGINT, n1_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 191.56us, Blocked wall time: 0ns, Peak memory: 2.00MB, Memory allocations: 8 HashBuild: Input: 582 rows (16.45KB, 1 batches), Output: 0 rows (0B, 0 batches), Cpu time: 1.84us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 distinctKey0 sum: 583, count: 1, min: 583, max: 583 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 59748, count: 1, min: 59748, max: 59748 HashProbe: Input: 37897 rows (296.07KB, 1 batches), Output: 535 rows (65.81KB, 1 batches), Cpu time: 189.71us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 5, Threads: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns -- ArrowStream[] -&gt; n0_0:BIGINT Input: 0 rows (0B, 0 batches), Output: 37897 rows (296.07KB, 1 batches), Cpu time: 1.29ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 -- ArrowStream[] -&gt; n1_0:BIGINT, n1_1:VARCHAR Input: 0 rows (0B, 0 batches), Output: 582 rows (16.45KB, 1 batches), Cpu time: 894.22us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 ----------------------------------------------------------------------------------------------------------------------------- Benchmark Time CPU Iterations UserCounters... ----------------------------------------------------------------------------------------------------------------------------- InputFromBatchVector/iterations:1/process_time/real_time/threads:1 41304520 ns 23740340 ns 1 collect_batch_time=34.7812M elapsed_time=41.3113M . ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/#try-the-example",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/#try-the-example"
  },"225": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Generate Substrait plan and input for any query",
    "content": "First, build Gluten with --build_benchmarks=ON. cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_benchmarks=ON # For debugging purpose, rebuild Gluten with build type `Debug`./dev/buildbundle-veloxbe.sh --build_benchmarks=ON --build_type=Debug . First, get the Stage Id from spark UI for the stage you want to simulate. And then re-run the query with below configurations to dump the inputs to micro benchmark. | Parameters | Description | Recommend Setting | . | spark.gluten.sql.benchmark_task.stageId | Spark task stage id | target stage id | . | spark.gluten.sql.benchmark_task.partitionId | Spark task partition id, default value -1 means all the partition of this stage | 0 | . | spark.gluten.sql.benchmark_task.taskId | If not specify partition id, use spark task attempt id, default value -1 means all the partition of this stage | target task attemp id | . | spark.gluten.saveDir | Directory to save the inputs to micro benchmark, should exist and be empty. | /path/to/saveDir | . Check the files in spark.gluten.saveDir. If the simulated stage is a first stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage, without input file splits. | Split file: JSON formatted, file name split_[stageId]_[partitionId]_[splitIndex].json. There can be more than one split file in a first stage task. Contains the substrait plan piece to the input file splits. | . Run benchmark. By default, the result will be printed to stdout. You can use --noprint-result to suppress this output. Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --split /absolut_path/to/split_[stageId]_[partitionId]_0.parquet,/absolut_path/to/split_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . If the simulated stage is a middle stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage. | Data file: Parquet formatted, file name data_[stageId]_[partitionId]_[iteratorIndex].json. There can be more than one input data file in a middle stage task. The input data files of a middle stage will be loaded as iterators to serve as the inputs for the pipeline: | . \"localFiles\": { \"items\": [ { \"uriFile\": \"iterator:0\" } ] } . Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --data /absolut_path/to/data_[stageId]_[partitionId]_0.parquet,/absolut_path/to/data_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . For some complex queries, stageId may cannot represent the Substrait plan input, please get the taskId from spark UI, and get your target parquet from saveDir. In this example, only one partition input with partition id 2, taskId is 36, iterator length is 2. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/complex_plan.json \\ --data /absolute_path/to/data_36_2_0.parquet,/absolute_path/to/data_36_2_1.parquet \\ --threads 1 --noprint-result . ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/#generate-substrait-plan-and-input-for-any-query",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/#generate-substrait-plan-and-input-for-any-query"
  },"226": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Save ouput to parquet to analyze",
    "content": "You can save the output to a parquet file to analyze. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --data /absolute_path/to/data.parquet --threads 1 --noprint-result --write-file=/absolute_path/to/result.parquet . ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/#save-ouput-to-parquet-to-analyze",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/#save-ouput-to-parquet-to-analyze"
  },"227": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Add shuffle write process",
    "content": "You can add the shuffle write process at the end of this stage. Note that this will ignore the --write-file option. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --split /absolute_path/to/split.json \\ --threads 1 --noprint-result --with-shuffle . By default, the compression codec for shuffle outputs is LZ4. You can switch to other codecs by adding one of the following argument flags to the command: . | –zstd: ZSTD codec, compression level 1 | –qat-gzip: QAT GZIP codec, compression level 1 | –qat-zstd: QAT ZSTD codec, compression level 1 | –iaa-gzip: IAA GZIP codec, compression level 1 | . Note using QAT or IAA codec requires Gluten cpp is built with these features. Please check the corresponding section in Velox document first for how to setup, build and enable these features in Gluten. For QAT support, please check Intel® QuickAssist Technology (QAT) support. For IAA support, please check Intel® In-memory Analytics Accelerator (IAA/IAX) support . ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/#add-shuffle-write-process",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/#add-shuffle-write-process"
  },"228": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Simulate Spark with multiple processes and threads",
    "content": "You can use below command to launch several processes and threads to simulate parallel execution on Spark. Each thread in the same process will be pinned to the core number starting from --cpu. Suppose running on a baremetal machine with 48C, 2-socket, HT-on, launching below command will utilize all vcores. processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --threads $threads --cpu $((i*threads)) &amp; done . If you want to add the shuffle write process, you can specify multiple direcotries by setting environment variable GLUTEN_SPARK_LOCAL_DIRS to a comma-separated string for shuffle write to spread the I/O pressure to multiple disks. mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done . Run Examples . We also provide some example inputs in cpp/velox/benchmarks/data. E.g. generic_q5/q5_first_stage_0.json simulates a first-stage in TPCH Q5, which has the the most heaviest table scan. You can follow below steps to run this example. | Open generic_q5/q5_first_stage_0.json with file editor. Search for \"uriFile\": \"LINEITEM\" and replace LINEITEM with the URI to one partition file in lineitem. In the next line, replace the number in \"length\": \"...\" with the actual file length. Suppose you are using the provided small TPCH table in cpp/velox/benchmarks/data/tpch_sf10m, the replaced JSON should be like: | . { \"items\": [ { \"uriFile\": \"file:///path/to/gluten/cpp/velox/benchmarks/data/tpch_sf10m/lineitem/part-00000-6c374e0a-7d76-401b-8458-a8e31f8ab704-c000.snappy.parquet\", \"length\": \"1863237\", \"parquet\": {} } ] } . | Launch multiple processes and multiple threads. Set GLUTEN_SPARK_LOCAL_DIRS and add –with-shuffle to the command. | . mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0.json --split /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0_split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done &gt;stdout.log 2&gt;stderr.log . You can find the “elapsed_time” and other metrics in stdout.log. In below output, the “elapsed_time” is ~10.75s. If you run TPCH Q5 with Gluten on Spark, a single task in the same Spark stage should take about the same time. ------------------------------------------------------------------------------------------------------------------ Benchmark Time CPU Iterations UserCounters... ------------------------------------------------------------------------------------------------------------------ SkipInput/iterations:1/process_time/real_time/threads:8 1317255379 ns 10061941861 ns 8 collect_batch_time=0 elapsed_time=10.7563G shuffle_compress_time=4.19964G shuffle_spill_time=0 shuffle_split_time=0 shuffle_write_time=1.91651G . ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/#simulate-spark-with-multiple-processes-and-threads",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/#simulate-spark-with-multiple-processes-and-threads"
  },"229": {
    "doc": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "title": "Micro Benchmarks for Velox Backend(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/microbenchmarks/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/microbenchmarks/"
  },"230": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Environment",
    "content": "Now gluten supports Ubuntu20.04, Ubuntu22.04, centos8, centos7 and macOS. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#environment",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#environment"
  },"231": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Openjdk8",
    "content": "Environment setting . For root user, the environment variables file is /etc/profile, it will make effect for all the users. For other user, you can set in ~/.bashrc. Guide for ubuntu . The default JDK version in ubuntu is java11, we need to set to java8. apt install openjdk-8-jdk update-alternatives --config java java -version . --config java to config java executable path, javac and other commands can also use this command to config. For some other uses, we suggest to set JAVA_HOME. export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # pay attention to $PATH double quote export PATH=\"$PATH:$JAVA_HOME/bin\" . Must set PATH with double quote in ubuntu. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#openjdk8",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#openjdk8"
  },"232": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Maven 3.6.3 or above",
    "content": "Maven Dowload Page And then set the environment setting. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#maven-363-or-above",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#maven-363-or-above"
  },"233": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "GCC 9.4 or above",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#gcc-94-or-above",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#gcc-94-or-above"
  },"234": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Compile gluten using debug mode",
    "content": "If you want to just debug java/scala code, there is no need to compile cpp code with debug mode. You can just refer to build-gluten-with-velox-backend. If you need to debug cpp code, please compile the backend code and gluten cpp code with debug mode. ## compile velox backend with benchmark and tests to debug gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . If you need to debug the tests in /gluten-ut, You need to compile java code with `-P spark-ut`. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#compile-gluten-using-debug-mode",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#compile-gluten-using-debug-mode"
  },"235": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Java/scala code development with Intellij",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#javascala-code-development-with-intellij",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#javascala-code-development-with-intellij"
  },"236": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Linux intellij local debug",
    "content": "Install the linux intellij version, and debug code locally. | Ask your linux maintainer to install the desktop, and then restart the server. | If you use Moba-XTerm to connect linux server, you don’t need to install x11 server, If not (e.g. putty), please follow this guide: X11 Forwarding: Setup Instructions for Linux and Mac . | Download intellij linux community version to linux server | Start Idea, bash &lt;idea_dir&gt;/idea.sh | . Notes: Sometimes, your desktop may stop accidently, left idea running. root@xx2:~bash idea-IC-221.5787.30/bin/idea.sh Already running root@xx2:~ps ux | grep intellij root@xx2:kill -9 &lt;pid&gt; . And then restart idea. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#linux-intellij-local-debug",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#linux-intellij-local-debug"
  },"237": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Windows/Mac intellij remote debug",
    "content": "If you have Ultimate intellij, you can try to debug remotely. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#windowsmac-intellij-remote-debug",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#windowsmac-intellij-remote-debug"
  },"238": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Set up gluten project",
    "content": ". | Make sure you have compiled gluten. | Load the gluten by File-&gt;Open, select &lt;gluten_home/pom.xml&gt;. | Activate your profiles such as , and Reload Maven Project, you will find all your need modules have been activated. | Create breakpoint and debug as you wish, maybe you can try CTRL+N to find TestOperator to start your test. | . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#set-up-gluten-project",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#set-up-gluten-project"
  },"239": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Java/Scala code style",
    "content": "Intellij IDE supports importing settings for Java/Scala code style. You can import intellij-codestyle.xml to your IDE. See Intellij guide. To generate a fix for Java/Scala code style, you can run one or more of the below commands according to the code modules involved in your PR. For Velox backend: . mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.3 -Pspark-ut -DskipTests . For Clickhouse backend: . mvn spotless:apply -Pbackends-clickhouse -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-clickhouse -Pspark-3.3 -Pspark-ut -DskipTests . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#javascala-code-style",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#javascala-code-style"
  },"240": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "CPP code development with Visual Studio Code",
    "content": "This guide is for remote debug. We will connect the remote linux server by SSH. Download the windows vscode software The important leftside bar is: . | Explorer (Project structure) | Search | Run and Debug | Extensions (Install C/C++ Extension Pack, Remote Development, GitLens at least, C++ Test Mate is also suggested) | Remote Explorer (Connect linux server by ssh command, click +, then input ssh user@10.1.7.003) | Manage (Settings) | . Input your password in the above pop-up window, it will take a few minutes to install linux vscode server in remote machine folder ~/.vscode-server If download failed, delete this folder and try again. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#cpp-code-development-with-visual-studio-code",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#cpp-code-development-with-visual-studio-code"
  },"241": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Usage",
    "content": "Set up project . File-&gt;Open Folder // select gluten folder Select cpp/CmakeList.txt as command prompt Select gcc version as command prompt . Settings . VSCode support 2 ways to set user setting. | Manage-&gt;Command Palette(Open settings.json, search by Preferences: Open Settings (JSON)) | Manage-&gt;Settings (Common setting) | . Build by vscode . VSCode will try to compile the debug version in /build. And we need to compile velox debug mode before, if you have compiled velox release mode, you just need to do. # Build the velox debug version in &lt;velox_home&gt;/_build/debug make debug EXTRA_CMAKE_FLAGS=\"-DVELOX_ENABLE_PARQUET=ON -DENABLE_HDFS=ON -DVELOX_BUILD_TESTING=OFF -DVELOX_ENABLE_DUCKDB=ON -DVELOX_BUILD_TEST_UTILS=ON\" . Then gluten will link velox debug library. Just click build in bottom bar, you will get intellisense search and link. Debug . The default compile command does not enable test and benchmark, so we cannot get any executable file Open the file in &lt;gluten_home&gt;/.vscode/settings.json (create if not exists) . { \"cmake.configureArgs\": [ \"-DBUILD_BENCHMARKS=ON\", \"-DBUILD_TESTS=ON\" ], \"C_Cpp.default.configurationProvider\": \"ms-vscode.cmake-tools\" } . Then we can get some executables, take velox_shuffle_writer_test as example . Click Run and Debug to create launch.json in &lt;gluten_home&gt;/.vscode/launch.json Click Add Configuration in the top of launch.json, select gdb launch or attach to exists program launch.json example . { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"velox shuffle writer test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/build/velox/tests/velox_shuffle_writer_test\", \"args\": [\"--gtest_filter=*TestSinglePartPartitioner*\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] }, { \"name\": \"benchmark test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/./generic_benchmark\", \"args\": [\"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/query.json\", \"--threads=1\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] } ] } . Change name, program, args to yours . Then you can create breakpoint and debug in Run and Debug section. Velox debug . For some velox tests such as ParquetReaderTest, tests need to read the parquet file in &lt;velox_home&gt;/velox/dwio/parquet/tests/examples, you should let the screen on ParquetReaderTest.cpp, then click Start Debuging, otherwise you will raise No such file or directory exception . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#usage",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#usage"
  },"242": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Usefule notes",
    "content": "Upgrade vscode . No need to upgrade vscode version, if upgraded, will download linux server again, switch update mode to off Search update in Manage-&gt;Settings to turn off update mode . Colour setting . \"workbench.colorTheme\": \"Quiet Light\", \"files.autoSave\": \"afterDelay\", \"workbench.colorCustomizations\": { \"editor.wordHighlightBackground\": \"#063ef7\", // \"editor.selectionBackground\": \"#d1d1c6\", // \"tab.activeBackground\": \"#b8b9988c\", \"editor.selectionHighlightBackground\": \"#c5293e\" }, . Clang format . Now gluten uses clang-format 12 to format source files. apt-get install clang-format-12 . Set config in settings.json . \"clang-format.executable\": \"clang-format-12\", \"editor.formatOnSave\": true, . If exists multiple clang-format version, formatOnSave may not take effect, specify the default formatter Search default formatter in Settings, select Clang-Format. If your formatOnSave still make no effect, you can use shortcut SHIFT+ALT+F to format one file mannually. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#usefule-notes",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#usefule-notes"
  },"243": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Debug cpp code with coredump",
    "content": "mkdir -p /mnt/DP_disk1/core sysctl -w kernel.core_pattern=/mnt/DP_disk1/core/core-%e-%p-%t cat /proc/sys/kernel/core_pattern # set the core file to unlimited size echo \"ulimit -c unlimited\" &gt;&gt; ~/.bashrc # then you will get the core file at `/mnt/DP_disk1/core` when the program crashes # gdb -c corefile # gdb &lt;gluten_home&gt;/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . ‘core-Executor task l-2000883-1671542526’ is the generated core file name. (gdb) bt (gdb) f7 (gdb) set print pretty on (gdb) p *this . | Get the backtrace | Switch to 7th stack | Print the variable in a more readable way | Print the variable fields | . Sometimes you only get the cpp exception message, you can generate core dump file by the following code: . char* p = nullptr; *p = 'a'; . or by the following commands: . | gcore &lt;pid&gt; | kill -s SIGSEGV &lt;pid&gt; | . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#debug-cpp-code-with-coredump",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#debug-cpp-code-with-coredump"
  },"244": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Debug cpp with gdb",
    "content": "You can use gdb to debug tests and benchmarks. And also you can debug jni call. Place the following code to your debug path. pid_t pid = getpid(); printf(\"----------------------------------pid: %lun\", pid); sleep(10); . You can also get the pid by java command or grep java program when executing unit test. jps 1375551 ScalaTestRunner ps ux | grep TestOperator . Execute gdb command to debug: . gdb attach &lt;pid&gt; . gdb attach 1375551 wait to attach.... (gdb) b &lt;velox_home&gt;/velox/substrait/SubstraitToVeloxPlan.cpp:577 (gdb) c . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#debug-cpp-with-gdb",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#debug-cpp-with-gdb"
  },"245": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Run TPC-H and TPC-DS",
    "content": "We supply &lt;gluten_home&gt;/tools/gluten-it to execute these queries Refer to velox_docker.yml . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#run-tpc-h-and-tpc-ds",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#run-tpc-h-and-tpc-ds"
  },"246": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "Run gluten+velox on clean machine",
    "content": "We can run gluten + velox on clean machine by one command (supported OS: Ubuntu20.04/22.04, Centos 7/8, etc.). spark-shell --name run_gluten \\ --master yarn --deploy-mode client \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --jars https://github.com/oap-project/gluten/releases/download/v1.0.0/gluten-velox-bundle-spark3.2_2.12-ubuntu_20.04_x86_64-1.0.0.jar \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager . ",
    "url": "/archives/v1.2.1/docs/developers/new-to/#run-glutenvelox-on-clean-machine",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/#run-glutenvelox-on-clean-machine"
  },"247": {
    "doc": "New To Gluten(v1.2.1)",
    "title": "New To Gluten(v1.2.1)",
    "content": "Help users to debug and test with gluten. For intel internal developer, you could refer to internal wiki New Employee Guide to get more information such as proxy settings, Gluten has cpp code and java/scala code, we can use some useful IDE to read and debug. ",
    "url": "/archives/v1.2.1/docs/developers/new-to/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/new-to/"
  },"248": {
    "doc": "Substrait Modifications(v1.2.1)",
    "title": "Substrait Modifications in Gluten",
    "content": "Substrait is a project aiming to create a well-defined, cross-language specification for data compute operations. Since it is still under active development, there are some lacking representations for Gluten needed computing operations. At the same time, some existing representations need to be modified a bit to satisfy the needs of computing. In Gluten, the base version of Substrait is v0.23.0. This page records all the Gluten changes to Substrait proto files for reference. It is preferred to upstream these changes to Substrait, but for those cannot be upstreamed, alternatives like AdvancedExtension could be considered. ",
    "url": "/archives/v1.2.1/docs/developers/substrait/#substrait-modifications-in-gluten",
    
    "relUrl": "/archives/v1.2.1/docs/developers/substrait/#substrait-modifications-in-gluten"
  },"249": {
    "doc": "Substrait Modifications(v1.2.1)",
    "title": "Modifications to algebra.proto",
    "content": ". | Added JsonReadOptions and TextReadOptions in FileOrFiles(#1584). | Changed join type JOIN_TYPE_SEMI to JOIN_TYPE_LEFT_SEMI and JOIN_TYPE_RIGHT_SEMI(#408). | Added WindowRel, added column_name and window_type in WindowFunction, changed Unbounded in WindowFunction into Unbounded_Preceding and Unbounded_Following, and added WindowType(#485). | Added output_schema in RelRoot(#1901). | Added ExpandRel(#1361). | Added GenerateRel(#574). | Added PartitionColumn in LocalFiles(#2405). | Added WriteRel (#3690). | . ",
    "url": "/archives/v1.2.1/docs/developers/substrait/#modifications-to-algebraproto",
    
    "relUrl": "/archives/v1.2.1/docs/developers/substrait/#modifications-to-algebraproto"
  },"250": {
    "doc": "Substrait Modifications(v1.2.1)",
    "title": "Modifications to type.proto",
    "content": ". | Added Nothing in Type(#791). | Added names in Struct(#1878). | Added PartitionColumns in NamedStruct(#320). | Remove PartitionColumns and add column_types in NamedStruct(#2405). | . ",
    "url": "/archives/v1.2.1/docs/developers/substrait/#modifications-to-typeproto",
    
    "relUrl": "/archives/v1.2.1/docs/developers/substrait/#modifications-to-typeproto"
  },"251": {
    "doc": "Substrait Modifications(v1.2.1)",
    "title": "Substrait Modifications(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/substrait/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/substrait/"
  },"252": {
    "doc": "Docker script for CentOS 7(v1.2.1)",
    "title": "Docker script for CentOS 7(v1.2.1)",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on CentOS 7: . Run on host as root user: . docker pull centos:7 docker run -itd --name gluten centos:7 /bin/bash docker attach gluten . Run in docker: . yum -y install epel-release centos-release-scl yum -y install \\ git236 \\ dnf \\ cmake3 \\ devtoolset-9 \\ java-1.8.0-openjdk \\ java-1.8.0-openjdk-devel \\ ninja-build \\ wget \\ autoconf \\ autoconf-archive \\ automake \\ perl-IPC-Cmd \\ libicu-devel \\ bison \\ libtool \\ patch \\ flex \\ sudo # gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin # cmake 3.x is required ln -s /usr/bin/cmake3 /usr/local/bin/cmake # enable gcc 9 . /opt/rh/devtoolset-9/enable || exit 1 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # If you have the same error with issue-3283, you need to add the parameter `--compile_arrow_java=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/archives/v1.2.1/docs/developers/docker-centos7/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/docker-centos7/"
  },"253": {
    "doc": "Docker script for CentOS 8(v1.2.1)",
    "title": "Docker script for CentOS 8(v1.2.1)",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on Centos8: . Run on host as root user: . docker pull centos:8 docker run -itd --name gluten centos:8 /bin/bash docker attach gluten . Run in docker: . #update mirror sed -i -e \"s|mirrorlist=|#mirrorlist=|g\" /etc/yum.repos.d/CentOS-* sed -i -e \"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\" /etc/yum.repos.d/CentOS-* dnf install -y epel-release sudo yum install -y dnf-plugins-core yum config-manager --set-enabled powertools dnf --enablerepo=powertools install -y ninja-build dnf --enablerepo=powertools install -y libdwarf-devel dnf install -y --setopt=install_weak_deps=False ccache gcc-toolset-9 git wget which libevent-devel \\ openssl-devel re2-devel libzstd-devel lz4-devel double-conversion-devel \\ curl-devel cmake libicu-devel source /opt/rh/gcc-toolset-9/enable || exit 1 yum install -y java-1.8.0-openjdk-devel patch export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH #gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/archives/v1.2.1/docs/developers/docker-centos8/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/docker-centos8/"
  },"254": {
    "doc": "Docker script for Ubuntu 22.04/20.04(v1.2.1)",
    "title": "Docker script for Ubuntu 22.04/20.04(v1.2.1)",
    "content": "To the first build, it’s suggested to build Gluten in a clean docker image. Otherwise it’s easy to run into library version conflict issues. Here is a docker script we verified to build Gluten+Velox backend on Ubuntu22.04/20.04: . Run on host as root user: . docker pull ubuntu:22.04 docker run -itd --network host --name gluten ubuntu:22.04 /bin/bash docker attach gluten . Run in docker: . apt-get update #install gcc and libraries to build arrow apt install software-properties-common apt install maven build-essential cmake libssl-dev libre2-dev libcurl4-openssl-dev clang lldb lld libz-dev git ninja-build uuid-dev autoconf-archive curl zip unzip tar pkg-config bison libtool flex vim #velox script needs sudo to install dependency libraries apt install sudo # make sure jemalloc is uninstalled, jemalloc will be build in vcpkg, which conflicts with the default jemalloc in system apt purge libjemalloc-dev libjemalloc2 librust-jemalloc-sys-dev #make sure jdk8 is used. New version of jdk is not supported apt install -y openjdk-8-jdk apt install -y default-jdk export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH #manually install tzdata to avoid the interactive timezone config ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata dpkg --configure -a #setup proxy on necessary #export http_proxy=xxxx #export https_proxy=xxxx #clone gluten git clone https://github.com/oap-project/gluten.git cd gluten/ #config maven proxy #mkdir ~/.m2/ #vim ~/.m2/settings.xml # the script download velox &amp; arrow and compile all dependency library automatically # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # It's suggested to build using static link, enabled by `--enable_vcpkg=ON` # For developer, it's suggested to enable Debug info, by --build_type=RelWithDebInfo. Note RelWithDebInfo uses -o2, release uses -o3 ./dev/buildbundle-veloxbe.sh --enable_vcpkg=ON --build_type=RelWithDebInfo . ",
    "url": "/archives/v1.2.1/docs/developers/docker-ubuntu/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/docker-ubuntu/"
  },"255": {
    "doc": "Developers(v1.2.1)",
    "title": "Gluten Developers",
    "content": "This document provides a developer overview of the project and covers the following topics: . ",
    "url": "/archives/v1.2.1/docs/developers/#gluten-developers",
    
    "relUrl": "/archives/v1.2.1/docs/developers/#gluten-developers"
  },"256": {
    "doc": "Developers(v1.2.1)",
    "title": "Developers(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/"
  },"257": {
    "doc": "Velox Function Development(v1.2.1)",
    "title": "Developer Guide for Implementing Spark Built-in SQL Functions in Velox",
    "content": "In velox, two folders prestosql &amp; sparksql are holding most sql functions, respective for presto and spark. Gluten will ask velox to firstly register prestosql functions, then sparksql functions. So if prestosql and sparksql share same signature for a function, the sparksql function will overwrite the corresponding prestosql function. If the required function is lacking in both folders (exceptions are some common functions defined outside, like cast), we need to implement the missing function in sparksql folder. It is possible that a prestosql function has some semantic difference with the corresponding spark function, even though they share the same name and function signature. If so, we also need to do an implementation in sparksql folder, generally based on the original impl. for prestosql. There are a few spark functions that can behave differently for some special cases, depending on ANSI on or off. Currently, gluten does NOT support ANSI mode. So only ANSI off needs to be considered in implementing spark built-in functions in velox. Take BitwiseAndFunction as example: . template &lt;typename T&gt; struct BitwiseAndFunction { template &lt;typename TInput&gt; // For void return type, it indicates null result will never be obtained for non-null input. // For bool return type, it indicates null result can be obtained for non-null input (false for null). FOLLY_ALWAYS_INLINE void call(TInput&amp; result, TInput a, TInput b) { result = a &amp; b; } }; . It is templated, as well as the call function, to allow multiple types. In the above impl., the result will be null for null input. Please use callNullable if you need different behavior for null input, e.g., get a non-null result for null input. Also see callNullFree in velox document. It is used for fast evaluation in the case that any input has null. The below code will register the implemented function for all kinds of integer types. The specified name bitwise_and will be actually used in calling this function. registerBinaryIntegral&lt;BitwiseAndFunction&gt;({prefix + \"bitwise_and\"}); . Functions for complex types have similar implementations. See ArrayAverageFunction in velox/functions/prestosql/ArrayFunctions.h. Reference: . Velox’s official developer guide: . | velox/docs/develop/scalar-functions.rst | velox/examples/SimpleFunctions.cpp | . ",
    "url": "/archives/v1.2.1/docs/developers/velox-function-dev/#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox",
    
    "relUrl": "/archives/v1.2.1/docs/developers/velox-function-dev/#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox"
  },"258": {
    "doc": "Velox Function Development(v1.2.1)",
    "title": "Velox Function Development(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/developers/velox-function-dev/",
    
    "relUrl": "/archives/v1.2.1/docs/developers/velox-function-dev/"
  },"259": {
    "doc": "Getting start with ClickHouse Backend(v1.2.1)",
    "title": "ClickHouse Backend",
    "content": "ClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP), which supports best in the industry query performance, while significantly reducing storage requirements through its innovative use of columnar storage and compression. We port ClickHouse ( based on version 23.1 ) as a library, called ‘libch.so’, and Gluten loads this library through JNI as the native engine. In this way, we don’t need to deploy a standalone ClickHouse Cluster, Spark uses Gluten as SparkPlugin to read and write ClickHouse MergeTree data. Architecture . The architecture of the ClickHouse backend is shown below: . | On Spark driver, Spark uses Gluten SparkPlugin to transform the physical plan to the Substrait plan, and then pass the Substrait plan to ClickHouse backend through JNI call on executors. | Based on Spark DataSource V2 interface, implementing a ClickHouse Catalog to support operating the ClickHouse tables, and then using Delta to save some metadata about ClickHouse like the MergeTree parts information, and also provide ACID transactions. | When querying from a ClickHouse table, it will fetch MergeTree parts information from Delta metadata and assign these parts into Spark partitions according to some strategies. | When writing data into a ClickHouse table, it will use ClickHouse library to write MergeTree parts data and collect these MergeTree parts information after writing successfully, and then save these MergeTree parts information into Delta metadata. ( The feature of writing MergeTree parts is coming soon. ) | On Spark executors, each executor will load the ‘libch.so’ through JNI when starting, and then call the operators according to the Substrait plan which is passed from Spark Driver, like reading data from the MergeTree parts, writing the MergeTree parts, filtering data, aggregating data and so on. | Currently, the ClickHouse backend only supports reading the MergeTree parts from local storage, it needs to use a high-performance shared file system to share a root bucket on every node of the cluster from the object storage, like JuiceFS. | . Development environment setup . In general, we use IDEA for Gluten development and CLion for ClickHouse backend development on Ubuntu 20. Prerequisites . Install the software required for compilation, run sudo ./ep/build-clickhouse/src/install_ubuntu.sh. Under the hood, it will install the following software: . | Clang 16.0 | cmake 3.20 or higher version | ninja-build 1.8.2 | . You can also refer to How-to-Build-ClickHouse-on-Linux. You need to install the following software manually: . | Java 8 | Maven 3.6.3 or higher version | Spark 3.2.2 or Spark 3.3.1 | . Then, get Gluten code: . git clone https://github.com/apache/incubator-gluten.git . Setup ClickHouse backend development environment . If you don’t care about development environment, you can skip this part. Otherwise, do: . | clone Kyligence/ClickHouse repo cd /to/some/place/ git clone --recursive --shallow-submodules -b clickhouse_backend https://github.com/Kyligence/ClickHouse.git . | Configure cpp-ch ${GLUTEN_SOURCE}/cpp-ch can be treated as an add-on of Kyligence/Clickhouse . First, initialize some configuration for this add-on: . export GLUTEN_SOURCE=/path/to/gluten export CH_SOURCE_DIR=/path/to/ClickHouse cmake -G Ninja -S ${GLUTEN_SOURCE}/cpp-ch -B ${GLUTEN_SOURCE}/cpp-ch/build_ch -DCH_SOURCE_DIR=${CH_SOURCE_DIR} \"-DCMAKE_C_COMPILER=$(command -v clang-16)\" \"-DCMAKE_CXX_COMPILER=$(command -v clang++-16)\" \"-DCMAKE_BUILD_TYPE=RelWithDebInfo\" . Next, you need to compile Kyligence/Clickhouse. There are two options: . | (Option 1) Use CLion . | Open ClickHouse repo | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; Toolchains, and then choose Bundled CMake, clang-16 as C Compiler, clang++-16 as C++ Compiler: . | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; CMake: . And then add these options into CMake options: . -DENABLE_PROTOBUF=ON -DENABLE_TESTS=OFF -DENABLE_JEMALLOC=ON -DENABLE_MULTITARGET_CODE=ON -DENABLE_EXTERN_LOCAL_ENGINE=ON . | Build ‘ch’ target on ClickHouse Project with Debug mode or Release mode: . If it builds with Release mode successfully, there is a library file called ‘libch.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-release/utils/extern-local-engine/’. If it builds with Debug mode successfully, there is a library file called ‘libchd.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-debug/utils/extern-local-engine/’. | . | (Option 2) Use command line cmake --build ${GLUTEN_SOURCE}/cpp-ch/build_ch --target build_ch . If it builds successfully, there is a library file called ‘libch.so’ in path ‘${GLUTEN_SOURCE}/cpp-ch/build/utils/extern-local-engine/’. | . Directly Compile ClickHouse backend . In case you don’t want a develop environment, you can use the following command to compile ClickHouse backend directly: . git clone https://github.com/apache/incubator-gluten.git cd incubator-gluten bash ./ep/build-clickhouse/src/build_clickhouse.sh . This will download Clickhouse for you and build everything. The target file is /path/to/gluten/cpp-ch/build/utils/extern-local-engine/libch.so. Compile Gluten . The prerequisites are the same as the one mentioned above. Compile Gluten with ClickHouse backend through maven: . | for Spark 3.2.2 | . git clone https://github.com/apache/incubator-gluten.git cd incubator-gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.2 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.2-jar-with-dependencies.jar . | for Spark 3.3.1 | . git clone https://github.com/apache/incubator-gluten.git cd incubator-gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.3 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.3-jar-with-dependencies.jar . Gluten in local Spark Thrift Server . Prepare working directory . | for Spark 3.2.2 | . tar zxf spark-3.2.2-bin-hadoop2.7.tgz cd spark-3.2.2-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.0.1.jar and delta-storage-2.0.1.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.0.1/delta-core_2.12-2.0.1.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.0.1/delta-storage-2.0.1.jar -P ./jars cp gluten-XXXXX-spark-3.2-jar-with-dependencies.jar jars/ . | for Spark 3.3.1 | . tar zxf spark-3.3.1-bin-hadoop2.7.tgz cd spark-3.3.1-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.2.0.jar and delta-storage-2.2.0.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar -P ./jars cp gluten-XXXXX-spark-3.3-jar-with-dependencies.jar jars/ . Query local data . Start Spark Thriftserver on local . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master local[3] \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 #connect to Spark Thriftserver by beeline bin/beeline -u jdbc:hive2://localhost:10000/ -n root . Query local MergeTree files . | Prepare data | . Currently, the feature of writing ClickHouse MergeTree parts by Spark is developing, so you need to use command ‘clickhouse-local’ to generate MergeTree parts data manually. We provide a python script to call the command ‘clickhouse-local’ to convert parquet data to MergeTree parts: . #install ClickHouse community version sudo apt-get install -y apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754 echo \"deb https://packages.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt install -y --allow-downgrades clickhouse-server=22.5.1.2079 clickhouse-client=22.5.1.2079 clickhouse-common-static=22.5.1.2079 #generate MergeTree parts mkdir -p /path_clickhouse_database/table_path/ python3 /path_to_clickhouse_backend_src/utils/local-engine/tool/parquet_to_mergetree.py --path=/tmp --source=/path_to_parquet_data/tpch-data-sf100/lineitem --dst=/path_clickhouse_database/table_path/lineitem . This python script will convert one parquet data file to one MergeTree parts. | Create a TPC-H lineitem table using ClickHouse DataSource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION '/path_clickhouse_database/table_path/lineitem'; . | TPC-H Q6 test | . SELECT sum(l_extendedprice * l_discount) AS revenue FROM lineitem WHERE l_shipdate &gt;= date'1994-01-01' AND l_shipdate &lt; date'1994-01-01' + interval 1 year AND l_discount BETWEEN 0.06 - 0.01 AND 0.06 + 0.01 AND l_quantity &lt; 24; . | Result . The DAG is shown on Spark UI as below: . | . Query local Parquet files . You can query local parquet files directly. -- query on a single file select * from parquet.`/your_data_root_dir/1.parquet`; -- query on a directly which has multiple files select * from parquet.`/your_data_roo_dir/`; . You can also create a TEMPORARY VIEW for parquet files. create or replace temporary view your_table_name using org.apache.spark.sql.parquet options( path \"/your_data_root_dir/\" ) . Query Parquet files in S3 . If you have parquet files in S3(either AWS S3 or S3 compatible storages like MINIO), you can query them directly. You need to add these additional configs to spark: . --config spark.hadoop.fs.s3a.endpoint=S3_ENDPOINT --config spark.hadoop.fs.s3a.path.style.access=true --config spark.hadoop.fs.s3a.access.key=YOUR_ACCESS_KEY --config spark.hadoop.fs.s3a.secret.key=YOUR_SECRET_KEY . where S3_ENDPOINT must follow the format of https://s3.region-code.amazonaws.com, e.g. https://s3.us-east-1.amazonaws.com (or `http://hostname:39090 for MINIO) . When you query the parquet files in S3, you need to add the prefix s3a:// to the path, e.g. s3a://your_bucket_name/path_to_your_parquet. Additionally, you can add these configs to enable local caching of S3 data. Each spark executor will have its own cache. Cache stealing between executors is not supported yet. --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.enabled=true --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.cache_path=/executor_local_folder_for_cache . Use beeline to execute queries . After start a spark thriftserver, we can use the beeline to connect to this server. # run a file /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -f &lt;your_sql_file&gt; # run a query /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -e '&lt;your_sql&gt;' # enter a interactive mode /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 . Query Hive tables in HDFS . Suppose that you have set up hive and hdfs, you can query the data on hive directly. | Copy hive-site.xml into /path_to_spark/conf/ | Copy hdfs-site.xml into /path_to_spark/conf/, and edit spark-env.sh | . # add this line into spark-env.sh export HADOOP_CONF_DIR=/path_to_spark/conf . | Start spark thriftserver with hdfs configurations | . hdfs_conf_file=/your_local_path/hdfs-site.xml cd spark-3.2.2-bin-hadoop2.7 # add a new option: spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf ./sbin/start-thriftserver.sh \\ --master local[3] \\ --files $hdfs_conf_file \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=./hdfs-site.xml . For example, you have a table demo_database.demo_table on the hive, you can run queries as below. select * from demo_database.demo_talbe; . Gluten in YARN cluster . We can to run a Spark SQL task by gluten on a yarn cluster as following . #!/bin/bash # The file contains the sql you want to run sql_file=/path/to/spark/sql/file export SPARK_HOME=/path/to/spark/home spark_cmd=$SPARK_HOME/bin/spark-sql # Define the path to libch.so ch_lib=/path/to/libch.so export LD_PRELOAD=$ch_lib # copy gluten jar file to $SPARK_HOME/jar gluten_jar=/path/to/gluten/jar/file cp $gluten_jar $SPARK_HOME/jar batchsize=20480 hdfs_conf=/path/to/hdfs-site.xml $spark_cmd \\ --name gluten_on_yarn --master yarn \\ --deploy-mode client \\ --files $ch_lib \\ --executor-cores 1 \\ --num-executors 2 \\ --executor-memory 10g \\ --conf spark.default.parallelism=4 \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=7g \\ --conf spark.driver.maxResultSize=2g \\ --conf spark.sql.autoBroadcastJoinThreshold=-1 \\ --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} \\ --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} \\ --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} \\ --conf spark.sql.broadcastTimeout=4800 \\ --conf spark.task.maxFailures=1 \\ --conf spark.excludeOnFailure.enabled=false \\ --conf spark.driver.maxResultSize=4g \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.dynamicAllocation.executorIdleTimeout=0s \\ --conf spark.sql.shuffle.partitions=112 \\ --conf spark.sql.sources.useV1SourceList=avro \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.gluten.sql.columnar.columnartorow=true \\ --conf spark.gluten.sql.columnar.loadnative=true \\ --conf spark.gluten.sql.columnar.libpath=$ch_lib \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=$hdfs_conf \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.logger.level=debug \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.executorEnv.LD_PRELOAD=$LD_PRELOAD \\ --conf spark.hadoop.input.connect.timeout=600000 \\ --conf spark.hadoop.input.read.timeout=600000 \\ --conf spark.hadoop.input.write.timeout=600000 \\ --conf spark.hadoop.dfs.client.log.severity=\"DEBUG2\" \\ --files $ch_lib \\ -f $sql_file . We also can use spark-submit to run a task. Benchmark with TPC-H 100 Q6 on Gluten with ClickHouse backend . This benchmark is tested on AWS EC2 cluster, there are 7 EC2 instances: . | Node Role | EC2 Type | Instances Count | Resources | AMI | . | Master | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . | Worker | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . Deploy on Cloud . | Tested on Spark Standalone cluster, its resources are shown below: . |   | CPU cores | Memory | Instances Count | . | Spark Worker | 15 | 60G | 6 | . | Prepare jars . Refer to Deploy Spark 3.2.2 . | Deploy gluten-core-XXXXX-jar-with-dependencies.jar . | . #deploy 'gluten-core-XXXXX-jar-with-dependencies.jar' to every node, and then cp gluten-core-XXXXX-jar-with-dependencies.jar /path_to_spark/jars/ . | Deploy ClickHouse library . Deploy ClickHouse library ‘libch.so’ to every worker node. | . Deploy JuiceFS . | JuiceFS uses Redis to save metadata, install redis firstly: | . wget https://download.redis.io/releases/redis-6.0.14.tar.gz sudo apt install build-essential tar -zxvf redis-6.0.14.tar.gz cd redis-6.0.14 make make install PREFIX=/home/ubuntu/redis6 cd .. rm -rf redis-6.0.14 #start redis server /home/ubuntu/redis6/bin/redis-server /home/ubuntu/redis6/redis.conf . | Use JuiceFS to format a S3 bucket and mount a volumn on every node . Please refer to The-JuiceFS-Command-Reference . | . wget https://github.com/juicedata/juicefs/releases/download/v0.17.5/juicefs-0.17.5-linux-amd64.tar.gz tar -zxvf juicefs-0.17.5-linux-amd64.tar.gz ./juicefs format --block-size 4096 --storage s3 --bucket https://s3.cn-northwest-1.amazonaws.com.cn/s3-gluten-tpch100/ --access-key \"XXXXXXXX\" --secret-key \"XXXXXXXX\" redis://:123456@master-ip:6379/1 gluten-tables #mount a volumn on every node ./juicefs mount -d --no-usage-report --no-syslog --attr-cache 7200 --entry-cache 7200 --dir-entry-cache 7200 --buffer-size 500 --prefetch 1 --open-cache 86400 --log /home/ubuntu/juicefs-logs/mount1.log --cache-dir /home/ubuntu/juicefs-cache/ --cache-size 102400 redis://:123456@master-ip:6379/1 /home/ubuntu/gluten/gluten_table #create a directory for lineitem table path mkdir -p /home/ubuntu/gluten/gluten_table/lineitem . Preparation . Please refer to Data-preparation to generate MergeTree parts data to the lineitem table path: /home/ubuntu/gluten/gluten_table/lineitem. Run Spark Thriftserver . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master spark://master-ip:7070 --deploy-mode client \\ --driver-memory 16g --driver-cores 4 \\ --total-executor-cores 90 --executor-memory 60g --executor-cores 15 \\ --conf spark.driver.memoryOverhead=8G \\ --conf spark.default.parallelism=90 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=536870912 \\ --conf spark.sql.parquet.filterPushdown=true \\ --conf spark.sql.parquet.enableVectorizedReader=true \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=42949672960 \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 . Test TPC-H Q6 with JMeter . | Create a lineitem table using clickhouse datasource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION 'file:///home/ubuntu/gluten/gluten_table/lineitem'; . | Run TPC-H Q6 test with JMeter . | Run TPC-H Q6 test 100 times in the first round; | Run TPC-H Q6 test 1000 times in the second round; | . | . Performance . The performance of Gluten + ClickHouse backend increases by about 1/3. |   | 70% | 80% | 90% | 99% | Avg | . | Spark + Parquet | 590ms | 592ms | 597ms | 609ms | 588ms | . | Spark + Gluten + ClickHouse backend | 402ms | 405ms | 409ms | 425ms | 399ms | . New CI System . https://opencicd.kyligence.com/job/Gluten/job/gluten-ci/ public read-only account：gluten/hN2xX3uQ4m . Celeborn support . Gluten with clickhouse backend supports Celeborn as remote shuffle service. Currently, the supported Celeborn versions are 0.3.x, 0.4.x and 0.5.0. Below introduction is used to enable this feature. First refer to this URL(https://github.com/apache/celeborn) to setup a celeborn cluster. When compiling the Gluten Java module, it’s required to enable celeborn profile, as follows: . mvn clean package -Pbackends-clickhouse -Pspark-3.3 -Pceleborn -DskipTests . Then add the Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . Columnar shuffle mode . We have two modes of columnar shuffle . | prefer cache | prefer spill | . Switch through the configuration spark.gluten.sql.columnar.backend.ch.shuffle.preferSpill, the default is false, enable prefer cache shuffle. In the prefer cache mode, as much memory as possible will be used to cache the shuffle data. When the memory is insufficient, spark will actively trigger the memory spill. You can also specify the threshold size through spark.gluten.sql.columnar.backend.ch.spillThreshold to Limit memory usage. The default value is 0MB, which means no limit on memory usage. ",
    "url": "/archives/v1.2.1/docs/getting-started/clickhouse-backend/#clickhouse-backend",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/clickhouse-backend/#clickhouse-backend"
  },"260": {
    "doc": "Getting start with ClickHouse Backend(v1.2.1)",
    "title": "Getting start with ClickHouse Backend(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/clickhouse-backend/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/clickhouse-backend/"
  },"261": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Supported Version",
    "content": "| Type | Version | . | Spark | 3.2.2, 3.3.1, 3.4.2, 3.5.1 | . | OS | Ubuntu20.04/22.04, Centos7/8 | . | jdk | openjdk8/jdk17 | . | scala | 2.12 | . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#supported-version",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#supported-version"
  },"262": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Prerequisite",
    "content": "Currently, Gluten+Velox backend is only tested on Ubuntu20.04/Ubuntu22.04/Centos7/Centos8. Other kinds of OS support are still in progress. The long term goal is to support several common OS and conda env deployment. Currently, the officially supported Spark versions are 3.2.2, 3.3.1, 3.4.2 and 3.5.1. We need to set up the JAVA_HOME env. Currently, Gluten supports java 8 and java 17. For x86_64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH . For aarch64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64 export PATH=$JAVA_HOME/bin:$PATH . Get gluten . ## config maven, like proxy in ~/.m2/settings.xml ## fetch gluten code git clone https://github.com/apache/incubator-gluten.git . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#prerequisite",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#prerequisite"
  },"263": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Build Gluten with Velox Backend",
    "content": "It’s recommended to use buildbundle-veloxbe.sh to build gluten in one script. Gluten build guide listed the parameters and their default value of build command for your reference. For x86_64 build . First time build for all supported spark versions./dev/buildbundle-veloxbe.sh . After a complete build, if only some gluten code is changed, you can use the following command to skip building velox/arrow and setting up build dependencies./dev/buildbundle-veloxbe.sh --enable_ep_cache=ON --build_arrow=OFF --run_setup_script=OFF . For aarch64 build: . export CPU_TARGET=\"aarch64\" ./dev/builddeps-veloxbe.sh . Build Velox separately . Currently, Gluten is using a forked Velox which is daily updated based on upstream Velox. Scripts under /path/to/gluten/ep/build-velox/src provide get_velox.sh and build_velox.sh to build Velox separately, you could use these scripts with custom repo/branch/location. Velox provides arrow/parquet lib. Gluten cpp module need a required VELOX_HOME parsed by –velox_home, if you specify custom ep location, make sure these variables be passed correctly. ## fetch Velox and compile ./dev/builddeps-veloxbe.sh build_velox ## compile Gluten cpp module ./dev/builddeps-veloxbe.sh build_gluten_cpp ## compile Gluten java module and create package jar cd /path/to/gluten # For spark3.2.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.2 -DskipTests # For spark3.3.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.3 -DskipTests # For spark3.4.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.4 -DskipTests # For spark3.5.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.5 -DskipTests . Notes： Building Velox may fail caused by oom. You can prevent this failure by adjusting NUM_THREADS (e.g., export NUM_THREADS=4) before building Gluten/Velox. Once building successfully, the Jar file will be generated in the directory: package/target/&lt;gluten-jar&gt; for Spark 3.2.x/Spark 3.3.x/Spark 3.4.x/Spark 3.5.x. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-velox-backend"
  },"264": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Dependency library deployment",
    "content": "With config enable_vcpkg=ON, the dependency libraries will be built and statically linked into libvelox.so and libgluten.so, which is packed into the gluten-jar. In this way, only the gluten-jar is needed to add to spark.&lt;driver|executor&gt;.extraClassPath and spark will deploy the jar to each worker node. It’s better to build the static version using a clean docker image without any extra libraries installed. On host with some libraries like jemalloc installed, the script may crash with odd message. You may need to uninstall those libraries to get a clean host. With config enable_vcpkg=OFF, not all dependency libraries will be statically linked, instead the script will install the libraries to system then pack the dependency libraries into another jar named gluten-package-${Maven-artifact-version}.jar. Then you need to add the jar to extraClassPath and set spark.gluten.loadLibFromJar=true. Otherwise, you need to install shared dependency libraries on each worker node. You may find the libraries list from the gluten-package jar. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#dependency-library-deployment",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#dependency-library-deployment"
  },"265": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "HDFS support",
    "content": "Hadoop hdfs support is ready via the libhdfs3 library. The libhdfs3 provides native API for Hadoop I/O without the drawbacks of JNI. It also provides advanced authentication like Kerberos based. Please note this library has several dependencies which may require extra installations on Driver and Worker node. Build with HDFS support . To build Gluten with HDFS support, below command is suggested: . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_hdfs=ON . Configuration about HDFS support . HDFS uris (hdfs://host:port) will be extracted from a valid hdfs file path to initialize hdfs client, you do not need to specify it explicitly. libhdfs3 need a configuration file and example here, this file is a bit different from hdfs-site.xml and core-site.xml. Download that example config file to local and do some needed modifications to support HA or else, then set env variable like below to use it, or upload it to HDFS to use, more details here. // Spark local mode export LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode --conf spark.executorEnv.LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode and upload hdfs config file cp /path/to/hdfs-client.xml hdfs-client.xml --files hdfs-client.xml . One typical deployment on Spark/HDFS cluster is to enable short-circuit reading. Short-circuit reads provide a substantial performance boost to many applications. By default libhdfs3 does not set the default hdfs domain socket path to support HDFS short-circuit read. If this feature is required in HDFS setup, users may need to setup the domain socket path correctly by patching the libhdfs3 source code or by setting the correct config environment. In Gluten the short-circuit domain socket path is set to “/var/lib/hadoop-hdfs/dn_socket” in build_velox.sh So we need to make sure the folder existed and user has write access as below script. sudo mkdir -p /var/lib/hadoop-hdfs/ sudo chown &lt;sparkuser&gt;:&lt;sparkuser&gt; /var/lib/hadoop-hdfs/ . You also need to add configuration to the “hdfs-site.xml” as below: . &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt; . Kerberos support . Here are two steps to enable kerberos. | Make sure the hdfs-client.xml contains | . &lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;/property&gt; . | Specify the environment variable KRB5CCNAME and upload the kerberos ticket cache file | . --conf spark.executorEnv.KRB5CCNAME=krb5cc_0000 --files /tmp/krb5cc_0000 . The ticket cache file can be found by klist. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#hdfs-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#hdfs-support"
  },"266": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Azure Blob File System (ABFS) support",
    "content": "Velox supports ABFS with the open source Azure SDK for C++ and Gluten uses the Velox ABFS connector to connect with ABFS. The build option for ABFS (enable_abfs) must be set to enable this feature as listed below. cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_abfs=ON . Please refer Velox ABFS part for more detailed configurations. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#azure-blob-file-system-abfs-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#azure-blob-file-system-abfs-support"
  },"267": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "AWS S3 support",
    "content": "Velox supports S3 with the open source AWS C++ SDK and Gluten uses Velox S3 connector to connect with S3. A new build option for S3(enable_s3) is added. Below command is used to enable this feature . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_s3=ON . Currently there are several ways to asscess S3 in Spark. Please refer Velox S3 part for more detailed configurations . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#aws-s3-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#aws-s3-support"
  },"268": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Celeborn support",
    "content": "Gluten with velox backend supports Celeborn as remote shuffle service. Currently, the supported Celeborn versions are 0.3.x, 0.4.x and 0.5.0. Below introduction is used to enable this feature. First refer to this URL(https://github.com/apache/celeborn) to setup a celeborn cluster. When compiling the Gluten Java module, it’s required to enable celeborn profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Pceleborn -DskipTests . Then add the Gluten and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar | Gluten: gluten-velox-bundle-spark3.x_2.12-xx_xx_xx-SNAPSHOT.jar, gluten-celeborn-package-xx-SNAPSHOT.jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#celeborn-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#celeborn-support"
  },"269": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Uniffle support",
    "content": "Uniffle with velox backend supports Uniffle as remote shuffle service. Currently, the supported Uniffle versions are 0.8.0. First refer to this URL(https://uniffle.apache.org/docs/intro) to get start with uniffle. When compiling the Gluten Java module, it’s required to enable uniffle profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Puniffle -DskipTests . Then add the Uniffle and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Uniffle: rss-client-spark3-shaded-[uniffleVersion].jar | Gluten: gluten-uniffle-velox-xxx-SNAPSHOT-3.x.jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.uniffle.UniffleShuffleManager # uniffle coordinator address spark.rss.coordinator.quorum ip:port # Support for Spark AQE spark.sql.adaptive.localShuffleReader.enabled false spark.shuffle.service.enabled false # Uniffle support mutilple storage types, you can choose one of them. # Such as MEMORY,LOCALFILE,MEMORY_LOCALFILE,HDFS,MEMORY_HDFS,LOCALFILE_HDFS,MEMORY_LOCALFILE_HDFS spark.rss.storage.type LOCALFILE_HDFS # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/incubator-uniffle/tree/master/patch/spark) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#uniffle-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#uniffle-support"
  },"270": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "DeltaLake Support",
    "content": "Gluten with velox backend supports DeltaLake table. How to use . First of all, compile gluten-delta module by a delta profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Pdelta -DskipTests . Then, put the additional gluten-delta-XX-SNAPSHOT.jar to the class path (usually it’s $SPARK_HOME/jars). The gluten-delta jar is in gluten-delta/target directory. After the two steps, you can query delta table by gluten/velox without scan’s fallback. Gluten with velox backends also support the column mapping of delta tables. About column mapping, see more here. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#deltalake-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#deltalake-support"
  },"271": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Iceberg Support",
    "content": "Gluten with velox backend supports Iceberg table. Currently, only reading COW (Copy-On-Write) tables is supported. How to use . First of all, compile gluten-iceberg module by a iceberg profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Piceberg -DskipTests . Once built successfully, iceberg features will be included in gluten-velox-bundle-X jar. Then you can query iceberg table by gluten/velox without scan’s fallback. After the two steps, you can query iceberg table by gluten/velox without scan’s fallback. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#iceberg-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#iceberg-support"
  },"272": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Coverage",
    "content": "Spark3.3 has 387 functions in total. ~240 are commonly used. To get the support status of all Spark built-in functions, please refer to Velox Backend’s Supported Operators &amp; Functions. Velox doesn’t support ANSI mode), so as Gluten. Once ANSI mode is enabled in Spark config, Gluten will fallback to Vanilla Spark. To identify what can be offloaded in a query and detailed fallback reasons, user can follow below steps to retrieve corresponding logs. 1) Enable Gluten by proper [configuration](https://github.com/apache/incubator-gluten/blob/main/docs/Configuration.md). 2) Disable Spark AQE to trigger plan validation in Gluten spark.sql.adaptive.enabled = false 3) Check physical plan sparkSession.sql(\"your_sql\").explain() . With above steps, you will get a physical plan output like: . == Physical Plan == -Execute InsertIntoHiveTable (7) +- Coalesce (6) +- VeloxColumnarToRowExec (5) +- ^ ProjectExecTransformer (3) +- GlutenRowToArrowColumnar (2) +- Scan hive default.extracted_db_pins (1) . GlutenRowToArrowColumnar/VeloxColumnarToRowExec indicates there is a fallback operator before or after it. And you may find fallback reason like below in logs. native validation failed due to: in ProjectRel, Scalar function name not registered: get_struct_field, called with arguments: (ROW&lt;col_0:INTEGER,col_1:BIGINT,col_2:BIGINT&gt;, INTEGER). In the above, the symbol ^ indicates a plan is offloaded to Velox in a stage. In Spark DAG, all such pipelined plans (consecutive plans marked with ^) are plotted inside an umbrella node named WholeStageCodegenTransformer (It’s not codegen node. The naming is just for making it well plotted like Spark Whole Stage Codegen). ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#coverage",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#coverage"
  },"273": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Spill (Experimental)",
    "content": "Velox backend supports spilling-to-disk. Using the following configuration options to customize spilling: . | Name | Default Value | Description | . | spark.gluten.sql.columnar.backend.velox.spillStrategy | auto | none: Disable spill on Velox backend; auto: Let Spark memory manager manage Velox’s spilling | . | spark.gluten.sql.columnar.backend.velox.spillFileSystem | local | The filesystem used to store spill data. local: The local file system. heap-over-local: Write files to JVM heap if having extra heap space. Otherwise write to local file system. | . | spark.gluten.sql.columnar.backend.velox.aggregationSpillEnabled | true | Whether spill is enabled on aggregations | . | spark.gluten.sql.columnar.backend.velox.joinSpillEnabled | true | Whether spill is enabled on joins | . | spark.gluten.sql.columnar.backend.velox.orderBySpillEnabled | true | Whether spill is enabled on sorts | . | spark.gluten.sql.columnar.backend.velox.maxSpillLevel | 4 | The max allowed spilling level with zero being the initial spilling level | . | spark.gluten.sql.columnar.backend.velox.maxSpillFileSize | 1GB | The max allowed spill file size. If it is zero, then there is no limit | . | spark.gluten.sql.columnar.backend.velox.spillStartPartitionBit | 29 | The start partition bit which is used with ‘spillPartitionBits’ together to calculate the spilling partition number | . | spark.gluten.sql.columnar.backend.velox.spillPartitionBits | 2 | The number of bits used to calculate the spilling partition number. The number of spilling partitions will be power of two | . | spark.gluten.sql.columnar.backend.velox.spillableReservationGrowthPct | 25 | The spillable memory reservation growth percentage of the previous memory reservation size | . | spark.gluten.sql.columnar.backend.velox.spillThreadNum | 0 | (Experimental) The thread num of a dedicated thread pool to do spill | . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#spill-experimental",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#spill-experimental"
  },"274": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Velox User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF)",
    "content": "Please check the VeloxNativeUDF.md for more detailed usage and configurations. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf"
  },"275": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "High-Bandwidth Memory (HBM) support",
    "content": "Gluten supports allocating memory on HBM. This feature is optional and is disabled by default. It is implemented on top of Memkind library. You can refer to memkind’s readme for more details. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#high-bandwidth-memory-hbm-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#high-bandwidth-memory-hbm-support"
  },"276": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Build Gluten with HBM",
    "content": "Gluten will internally build and link to a specific version of Memkind library and hwloc. Other dependencies should be installed on Driver and Worker node first: . sudo apt install -y autoconf automake g++ libnuma-dev libtool numactl unzip libdaxctl-dev . After the set-up, you can now build Gluten with HBM. Below command is used to enable this feature . cd /path/to/gluten ## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1./dev/buildbundle-veloxbe.sh --enable_hbm=ON . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-hbm",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-hbm"
  },"277": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Configure and enable HBM in Spark Application",
    "content": "At runtime, MEMKIND_HBW_NODES enviroment variable is detected for configuring HBM NUMA nodes. For the explaination to this variable, please refer to memkind’s manual page. This can be set for all executors through spark conf, e.g. --conf spark.executorEnv.MEMKIND_HBW_NODES=8-15. Note that memory allocation fallback is also supported and cannot be turned off. If HBM is unavailable or fills up, the allocator will use default(DDR) memory. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#configure-and-enable-hbm-in-spark-application",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#configure-and-enable-hbm-in-spark-application"
  },"278": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Intel® QuickAssist Technology (QAT) support",
    "content": "Gluten supports using Intel® QuickAssist Technology (QAT) for data compression during Spark Shuffle. It benefits from QAT Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on QAT driver library and QATzip library. Please manually download QAT driver for your system, and follow its README to build and install on all Driver and Worker node: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#intel-quickassist-technology-qat-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#intel-quickassist-technology-qat-support"
  },"279": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Software Requirements",
    "content": ". | Download QAT driver for your system, and follow its README to build and install on all Driver and Worker nodes: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. | Below compression libraries need to be installed on all Driver and Worker nodes: . | Zlib* library of version 1.2.7 or higher | ZSTD* library of version 1.5.4 or higher | LZ4* library | . | . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#software-requirements",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#software-requirements"
  },"280": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Build Gluten with QAT",
    "content": ". | Setup ICP_ROOT environment variable to the directory where QAT driver is extracted. This environment variable is required during building Gluten and running Spark applications. It’s recommended to put it in .bashrc on Driver and Worker nodes. | . echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc source ~/.bashrc # Also set for root if running as non-root user sudo su - echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc exit . | This step is required if your application is running as Non-root user. The users must be added to the ‘qat’ group after QAT drvier is installed. And change the amount of max locked memory for the username that is included in the group name. This can be done by specifying the limit in /etc/security/limits.conf. | . sudo su - usermod -aG qat username # need relogin to take effect # To set 500MB add a line like this in /etc/security/limits.conf echo \"@qat - memlock 500000\" &gt;&gt; /etc/security/limits.conf exit . | Enable huge page. This step is required to execute each time after system reboot. We recommend using systemctl to manage at system startup. You change the values for “max_huge_pages” and “max_huge_pages_per_process” to make sure there are enough resources for your workload. As for Spark applications, one process matches one executor. Within the executor, every task is allocated a maximum of 5 huge pages. | . sudo su - cat &lt;&lt; EOF &gt; /usr/local/bin/qat_startup.sh #!/bin/bash echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages rmmod usdm_drv insmod $ICP_ROOT/build/usdm_drv.ko max_huge_pages=1024 max_huge_pages_per_process=32 EOF chmod +x /usr/local/bin/qat_startup.sh cat &lt;&lt; EOF &gt; /etc/systemd/system/qat_startup.service [Unit] Description=Configure QAT [Service] ExecStart=/usr/local/bin/qat_startup.sh [Install] WantedBy=multi-user.target EOF systemctl enable qat_startup.service systemctl start qat_startup.service # setup immediately systemctl status qat_startup.service exit . | After the setup, you are now ready to build Gluten with QAT. Use the command below to enable this feature: | . cd /path/to/gluten ## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1./dev/buildbundle-veloxbe.sh --enable_qat=ON . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-qat",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-qat"
  },"281": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Enable QAT with Gzip/Zstd for shuffle compression",
    "content": ". | To offload shuffle compression into QAT, first make sure you have the right QAT configuration file at /etc/4xxx_devX.conf. We provide a example configuration file. This configuration sets up to 4 processes that can bind to 1 QAT, and each process can use up to 16 QAT DC instances. | . ## run as root ## Overwrite QAT configuration file. cd /etc for i in {0..7}; do echo \"4xxx_dev$i.conf\"; done | xargs -i cp -f /path/to/gluten/docs/qat/4x16.conf {} ## Restart QAT after updating configuration files. adf_ctl restart . | Check QAT status and make sure the status is up | . adf_ctl status . The output should be like: . Checking status of all devices. There is 8 QAT acceleration device(s) in the system: qat_dev0 - type: 4xxx, inst_id: 0, node_id: 0, bsf: 0000:6b:00.0, #accel: 1 #engines: 9 state: up qat_dev1 - type: 4xxx, inst_id: 1, node_id: 1, bsf: 0000:70:00.0, #accel: 1 #engines: 9 state: up qat_dev2 - type: 4xxx, inst_id: 2, node_id: 2, bsf: 0000:75:00.0, #accel: 1 #engines: 9 state: up qat_dev3 - type: 4xxx, inst_id: 3, node_id: 3, bsf: 0000:7a:00.0, #accel: 1 #engines: 9 state: up qat_dev4 - type: 4xxx, inst_id: 4, node_id: 4, bsf: 0000:e8:00.0, #accel: 1 #engines: 9 state: up qat_dev5 - type: 4xxx, inst_id: 5, node_id: 5, bsf: 0000:ed:00.0, #accel: 1 #engines: 9 state: up qat_dev6 - type: 4xxx, inst_id: 6, node_id: 6, bsf: 0000:f2:00.0, #accel: 1 #engines: 9 state: up qat_dev7 - type: 4xxx, inst_id: 7, node_id: 7, bsf: 0000:f7:00.0, #accel: 1 #engines: 9 state: up . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip # Valid options are gzip and zstd --conf spark.gluten.sql.columnar.shuffle.codecBackend=qat . | You can use below command to check whether QAT is working normally at run-time. The value of fw_counters should continue to increase during shuffle. | . while :; do cat /sys/kernel/debug/qat_4xxx_0000:6b:00.0/fw_counters; sleep 1; done . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#enable-qat-with-gzipzstd-for-shuffle-compression",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#enable-qat-with-gzipzstd-for-shuffle-compression"
  },"282": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "QAT driver references",
    "content": "Documentation . README Text Files (README_QAT20.L.1.0.0-00021.txt) . Release Notes . Check out the Intel® QuickAssist Technology Software for Linux* - Release Notes for the latest changes in this release. Getting Started Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Getting Started Guide for detailed installation instructions. Programmer’s Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Programmer’s Guide for software usage guidelines. For more Intel® QuickAssist Technology resources go to Intel® QuickAssist Technology (Intel® QAT) . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#qat-driver-references",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#qat-driver-references"
  },"283": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Intel® In-memory Analytics Accelerator (IAA/IAX) support",
    "content": "Similar to Intel® QAT, Gluten supports using Intel® In-memory Analytics Accelerator (IAA, also called IAX) for data compression during Spark Shuffle. It benefits from IAA Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on Intel® QPL. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#intel-in-memory-analytics-accelerator-iaaiax-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#intel-in-memory-analytics-accelerator-iaaiax-support"
  },"284": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Build Gluten with IAA",
    "content": "Gluten will internally build and link to a specific version of QPL library, but extra environment setup is still required. Please refer to QPL Installation Guide to install dependencies and configure accelerators. This step is required if your application is running as Non-root user. Create a group for the users who have privilege to use IAA, and grant group iaa read/write access to the IAA Work-Queues. sudo groupadd iaa sudo usermod -aG iaa username # need to relogin sudo chgrp -R iaa /dev/iax sudo chmod -R g+rw /dev/iax . After the set-up, you can now build Gluten with QAT. Below command is used to enable this feature . cd /path/to/gluten ## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1./dev/buildbundle-veloxbe.sh --enable_iaa=ON . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-iaa",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#build-gluten-with-iaa"
  },"285": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Enable IAA with Gzip Compression for shuffle compression",
    "content": ". | To enable QAT at run-time, first make sure you have configured the IAA Work-Queues correctly, and the file permissions of /dev/iax/wqX.0 are correct. | . sudo ls -l /dev/iax . The output should be like: . total 0 crw-rw---- 1 root iaa 509, 0 Apr 5 18:54 wq1.0 crw-rw---- 1 root iaa 509, 5 Apr 5 18:54 wq11.0 crw-rw---- 1 root iaa 509, 6 Apr 5 18:54 wq13.0 crw-rw---- 1 root iaa 509, 7 Apr 5 18:54 wq15.0 crw-rw---- 1 root iaa 509, 1 Apr 5 18:54 wq3.0 crw-rw---- 1 root iaa 509, 2 Apr 5 18:54 wq5.0 crw-rw---- 1 root iaa 509, 3 Apr 5 18:54 wq7.0 crw-rw---- 1 root iaa 509, 4 Apr 5 18:54 wq9.0 . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip --conf spark.gluten.sql.columnar.shuffle.codecBackend=iaa . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#enable-iaa-with-gzip-compression-for-shuffle-compression",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#enable-iaa-with-gzip-compression-for-shuffle-compression"
  },"286": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "IAA references",
    "content": "Intel® IAA Enabling Guide . Check out the Intel® In-Memory Analytics Accelerator (Intel® IAA) Enabling Guide . Intel® QPL Documentation . Check out the Intel® Query Processing Library (Intel® QPL) Documentation . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#iaa-references",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#iaa-references"
  },"287": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Test TPC-H or TPC-DS on Gluten with Velox backend",
    "content": "All TPC-H and TPC-DS queries are supported in Gluten Velox backend. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend"
  },"288": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Data preparation",
    "content": "The data generation scripts are TPC-H dategen script and TPC-DS dategen script. The used TPC-H and TPC-DS queries are the original ones, and can be accessed from TPC-DS queries and TPC-H queries. Some other versions of TPC-DS queries are also provided, but are not recommended for testing, including: . | the modified TPC-DS queries with “Decimal-to-Double”: TPC-DS non-decimal queries (outdated). | . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#data-preparation",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#data-preparation"
  },"289": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Submit the Spark SQL job",
    "content": "Submit test script from spark-shell. You can find the scala code to Run TPC-H as an example. Please remember to modify the location of TPC-H files as well as TPC-H queries before you run the testing. var parquet_file_path = \"/PATH/TO/TPCH_PARQUET_PATH\" var gluten_root = \"/PATH/TO/GLUTEN\" . Below script shows an example about how to run the testing, you should modify the parameters such as executor cores, memory, offHeap size based on your environment. export GLUTEN_JAR = /PATH/TO/GLUTEN/package/target/&lt;gluten-jar&gt; cat tpch_parquet.scala | spark-shell --name tpch_powertest_velox \\ --master yarn --deploy-mode client \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.driver.extraClassPath=${GLUTEN_JAR} \\ --conf spark.executor.extraClassPath=${GLUTEN_JAR} \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --num-executors 6 \\ --executor-cores 6 \\ --driver-memory 20g \\ --executor-memory 25g \\ --conf spark.executor.memoryOverhead=5g \\ --conf spark.driver.maxResultSize=32g . Refer to Gluten configuration for more details. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#submit-the-spark-sql-job",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#submit-the-spark-sql-job"
  },"290": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Result",
    "content": "wholestagetransformer indicates that the offloading works. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#result",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#result"
  },"291": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Performance",
    "content": "Below table shows the TPC-H Q1 and Q6 Performance in a multiple-thread test (–num-executors 6 –executor-cores 6) for Velox and vanilla Spark. Both Parquet and ORC datasets are sf1024. | Query Performance (s) | Velox (ORC) | Vanilla Spark (Parquet) | Vanilla Spark (ORC) | . | TPC-H Q6 | 13.6 | 21.6 | 34.9 | . | TPC-H Q1 | 26.1 | 76.7 | 84.9 | . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#performance",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#performance"
  },"292": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "External reference setup",
    "content": "TO ease your first-hand experience of using Gluten, we have set up an external reference cluster. If you are interested, please contact Weiting.Chen@intel.com. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#external-reference-setup",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#external-reference-setup"
  },"293": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Gluten UI",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#gluten-ui",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#gluten-ui"
  },"294": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Gluten event",
    "content": "Gluten provides two events GlutenBuildInfoEvent and GlutenPlanFallbackEvent: . | GlutenBuildInfoEvent, it contains the Gluten build information so that we are able to be aware of the environment when doing some debug. It includes Java Version, Scala Version, GCC Version, Gluten Version, Spark Version, Hadoop Version, Gluten Revision, Backend, Backend Revision, etc. | GlutenPlanFallbackEvent, it contains the fallback information for each query execution. Note, if the query execution is in AQE, then Gluten will post it for each stage. | . Developers can register SparkListener to handle these two Gluten events. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#gluten-event",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#gluten-event"
  },"295": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "SQL tab",
    "content": "Gluten provides a tab based on Spark UI, named Gluten SQL / DataFrame . This tab contains two parts: . | The Gluten build information. | SQL/Dataframe queries fallback information. | . If you want to disable Gluten UI, add a config when submitting --conf spark.gluten.ui.enabled=false. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#sql-tab",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#sql-tab"
  },"296": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "History server",
    "content": "Gluten UI also supports Spark history server. Add gluten-ui jar into the history server classpath, e.g., $SPARK_HOME/jars, then restart history server. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#history-server",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#history-server"
  },"297": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Native plan string",
    "content": "Gluten supports inject native plan string into Spark explain with formatted mode by setting --conf spark.gluten.sql.injectNativePlanStringToExplain=true. Here is an example, how Gluten show the native plan string. (9) WholeStageCodegenTransformer (2) Input [6]: [c1#0L, c2#1L, c3#2L, c1#3L, c2#4L, c3#5L] Arguments: false Native Plan: -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#native-plan-string",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#native-plan-string"
  },"298": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Native plan with stats",
    "content": "Gluten supports print native plan with stats to executor system output stream by setting --conf spark.gluten.sql.debug=true. Note that, the plan string with stats is task level which may cause executor log size big. Here is an example, how Gluten show the native plan string with stats. I20231121 10:19:42.348845 90094332 WholeStageResultIterator.cc:220] Native Plan with stats for: [Stage: 1 TID: 16] -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 10.58us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 queuedWallNanos sum: 2.00us, count: 1, min: 2.00us, max: 2.00us runningAddInputWallNanos sum: 626ns, count: 1, min: 626ns, max: 626ns runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 5.54us, count: 1, min: 5.54us, max: 5.54us -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 223.00us, Blocked wall time: 0ns, Peak memory: 93.12KB, Memory allocations: 15 HashBuild: Input: 10 rows (960B, 10 batches), Output: 0 rows (0B, 0 batches), Cpu time: 185.67us, Blocked wall time: 0ns, Peak memory: 68.00KB, Memory allocations: 2, Threads: 1 distinctKey0 sum: 4, count: 1, min: 4, max: 4 hashtable.capacity sum: 4, count: 1, min: 4, max: 4 hashtable.numDistinct sum: 10, count: 1, min: 10, max: 10 hashtable.numRehashes sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 4, count: 1, min: 4, max: 4 runningAddInputWallNanos sum: 1.27ms, count: 1, min: 1.27ms, max: 1.27ms runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 1.29us, count: 1, min: 1.29us, max: 1.29us H23/11/21 10:19:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 13) in 335 ms on 10.221.97.35 (executor driver) (1/10) ashProbe: Input: 9 rows (864B, 3 batches), Output: 27 rows (3.56KB, 3 batches), Cpu time: 37.33us, Blocked wall time: 0ns, Peak memory: 25.12KB, Memory allocations: 13, Threads: 1 dynamicFiltersProduced sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningAddInputWallNanos sum: 4.54us, count: 1, min: 4.54us, max: 4.54us runningFinishWallNanos sum: 83ns, count: 1, min: 83ns, max: 83ns runningGetOutputWallNanos sum: 29.08us, count: 1, min: 29.08us, max: 29.08us -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT Input: 9 rows (864B, 3 batches), Output: 9 rows (864B, 3 batches), Cpu time: 630.75us, Blocked wall time: 0ns, Peak memory: 2.44KB, Memory allocations: 63, Threads: 1, Splits: 3 dataSourceWallNanos sum: 102.00us, count: 1, min: 102.00us, max: 102.00us dynamicFiltersAccepted sum: 1, count: 1, min: 1, max: 1 flattenStringDictionaryValues sum: 0, count: 1, min: 0, max: 0 ioWaitNanos sum: 312.00us, count: 1, min: 312.00us, max: 312.00us localReadBytes sum: 0B, count: 1, min: 0B, max: 0B numLocalRead sum: 0, count: 1, min: 0, max: 0 numPrefetch sum: 0, count: 1, min: 0, max: 0 numRamRead sum: 0, count: 1, min: 0, max: 0 numStorageRead sum: 6, count: 1, min: 6, max: 6 overreadBytes sum: 0B, count: 1, min: 0B, max: 0B prefetchBytes sum: 0B, count: 1, min: 0B, max: 0B queryThreadIoLatency sum: 12, count: 1, min: 12, max: 12 ramReadBytes sum: 0B, count: 1, min: 0B, max: 0B runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 125ns, count: 1, min: 125ns, max: 125ns runningGetOutputWallNanos sum: 1.07ms, count: 1, min: 1.07ms, max: 1.07ms skippedSplitBytes sum: 0B, count: 1, min: 0B, max: 0B skippedSplits sum: 0, count: 1, min: 0, max: 0 skippedStrides sum: 0, count: 1, min: 0, max: 0 storageReadBytes sum: 3.44KB, count: 1, min: 3.44KB, max: 3.44KB totalScanTime sum: 0ns, count: 1, min: 0ns, max: 0ns -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Input: 0 rows (0B, 0 batches), Output: 10 rows (960B, 10 batches), Cpu time: 1.03ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 54.62us, count: 1, min: 54.62us, max: 54.62us runningGetOutputWallNanos sum: 1.10ms, count: 1, min: 1.10ms, max: 1.10ms . ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#native-plan-with-stats",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#native-plan-with-stats"
  },"299": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Gluten Implicits",
    "content": "Gluten provides a helper class to get the fallback summary from a Spark Dataset. import org.apache.spark.sql.execution.GlutenImplicits._ val df = spark.sql(\"SELECT * FROM t\") df.fallbackSummary . Note that, if AQE is enabled, but the query is not materialized, then it will re-plan the query execution with disabled AQE. It is a workaround to get the final plan, and it may cause the inconsistent results with a materialized query. However, we have no choice. ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/#gluten-implicits",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/#gluten-implicits"
  },"300": {
    "doc": "Getting Start with Velox Backend(v1.2.1)",
    "title": "Getting Start with Velox Backend(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/velox-backend/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/velox-backend/"
  },"301": {
    "doc": "Using ABFS with Gluten(v1.2.1)",
    "title": "Working with ABFS",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/abfs/#working-with-abfs",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/abfs/#working-with-abfs"
  },"302": {
    "doc": "Using ABFS with Gluten(v1.2.1)",
    "title": "Configuring ABFS Access Token",
    "content": "To configure access to your storage account, replace with the name of your account. This property aligns with Spark configurations. By setting this config multiple times using different storage account names, you can access multiple ABFS accounts. spark.hadoop.fs.azure.account.key.&lt;storage-account&gt;.dfs.core.windows.net XXXXXXXXX . Other authentatication methods are not yet supported. ",
    "url": "/archives/v1.2.1/docs/getting-started/abfs/#configuring-abfs-access-token",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/abfs/#configuring-abfs-access-token"
  },"303": {
    "doc": "Using ABFS with Gluten(v1.2.1)",
    "title": "Local Caching support",
    "content": "Velox supports a local cache when reading data from ABFS. Please refer Velox Local Cache part for more detailed configurations. ",
    "url": "/archives/v1.2.1/docs/getting-started/abfs/#local-caching-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/abfs/#local-caching-support"
  },"304": {
    "doc": "Using ABFS with Gluten(v1.2.1)",
    "title": "Using ABFS with Gluten(v1.2.1)",
    "content": "ABFS is an important data store for big data users. This doc discusses config details and use cases of Gluten with ABFS. To use an ABFS account as your data source, please ensure you use the listed ABFS config in your spark-defaults.conf. If you would like to authenticate with ABFS using additional auth mechanisms, please reach out using the ‘Issues’ tab. ",
    "url": "/archives/v1.2.1/docs/getting-started/abfs/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/abfs/"
  },"305": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Working with GCS",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#working-with-gcs",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#working-with-gcs"
  },"306": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Installing the gcloud CLI",
    "content": "To access GCS Objects using Gluten and Velox, first you have to [download an install the gcloud CLI] (https://cloud.google.com/sdk/docs/install). ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#installing-the-gcloud-cli",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#installing-the-gcloud-cli"
  },"307": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Configuring GCS using a user account",
    "content": "This is recommended for regular users, follow the instructions to authorize a user account. After these steps, no specific configuration is required for Gluten, since the authorization was handled entirely by the gcloud tool. ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-using-a-user-account",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-using-a-user-account"
  },"308": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Configuring GCS using a credential file",
    "content": "For workloads that need to be fully automated, manually authorizing can be problematic. For such cases it is better to use a json file with the credentials. This is described in the [instructions to configure a service account]https://cloud.google.com/sdk/docs/authorizing#service-account. Such json file with the credetials can be passed to Gluten: . spark.hadoop.fs.gs.auth.type SERVICE_ACCOUNT_JSON_KEYFILE spark.hadoop.fs.gs.auth.service.account.json.keyfile // path to the json file with the credentials. ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-using-a-credential-file",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-using-a-credential-file"
  },"309": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Configuring GCS endpoints",
    "content": "For cases when a GCS mock is used, an optional endpoint can be provided: . spark.hadoop.fs.gs.storage.root.url // url to the mock gcs service including starting with http or https . ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-endpoints",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-endpoints"
  },"310": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Configuring GCS max retry count",
    "content": "For cases when a transient server error is detected, GCS can be configured to keep retrying until a number of transient error is detected. spark.hadoop.fs.gs.http.max.retry // number of times to keep retrying unless a non-transient error is detected . ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-max-retry-count",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-max-retry-count"
  },"311": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Configuring GCS max retry time",
    "content": "For cases when a transient server error is detected, GCS can be configured to keep retrying until the retry loop exceeds a prescribed duration. spark.hadoop.fs.gs.http.max.retry-time // a string representing the time keep retring (10s, 1m, etc). ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-max-retry-time",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/#configuring-gcs-max-retry-time"
  },"312": {
    "doc": "Using GCS with Gluten(v1.2.1)",
    "title": "Using GCS with Gluten(v1.2.1)",
    "content": "Object stores offered by CSPs such as GCS are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use a GCS endpoint as your data source, please ensure you are using the following GCS configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to GCS with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/archives/v1.2.1/docs/getting-started/gcs/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/gcs/"
  },"313": {
    "doc": "Velox Local Caching(v1.2.1)",
    "title": "Velox Local Caching(v1.2.1)",
    "content": "Velox supports a local cache when reading data from HDFS/S3/ABFS. With this feature, Velox can asynchronously cache the data on local disk when reading from remote storage and future read requests on previously cached blocks will be serviced from local cache files. To enable the local caching feature, the following configurations are required: . spark.gluten.sql.columnar.backend.velox.cacheEnabled // enable or disable velox cache, default false. spark.gluten.sql.columnar.backend.velox.memCacheSize // the total size of in-mem cache, default is 128MB. spark.gluten.sql.columnar.backend.velox.ssdCachePath // the folder to store the cache files, default is \"/tmp\". spark.gluten.sql.columnar.backend.velox.ssdCacheSize // the total size of the SSD cache, default is 128MB. Velox will do in-mem cache only if this value is 0. spark.gluten.sql.columnar.backend.velox.ssdCacheShards // the shards of the SSD cache, default is 1. spark.gluten.sql.columnar.backend.velox.ssdCacheIOThreads // the IO threads for cache promoting, default is 1. Velox will try to do \"read-ahead\" if this value is bigger than 1 spark.gluten.sql.columnar.backend.velox.ssdODirect // enable or disable O_DIRECT on cache write, default false. It’s recommended to mount SSDs to the cache path to get the best performance of local caching. Cache files will be written to “spark.gluten.sql.columnar.backend.velox.cachePath”, with UUID based suffix, e.g. “/tmp/cache.13e8ab65-3af4-46ac-8d28-ff99b2a9ec9b0”. Gluten cannot reuse older caches for now, and the old cache files are left after Spark context shutdown. ",
    "url": "/archives/v1.2.1/docs/getting-started/localcache/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/localcache/"
  },"314": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Working with S3",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#working-with-s3",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#working-with-s3"
  },"315": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Configuring S3 endpoint",
    "content": "S3 provides the endpoint based method to access the files, here’s the example configuration. Users may need to modify some values based on real setup. spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider spark.hadoop.fs.s3a.access.key XXXXXXXXX spark.hadoop.fs.s3a.secret.key XXXXXXXXX spark.hadoop.fs.s3a.endpoint https://s3.us-west-1.amazonaws.com spark.hadoop.fs.s3a.connection.ssl.enabled true spark.hadoop.fs.s3a.path.style.access false . ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#configuring-s3-endpoint",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#configuring-s3-endpoint"
  },"316": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Configuring S3 instance credentials",
    "content": "S3 also provides other methods for accessing, you can also use instance credentials by setting the following config . spark.hadoop.fs.s3a.use.instance.credentials true . Note that in this case, “spark.hadoop.fs.s3a.endpoint” won’t take affect as Gluten will use the endpoint set during instance creation. ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#configuring-s3-instance-credentials",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#configuring-s3-instance-credentials"
  },"317": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Configuring S3 IAM roles",
    "content": "You can also use iam role credentials by setting the following configurations. Instance credentials have higher priority than iam credentials. spark.hadoop.fs.s3a.iam.role xxxx spark.hadoop.fs.s3a.iam.role.session.name xxxx . Note that spark.hadoop.fs.s3a.iam.role.session.name is optional. ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#configuring-s3-iam-roles",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#configuring-s3-iam-roles"
  },"318": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Other authentatication methods are not supported yet",
    "content": ". | AWS temporary credential | . ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#other-authentatication-methods-are-not-supported-yet",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#other-authentatication-methods-are-not-supported-yet"
  },"319": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Log granularity of AWS C++ SDK in velox",
    "content": "You can change log granularity of AWS C++ SDK by setting the spark.gluten.velox.awsSdkLogLevel configuration. The Allowed values are: . | OFF | FATAL | ERROR | WARN | INFO | DEBUG | TRACE | . ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#log-granularity-of-aws-c-sdk-in-velox",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#log-granularity-of-aws-c-sdk-in-velox"
  },"320": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Local Caching support",
    "content": "Velox supports a local cache when reading data from S3. Please refer Velox Local Cache part for more detailed configurations. ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/#local-caching-support",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/#local-caching-support"
  },"321": {
    "doc": "Using S3 with Gluten(v1.2.1)",
    "title": "Using S3 with Gluten(v1.2.1)",
    "content": "Object stores offered by CSPs such as AWS S3 are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use an S3 endpoint as your data source, please ensure you are using the following S3 configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to S3 with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/archives/v1.2.1/docs/getting-started/s3/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/s3/"
  },"322": {
    "doc": "Build Parameters for Velox Backend(v1.2.1)",
    "title": "Build Parameters",
    "content": "Native build parameters for buildbundle-veloxbe.sh or builddeps-veloxbe.sh . Please set them via --, e.g. --build_type=Release. | Parameters | Description | Default | . | build_type | Build type for Velox &amp; gluten cpp, CMAKE_BUILD_TYPE. | Release | . | build_tests | Build gluten cpp tests. | OFF | . | build_examples | Build udf example. | OFF | . | build_benchmarks | Build gluten cpp benchmarks. | OFF | . | build_jemalloc | Build with jemalloc. | OFF | . | build_protobuf | Build protobuf lib. | OFF | . | enable_qat | Enable QAT for shuffle data de/compression. | OFF | . | enable_iaa | Enable IAA for shuffle data de/compression. | OFF | . | enable_hbm | Enable HBM allocator. | OFF | . | enable_s3 | Build with S3 support. | OFF | . | enable_gcs | Build with GCS support. | OFF | . | enable_hdfs | Build with HDFS support. | OFF | . | enable_abfs | Build with ABFS support. | OFF | . | enable_ep_cache | Enable caching for external project build (Velox). | OFF | . | enable_vcpkg | Enable vcpkg for static build. | OFF | . | run_setup_script | Run setup script to install Velox dependencies. | ON | . | velox_repo | Specify your own Velox repo to build. | ”” | . | velox_branch | Specify your own Velox branch to build. | ”” | . | velox_home | Specify your own Velox source path to build. | ”” | . | build_velox_tests | Build Velox tests. | OFF | . | build_velox_benchmarks | Build Velox benchmarks (velox_tests and connectors will be disabled if ON) | OFF | . | build_arrow | Build arrow java/cpp and install the libs in local. Can turn it OFF after first build. | ON | . | spark_version | Build for specified version of Spark(3.2, 3.3, 3.4, 3.5, ALL). ALL means build for all versions. | ALL | . Velox build parameters for build_velox.sh . Please set them via --, e.g., --velox_home=/YOUR/PATH. | Parameters | Description | Default | . | velox_home | Specify Velox source path to build. | GLUTEN_SRC/ep/build-velox/build/velox_ep | . | build_type | Velox build type, i.e., CMAKE_BUILD_TYPE. | Release | . | enable_s3 | Build Velox with S3 support. | OFF | . | enable_gcs | Build Velox with GCS support. | OFF | . | enable_hdfs | Build Velox with HDFS support. | OFF | . | enable_abfs | Build Velox with ABFS support. | OFF | . | run_setup_script | Run setup script to install Velox dependencies before build. | ON | . | enable_ep_cache | Enable and reuse cache of Velox build. | OFF | . | build_test_utils | Build Velox with cmake arg -DVELOX_BUILD_TEST_UTILS=ON if ON. | OFF | . | build_tests | Build Velox test. | OFF | . | build_benchmarks | Build Velox benchmarks. | OFF | . Maven build parameters . The below parameters can be set via -P for mvn. | Parameters | Description | Default state | . | backends-velox | Build Gluten Velox backend. | disabled | . | backends-clickhouse | Build Gluten ClickHouse backend. | disabled | . | celeborn | Build Gluten with Celeborn. | disabled | . | uniffle | Build Gluten with Uniffle. | disabled | . | delta | Build Gluten with Delta Lake support. | disabled | . | iceberg | Build Gluten with Iceberg support. | disabled | . | spark-3.2 | Build Gluten for Spark 3.2. | enabled | . | spark-3.3 | Build Gluten for Spark 3.3. | disabled | . | spark-3.4 | Build Gluten for Spark 3.4. | disabled | . | spark-3.5 | Build Gluten for Spark 3.5. | disabled | . ",
    "url": "/archives/v1.2.1/docs/getting-started/build-guide/#build-parameters",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/build-guide/#build-parameters"
  },"323": {
    "doc": "Build Parameters for Velox Backend(v1.2.1)",
    "title": "Gluten Jar for Deployment",
    "content": "The gluten jar built out is under GLUTEN_SRC/package/target/. It’s name pattern is gluten-&lt;backend_type&gt;-bundle-spark&lt;spark.bundle.version&gt;_&lt;scala.binary.version&gt;-&lt;os.detected.release&gt;_&lt;os.detected.release.version&gt;-&lt;project.version&gt;.jar. | Spark Version | spark.bundle.version | scala.binary.version | . | 3.2.2 | 3.2 | 2.12 | . | 3.3.1 | 3.3 | 2.12 | . | 3.4.2 | 3.4 | 2.12 | . | 3.5.1 | 3.5 | 2.12 | . ",
    "url": "/archives/v1.2.1/docs/getting-started/build-guide/#gluten-jar-for-deployment",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/build-guide/#gluten-jar-for-deployment"
  },"324": {
    "doc": "Build Parameters for Velox Backend(v1.2.1)",
    "title": "Build Parameters for Velox Backend(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/build-guide/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/build-guide/"
  },"325": {
    "doc": "Getting-Started(v1.2.1)",
    "title": "Getting Started with Gluten for Apache Spark",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/#getting-started-with-gluten-for-apache-spark",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/#getting-started-with-gluten-for-apache-spark"
  },"326": {
    "doc": "Getting-Started(v1.2.1)",
    "title": "Getting-Started(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/getting-started/",
    
    "relUrl": "/archives/v1.2.1/docs/getting-started/"
  },"327": {
    "doc": "Documentations(v1.2.1)",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.2.1/docs/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.2.1/docs/#gluten-documents-by-version"
  },"328": {
    "doc": "Documentations(v1.2.1)",
    "title": "Documentations(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/",
    
    "relUrl": "/archives/v1.2.1/docs/"
  },"329": {
    "doc": "Velox Backend(v1.2.1)",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/#gluten-documents-by-version"
  },"330": {
    "doc": "Velox Backend(v1.2.1)",
    "title": "Velox Backend(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/"
  },"331": {
    "doc": "Limitations(v1.2.1)",
    "title": "Velox Backend Limitations",
    "content": "This document describes the limitations of velox backend by listing some known cases where exception will be thrown, gluten behaves incompatibly with spark, or certain plan’s execution must fall back to vanilla spark, etc. Override of Spark classes (For Spark3.2 and Spark3.3) . Gluten avoids to modify Spark’s existing code and use Spark APIs if possible. However, some APIs aren’t exposed in Vanilla spark and we have to copy the Spark file and do the hardcode changes. The list of override classes can be found as ignoreClasses in package/pom.xml . If you use customized Spark, you may check if the files are modified in your spark, otherwise your changes will be overrided. So you need to ensure preferentially load the Gluten jar to overwrite the jar of vanilla spark. Refer to How to prioritize loading Gluten jars in Spark. If not officially supported spark3.2/3.3 version is used, NoSuchMethodError can be thrown at runtime. More details see issue-4514. Fallbacks . Except the unsupported operators, functions, file formats, data sources listed in , there are some known cases also fall back to Vanilla Spark. ANSI . Gluten currently doesn’t support ANSI mode. If ANSI is enabled, Spark plan’s execution will always fall back to vanilla Spark. Runtime BloomFilter . Velox BloomFilter’s serialization format is different from Spark’s. BloomFilter binary generated by Velox can’t be deserialized by vanilla spark. So if might_contain falls back, we fall back bloom_filter_agg to vanilla spark also. Case Sensitive mode . Gluten only supports spark default case-insensitive mode. If case-sensitive mode is enabled, user may get incorrect result. Regexp functions . In Velox, regexp functions (rlike, regexp_extract, etc.) are implemented based on RE2, while in Spark they are based on java.util.regex. | Lookaround (lookahead/lookbehind) pattern is not supported in RE2. | When matching white space with pattern “\\s”, RE2 doesn’t treat “\\v” (or “\\x0b”) as white space, but java.util.regex does. | . There are a few unknown incompatible cases. If user cannot tolerate the incompatibility risk, please enable the below configuration property. spark.gluten.sql.fallbackRegexpExpressions . FileSource format . Currently, Gluten only fully supports parquet file format and partially support ORC. If other format is used, scan operator falls back to vanilla spark. Partitioned Table Scan . Gluten only support the partitioned table scan when the file path contain the partition info, otherwise will fall back to vanilla spark. Incompatible behavior . In certain cases, Gluten result may be different from Vanilla spark. JSON functions . Velox only supports double quotes surrounded strings, not single quotes, in JSON data. If single quotes are used, gluten will produce incorrect result. Velox doesn’t support [*] in path when get_json_object function is called and returns null instead. Parquet read conf . Gluten supports spark.files.ignoreCorruptFiles with default false, if true, the behavior is same as config false. Gluten ignores spark.sql.parquet.datetimeRebaseModeInRead, it only returns what write in parquet file. It does not consider the difference between legacy hybrid (Julian Gregorian) calendar and Proleptic Gregorian calendar. The result may be different with vanilla spark. Parquet write conf . Spark has spark.sql.parquet.datetimeRebaseModeInWrite config to decide whether legacy hybrid (Julian + Gregorian) calendar or Proleptic Gregorian calendar should be used during parquet writing for dates/timestamps. If the parquet to read is written by Spark with this config as true, Velox’s TableScan will output different result when reading it back. Partition write (For Spark3.2 and Spark3.3) . Gluten only supports static partition writes and does not support dynamic partition writes. spark.sql(\"CREATE TABLE t (c int, d long, e long) STORED AS PARQUET partitioned by (c, d)\") spark.sql(\"INSERT OVERWRITE TABLE t partition(c=1, d=2) SELECT 3 as e\") . Gluten does not support dynamic partition write and bucket write, Exception may be raised if you use. e.g., . spark.range(100).selectExpr(\"id as c1\", \"id % 7 as p\") .write .format(\"parquet\") .partitionBy(\"p\") .save(f.getCanonicalPath) . Partition write (For Spark3.4 and later) . Gluten supports static partition writes and dynamic partition writes. spark.sql(\"CREATE TABLE t (c int, d long, e long) STORED AS PARQUET partitioned by (c, d)\") spark.sql(\"INSERT OVERWRITE TABLE t partition(c=1, d) SELECT 2 as d, 3 as e\") . Gluten does not support bucket write, and will fall back to vanilla Spark. spark.range(100).selectExpr(\"id as c1\", \"id % 7 as p\") .write .format(\"parquet\") .bucketBy(2, \"c1\") .save(f.getCanonicalPath) . CTAS write (For Spark3.2 and Spark3.3) . Gluten does not create table as select. It may raise exception. e.g., . spark.range(100).toDF(\"id\") .write .format(\"parquet\") .saveAsTable(\"velox_ctas\") . CTAS write (For Spark3.4 and later) . Gluten supports create table as select with parquet file format. spark.range(100).toDF(\"id\") .write .format(\"parquet\") .saveAsTable(\"velox_ctas\") . HiveFileFormat write . Gluten supports writes of HiveFileFormat when the output file type is of type parquet only . NaN support . Velox does NOT support NaN. So unexpected result can be obtained for a few cases, e.g., comparing a number with NaN. Configuration . Parquet write only support three configs, other will not take effect. | compression code: . | sql conf: spark.sql.parquet.compression.codec | option: compression.codec | . | block size . | sql conf: spark.gluten.sql.columnar.parquet.write.blockSize | option: parquet.block.size | . | block rows . | sql conf: spark.gluten.sql.native.parquet.write.blockRows | option: parquet.block.rows | . | . Fetal error caused by Spark’s columnar reading . If the user enables Spark’s columnar reading, error can occur due to Spark’s columnar vector is not compatible with Gluten’s. Spill . OutOfMemoryExcetpion may still be triggered within current implementation of spill-to-disk feature, when shuffle partitions is set to a large number. When this case happens, please try to reduce the partition number to get rid of the OOM. Unsupported Data type support in ParquetScan . | Byte type causes fallback to vanilla spark | Timestamp type . Only reading with INT96 and dictionary encoding is supported. When reading INT64 represented millisecond/microsecond timestamps, or INT96 represented timestamps of other encodings, exceptions can occur. | Complex types . | Parquet scan of nested array with struct or array as element type is not supported in Velox (fallback behavior). | Parquet scan of nested map with struct as key type, or array type as value type is not supported in Velox (fallback behavior). | . | . CSV Read . The header option should be true. And now we only support DatasourceV1, i.e., user should set spark.sql.sources.useV1SourceList=csv. User defined read option is not supported, which will make CSV read fall back to vanilla Spark in most case. CSV read will also fall back to vanilla Spark and log warning when user specifies schema is different with file schema. ",
    "url": "/archives/v1.2.1/docs/velox-backend/limitations/#velox-backend-limitations",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/limitations/#velox-backend-limitations"
  },"332": {
    "doc": "Limitations(v1.2.1)",
    "title": "Limitations(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/limitations/",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/limitations/"
  },"333": {
    "doc": "Supported Operators & Functions(v1.2.1)",
    "title": "The Operators and Functions Support Progress",
    "content": "Gluten is still under active development. Here is a list of supported operators and functions. Since the same function may have different semantics between Presto and Spark, Velox implement the functions in Presto category, if we note a different semantics from Spark, then the function is implemented in Spark category. So Gluten will first try to find function in Velox’s spark category, if a function isn’t implemented then refer to Presto category. The total number of functions in Spark3.3 is 387, Gluten supports 189 of them. We use some notations to describe the supporting status of operators/functions in the tables below, they are: . | Value | Description | . | S | Supported. Gluten or Velox supports fully. | . | S* | Mark for foldable expression that will be converted to alias after spark’s optimization. | . | [Blank Cell] | Not applicable case or needs to confirm. | . | PS | Partial Support. Velox only partially supports it. | . | NS | Not Supported. Velox backend does not support it. | . And also some notations for the function implementation’s restrictions: . | Value | Description | . | Mismatched | Some functions are implemented by Velox, but have different semantics from Apache Spark, we mark them as “Mismatched”. | . | ANSI OFF | Gluten doesn’t support ANSI mode. If it is enabled, Gluten will fall back to Vanilla Spark. | . Operator Map . Gluten supports 28 operators (Drag to right to see all data types) . | Executor | Description | Gluten Name | Velox Name | BOOLEAN | BYTE | SHORT | INT | LONG | FLOAT | DOUBLE | STRING | NULL | BINARY | ARRAY | MAP | STRUCT(ROW) | DATE | TIMESTAMP | DECIMAL | CALENDAR | UDT | . | FileSourceScanExec | Reading data from files, often from Hive tables | FileSourceScanExecTransformer | TableScanNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | BatchScanExec | The backend for most file input | BatchScanExecTransformer | TableScanNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | FilterExec | The backend for most filter statements | FilterExecTransformer | FilterNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ProjectExec | The backend for most select, withColumn and dropColumn statements | ProjectExecTransformer | ProjectNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | HashAggregateExec | The backend for hash based aggregations | HashAggregateBaseTransformer | AggregationNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | BroadcastHashJoinExec | Implementation of join using broadcast data | BroadcastHashJoinExecTransformer | HashJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ShuffledHashJoinExec | Implementation of join using hashed shuffled data | ShuffleHashJoinExecTransformer | HashJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | SortExec | The backend for the sort operator | SortExecTransformer | OrderByNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | SortMergeJoinExec | Sort merge join, replacing with shuffled hash join | SortMergeJoinExecTransformer | MergeJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | WindowExec | Window operator backend | WindowExecTransformer | WindowNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | GlobalLimitExec | Limiting of results across partitions | LimitTransformer | LimitNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | LocalLimitExec | Per-partition limiting of results | LimitTransformer | LimitNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ExpandExec | The backend for the expand operator | ExpandExecTransformer | GroupIdNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | UnionExec | The backend for the union operator | UnionExecTransformer | N | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | DataWritingCommandExec | Writing data | Y | TableWriteNode | S | S | S | S | S | S | S | S | S | S | S | NS | S | S | NS | S | NS | NS | . | CartesianProductExec | Implementation of join using brute force | CartesianProductExecTransformer | NestedLoopJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ShuffleExchangeExec | The backend for most data being exchanged between processes | ColumnarShuffleExchangeExec | ExchangeNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The unnest operation expands arrays and maps into separate columns | N | UnnestNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The top-n operation reorders a dataset based on one or more identified sort fields as well as a sorting order | N | TopNNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The partitioned output operation redistributes data based on zero or more distribution fields | N | PartitionedOutputNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The values operation returns specified data | N | ValuesNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | A receiving operation that merges multiple ordered streams to maintain orderedness | N | MergeExchangeNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | An operation that merges multiple ordered streams to maintain orderedness | N | LocalMergeNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | Partitions input data into multiple streams or combines data from multiple streams into a single stream | N | LocalPartitionNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The enforce single row operation checks that input contains at most one row and returns that row unmodified | N | EnforceSingleRowNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The assign unique id operation adds one column at the end of the input columns with unique value per row | N | AssignUniqueIdNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | S | S | S | S | S | . | ReusedExchangeExec | A wrapper for reused exchange to have different output | ReusedExchangeExec | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | CollectLimitExec | Reduce to single partition and apply limit | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | BroadcastExchangeExec | The backend for broadcast exchange of data | Y | Y | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | S | NS | NS | . | ObjectHashAggregateExec | The backend for hash based aggregations supporting TypedImperativeAggregate functions | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | SortAggregateExec | The backend for sort based aggregations | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | CoalesceExec | Reduce the partition numbers | CoalesceExecTransformer | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | GenerateExec | The backend for operations that generate more output rows than input rows like explode | GenerateExecTransformer | UnnestNode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | RangeExec | The backend for range operator | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | SampleExec | The backend for the sample operator | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | SubqueryBroadcastExec | Plan to collect and transform the broadcast key values | Y | Y | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | S | NS | NS | . | TakeOrderedAndProjectExec | Take the first limit elements as defined by the sortOrder, and do projection if needed | Y | Y | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | S | NS | NS | . | CustomShuffleReaderExec | A wrapper of shuffle query stage | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | InMemoryTableScanExec | Implementation of InMemory Table Scan | Y | Y |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | BroadcastNestedLoopJoinExec | Implementation of join using brute force. Full outer joins and joins where the broadcast side matches the join side (e.g.: LeftOuter with left broadcast) are not supported | BroadcastNestedLoopJoinExecTransformer | NestedLoopJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | AggregateInPandasExec | The backend for an Aggregation Pandas UDF, this accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ArrowEvalPythonExec | The backend of the Scalar Pandas UDFs. Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | FlatMapGroupsInPandasExec | The backend for Flat Map Groups Pandas UDF, Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | MapInPandasExec | The backend for Map Pandas Iterator UDF. Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | WindowInPandasExec | The backend for Window Aggregation Pandas UDF, Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | HiveTableScanExec | The Hive table scan operator. Column and partition pruning are both handled | Y | Y |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | InsertIntoHiveTable | Command for writing data out to a Hive table | Y | Y |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | Velox2Row | Convert Velox format to Row format | Y | Y | S | S | S | S | S | S | S | S | NS | S | NS | NS | NS | S | S | NS | NS | NS | . | Velox2Arrow | Convert Velox format to Arrow format | Y | Y | S | S | S | S | S | S | S | S | NS | S | S | S | S | S | NS | S | NS | NS | . Function support . Gluten supports 199 functions. (Drag to right to see all data types) . | Spark Functions | Velox/Presto Functions | Velox/Spark functions | Gluten | Restrictions | BOOLEAN | BYTE | SHORT | INT | LONG | FLOAT | DOUBLE | DATE | TIMESTAMP | STRING | DECIMAL | NULL | BINARY | CALENDAR | ARRAY | MAP | STRUCT | UDT | . | ! |   | not | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | != | neq |   | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | % | mod | remainder | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | &amp; | bitwise_and | bitwise_and | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | * | multiply | multiply | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | + | plus | add | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | - | minus | subtract | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | / | divide | divide | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | &lt; | lt | lessthan | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &lt;= | lte | lessthanorequa | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &lt;=&gt; |   | equalnullsafe | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | &lt;&gt; | neq | notequalto | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | = |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | == | eq | equalto | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &gt; | gt | greaterthan | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &gt;= | gte | greaterthanorequal | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | ^ | bitwise_xor |   | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | | bitwise_or | bitwise_or | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . || |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ~ | bitwise_not |   | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | and |   |   | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | between | between | between | S |   | S | S | S | S | S | S | S | S |   | S |   |   |   |   |   |   |   |   | . | bit_and | bitwise_and_agg |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_count | bit_count | bit_count | S |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_get |   | bit_get | S |   |   | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_or |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_xor |   | bit_xor | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | case |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | div |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | getbit |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | if |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ifnull |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | in |   | in | S |   |   |   | S | S | S | S | S | S | S | S |   |   |   |   |   |   |   |   | . | isnan | is_nan | isnan | S |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   |   |   | . | isnotnull |   | isnotnull | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | isnull | is_null | isnull | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | mod | mod | remainder | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | negative | negate | unaryminus |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | not |   | not | S |   | S | S | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   | . | nullif |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | or |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | positive |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | when |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ascii |   | ascii | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | base64 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bin |   | bin |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_length |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | btrim |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | char, chr | chr | chr | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | char_length/character_length | length | length | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | character_lengt/char_length | length | length | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | chr, char | chr | chr | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | concat | concat | concat | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | concat_ws |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | contains |   | contains |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | decode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | elt |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | encode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | endswith |   | endsWith |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | find_in_set |   |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | format_number |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | format_string |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | initcap |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | instr |   | instr | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | lcase, lower | lower | lower | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | left |   |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | length | length | length | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | levenshtein |   | levenshtein | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | locate | strpos |   | S | Mismatched |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | lower | lower | lower | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | lpad | lpad |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | ltrim | ltrim | ltrim | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | octet_length |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | overlay |   | overlay | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | parse_url |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | position | strpos |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | printf |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | repeat |   | repeat | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | replace | replace | replace | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | reverse | reverse |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | right |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rpad | rpad |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | rtrim | rtrim | rtrim | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | sentences |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | soundex |   | soundex | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | space |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | split | split | split | S | Mismatched |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | split_part | split_part |   |   | Mismatched |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | startswith |   | startsWith |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | substr, substring | substr | substring | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | substring, substr | substr | substring | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | substring_index |   | substring_index | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | translate |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | trim | trim | trim | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | ucase, upper | upper | upper | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | unbase64 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unhex |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | upper, ucase | upper | upper | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | xpath |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_boolean |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_double |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_float |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_int |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_long |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_number |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_short |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_string |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | like | like |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp |   | rlike | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_extract | regexp_extract | regexp_extract | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_extract_all | regexp_extract_all |   | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_like | regexp_like | rlike | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_replace | regexp_replace |   | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | rlike |   | rlike | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | abs | abs | abs | S | ANSI OFF |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | acos | acos |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | acosh |   | acosh | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | asin | asin |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | asinh |   | asinh | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | atan | atan |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | atan2 | atan2 |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | atanh |   | atanh | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | bround |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | cbrt | cbrt |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | ceil | ceil | ceil | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | ceiling | ceiling |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | conv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | cos | cos |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | cosh | cosh |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | cot |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | degrees | degrees |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | e | e |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | exp | exp | exp | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | expm1 |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | factorial |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | floor | floor | floor | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | hex |   |   | S |   |   |   |   |   | S |   |   |   |   | S |   |   | S |   |   |   |   |   | . | hypot |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ln | ln |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | log | ln | log | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | log10 | log10 |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | log1p |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | log2 | log2 |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | pi | pi |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | pmod |   | pmod | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | pow, power | pow,power | power |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | power, pow | power,pow | power | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | radians | radians |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | rand | rand | rand | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rand | rand | rand |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | random | random |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rint |   | rint | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | round | round | round | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | shiftleft | bitwise_left_shift | shiftleft | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | shiftright | bitwise_right_shift | shiftright | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | shiftrightunsigned |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sign, signum | sign |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | signum, sign | sign |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | sin | sin |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | sinh |   | sinh |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sqrt | sqrt |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | tan | tan |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | tanh | tanh |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | width_bucket | width_bucket | width_bucket | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array |   | array | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | aggregate | aggregate | reduce | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_contains |   | array_contains | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_distinct | array_distinct |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_except | array_except |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_intersect | array_intersect | array_intersect | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_join | array_join |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_max | array_max |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_min | array_min |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_position | array_position |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_remove | array_remove |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_repeat |   |   | S |   | S | S | S | S | S | S | S | S | S | S | S |   |   |   |   |   |   |   | . | array_sort | array_sort | array_sort | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_union |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | arrays_overlap | array_overlap | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | arrays_zip | zip |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | cardinality | cardinality |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | element_at | element_at | element_at | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S | S |   |   | . | exists | any_match |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | explode, explode_outer |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | explode_outer, explode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | filter | filter | filter | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | forall | all_match |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | flatten | flatten | flatten | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map | map | map | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_concat | map_concat |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_entries | map_entries |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_filter | map_filter | map_filter |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | get_map_value |   | element_at | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | map_from_arrays |   | map_from_arrays | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | map_from_entries | map_from_entries |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_keys | map_keys | map_keys | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_values | map_values | map_values | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | map_zip_with | map_zip_with |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | named_struct,struct | row_construct | named_struct | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   | . | posexplode_outer,posexplode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sequence |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | shuffle | shuffle | shuffle | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | size |   | size | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_size |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | slice | slice |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sort_array |   | sort_array | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | str_to_map |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | transform | transform | transofrm |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | transform_keys | transform_keys |   | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | transform_values | transform_values |   | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | zip_with | zip_with |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | add_months |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_date |   |   | S* |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_timestamp |   |   | S* |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_timezone |   |   | S* |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date | date |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_add | date_add | date_add | S |   |   | S | S | S |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | date_format | date_format |   | S |   |   |   |   | S |   |   |   |   | S |   |   |   |   |   |   |   |   |   | . | date_from_unix_date |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_part |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_sub |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_trunc | date_trunc |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | datediff | date_diff |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | day | day |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | dayofmonth | day_of_month |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | dayofweek | day_of_week,dow |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | dayofyear | day_of_year,doy |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | extract |   |   |   |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | from_unixtime | from_unixtime |   | S |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | from_utc_timestamp |   | from_utc_timestamp | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | hour | hour |   | S |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   | . | last_day |   | last_day | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_date |   | make_date | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_dt_interval |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_interval |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_ym_interval |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | minute | minute |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | month | month |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | months_between |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | next_day |   |   | S |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   |   | . | now |   |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | quarter | quarter |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | second | second |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | session_window |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp_micros |   | timestamp_micros | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp_millis |   | timestamp_millis | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp_seconds |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_date |   |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | to_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_unix_timestamp | to_unixtime | to_unix_timestamp | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_utc_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | trunc |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_timestamp |   | unix_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_seconds |   | unix_seconds | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_millis |   | unix_millis | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_micros |   | unix_micros | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | weekday |   |   | S |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   |   | . | weekofyear | week,week_of_year |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | window |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | year | year | year | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | aggregate |   | aggregate | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | any |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | approx_count_distinct | approx_distinct |   | S |   | S | S | S | S | S | S | S | S |   | S |   |   |   |   |   |   |   |   | . | approx_percentile |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | avg | avg |   | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | bool_and |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bool_or |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | collect_list |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | collect_set |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | corr | corr |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | count | count |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | count_if | count_if |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | count_min_sketch |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | covar_pop | covar_pop |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | covar_samp | covar_samp |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | every |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | first |   | first | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | first_value |   | first_value | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | grouping |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | grouping_id |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | kurtosis | kurtosis | kurtosis | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | last |   | last | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | last_value |   | last_value | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | max | max |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | max_by |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | mean | avg |   | S | ANSI OFF |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | min | min |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | min_by |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | regr_avgx | regr_avgx | regr_avgx | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_avgy | regr_avgy | regr_avgy | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_count | regr_count | regr_count | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_r2 | regr_r2 | regr_r2 | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_intercept | regr_intercept | regr_intercept | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_slope | regr_slope | regr_slope | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_sxy | regr_sxy | regr_sxy | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_sxx | regr_sxx | regr_sxx | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_syy | regr_syy | regr_syy | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | skewness | skewness | skewness | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | some |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | std,stddev | stddev |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | stddev,std | stddev |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | stddev_pop | stddev_pop |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | stddev_samp | stddev_samp |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | sum | sum |   | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | var_pop | var_pop |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | var_samp | var_samp |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | variance | variance |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | cume_dist | cume_dist |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | dense_rank | dense_rank |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | lag |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | lead |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nth_value | nth_value | nth_value | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ntile | ntile | ntile | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | percent_rank | percent_rank |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rank | rank |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | row_number | row_number |   | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | from_csv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | from_json |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | get_json_object | json_extract_scalar | get_json_object | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   | . | json_array_length | json_array_length |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   | . | json_tuple |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | schema_of_csv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | schema_of_json |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_csv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_json |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | assert_true |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | coalesce |   |   | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | crc32 | crc32 |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | current_user |   |   | S* |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | current_catalog |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_database |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | greatest | greatest | greatest | S |   |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   | . | hash | hash | hash | S |   | S | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | inline |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | inline_outer |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | input_file_name |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | input_file_block_length |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | input_file_block_start |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | java_method |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | least | least | least | S |   |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   | . | md5 | md5 |   | S |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | monotonically_increasing_id |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nanvl |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nvl |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nvl2 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | raise_error |   | raise_error | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | reflect |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sha |   |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | sha1 | sha1 | sha1 | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | sha2 |   | sha2 | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | spark_partition_id |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | stack |   |   | S |   | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | . | xxhash64 | xxhash64 | xxhash64 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | uuid | uuid | uuid | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rand | rand | rand | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_add |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_substract |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_multiply |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_divide |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . ",
    "url": "/archives/v1.2.1/docs/velox-backend/support/#the-operators-and-functions-support-progress",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/support/#the-operators-and-functions-support-progress"
  },"334": {
    "doc": "Supported Operators & Functions(v1.2.1)",
    "title": "Supported Operators & Functions(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/support/",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/support/"
  },"335": {
    "doc": "Troubleshooting(v1.2.1)",
    "title": "Velox Backend Troubleshooting",
    "content": "Fatal error after native exception is thrown . We depend on checking exceptions thrown from native code to validate whether a spark plan can be really offloaded to native engine. But if libunwind-dev is installed, native exception will not be caught and will interrupt the program. So far, we observed this fatal error can happen only on Ubuntu 20.04. Please remove libunwind-dev and then re-build the project to address this issue. sudo apt-get purge --auto-remove libunwind-dev . Jar conflict issue . With the latest version of Gluten, there should not be any jar conflict issue anymore. If you still get hit with such issue, please follow the below instructions. The potentially conflicting libraries include protobuf (Both Velox and CK backend), flatbuffers (Velox backend), and arrow-* (Velox backend). These libraries are compiled from source and packed into Gluten’s jar. Jvm should search them from Gluten.jar firstly and load them. But for some reason jvm loads the jars from spark_home/jars which causes conflict. You may use below commands to remove the jars from spark_home/jars. We are still investigating the root cause. Welcome to share if you have good solution. rm -rf $SPARK_HOME/jars/protobuf-* # velox backend only rm -rf $SPARK_HOME/jars/flatbuffers-* rm -rf $SPARK_HOME/jars/arrow-* . Incompatible class error when using native writer . Gluten native writer overwrite some vanilla spark classes. Therefore, when running a program that uses gluten, it is essential to ensure that the gluten jar is loaded prior to the vanilla spark jar. In this section, we will provide some configuration settings in $SPARK_HOME/conf/spark-defaults.conf for Yarn client, Yarn cluster, and Local&amp;Standalone mode to guarantee that the gluten jar is prioritized. Configurations for Yarn Client mode . // spark will upload the gluten jar to hdfs and then the nodemanager will fetch the gluten jar before start the executor process. Here also can set the spark.jars. spark.files = {absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The absolute path on running node spark.driver.extraClassPath={absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The relative path under the executor working directory spark.executor.extraClassPath=./gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar . Configurations for Yarn Cluster mode . spark.driver.userClassPathFirst = true spark.executor.userClassPathFirst = true spark.files = {absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The relative path under the executor working directory spark.driver.extraClassPath=./gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The relative path under the executor working directory spark.executor.extraClassPath=./gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar . Configurations for Local &amp; Standalone mode . // The absolute path on running node spark.driver.extraClassPath={absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The absolute path on running node spark.executor.extraClassPath={absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar . Invalid pointer error . If the below error is reported at runtime, please re-build gluten with --compile_arrow_java=ON, then redeploy Gluten jar. *** Error in `/usr/local/jdk1.8.0_381/bin/java': free(): invalid pointer: 0x00007f36cb5cec80 *** ======= Backtrace: ========= /lib64/libc.so.6(+0x7d1fd)[0x7f38c29da1fd] /lib64/libstdc++.so.6(_ZNSt6locale5_Impl16_M_install_facetEPKNS_2idEPKNS_5facetE+0x142)[0x7f36cb3370d2] /lib64/libstdc++.so.6(_ZNSt6locale5_ImplC1Em+0x1e3)[0x7f36cb337523] /lib64/libstdc++.so.6(+0x71495)[0x7f36cb338495] /lib64/libpthread.so.0(pthread_once+0x50)[0x7f38c3147be0] /lib64/libstdc++.so.6(+0x714e1)[0x7f36cb3384e1] /lib64/libstdc++.so.6(_ZNSt6localeC2Ev+0x13)[0x7f36cb338523] /lib64/libstdc++.so.6(_ZNSt8ios_base4InitC2Ev+0xbc)[0x7f36cb33537c] /tmp/jnilib-645156599284574767.tmp(+0x2a90)[0x7f375d235a90] /lib64/ld-linux-x86-64.so.2(+0xf4e3)[0x7f38c33664e3] /lib64/ld-linux-x86-64.so.2(+0x13b04)[0x7f38c336ab04] /lib64/ld-linux-x86-64.so.2(+0xf2f4)[0x7f38c33662f4] /lib64/ld-linux-x86-64.so.2(+0x1321b)[0x7f38c336a21b] /lib64/libdl.so.2(+0x102b)[0x7f38c2d1f02b] /lib64/ld-linux-x86-64.so.2(+0xf2f4)[0x7f38c33662f4] /lib64/libdl.so.2(+0x162d)[0x7f38c2d1f62d] /lib64/libdl.so.2(dlopen+0x31)[0x7f38c2d1f0c1] /usr/local/jdk1.8.0_381/jre/lib/amd64/server/libjvm.so(+0x9292b1)[0x7f38c22732b1] /usr/local/jdk1.8.0_381/jre/lib/amd64/server/libjvm.so(JVM_LoadLibrary+0xa1)[0x7f38c205e0c1] /usr/local/jdk1.8.0_381/jre/lib/amd64/libjava.so(Java_java_lang_ClassLoader_00024NativeLibrary_load+0x1ac) ... ",
    "url": "/archives/v1.2.1/docs/velox-backend/troubleshooting/#velox-backend-troubleshooting",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/troubleshooting/#velox-backend-troubleshooting"
  },"336": {
    "doc": "Troubleshooting(v1.2.1)",
    "title": "Troubleshooting(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/troubleshooting/",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/troubleshooting/"
  },"337": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Velox User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf"
  },"338": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Introduction",
    "content": "Velox backend supports User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF). Users can create their own functions using the UDF interface provided in Velox backend and build libraries for these functions. At runtime, the UDF are registered at the start of applications. Once registered, Gluten will be able to parse and offload these UDF into Velox during execution. ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#introduction",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#introduction"
  },"339": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Create and Build UDF/UDAF library",
    "content": "The following steps demonstrate how to set up a UDF library project: . | Include the UDF Interface Header: First, include the UDF interface header file Udf.h in the project file. The header file defines the UdfEntry struct, along with the macros for declaring the necessary functions to integrate the UDF into Gluten and Velox. | Implement the UDF: Implement UDF. These functions should be able to register to Velox. | Implement the Interface Functions: Implement the following interface functions that integrate UDF into Project Gluten: . | getNumUdf(): This function should return the number of UDF in the library. This is used to allocating udfEntries array as the argument for the next function getUdfEntries. | getUdfEntries(gluten::UdfEntry* udfEntries): This function should populate the provided udfEntries array with the details of the UDF, including function names and signatures. | registerUdf(): This function is called to register the UDF to Velox function registry. This is where users should register functions by calling facebook::velox::exec::registerVecotorFunction or other Velox APIs. | The interface functions are mapped to marcos in Udf.h. Here’s an example of how to implement these functions: . | . // Filename MyUDF.cc #include &lt;velox/expression/VectorFunction.h&gt; #include &lt;velox/udf/Udf.h&gt; namespace { static const char* kInteger = \"integer\"; } const int kNumMyUdf = 1; const char* myUdfArgs[] = {kInteger}: gluten::UdfEntry myUdfSig = {\"myudf\", kInteger, 1, myUdfArgs}; class MyUdf : public facebook::velox::exec::VectorFunction { ... // Omit concrete implementation } static std::vector&lt;std::shared_ptr&lt;exec::FunctionSignature&gt;&gt; myUdfSignatures() { return {facebook::velox::exec::FunctionSignatureBuilder() .returnType(myUdfSig.dataType) .argumentType(myUdfSig.argTypes[0]) .build()}; } DEFINE_GET_NUM_UDF { return kNumMyUdf; } DEFINE_GET_UDF_ENTRIES { udfEntries[0] = myUdfSig; } DEFINE_REGISTER_UDF { facebook::velox::exec::registerVectorFunction( myUdf[0].name, myUdfSignatures(), std::make_unique&lt;MyUdf&gt;()); } . | . To build the UDF library, users need to compile the C++ code and link to libvelox.so. It’s recommended to create a CMakeLists.txt for the project. Here’s an example: . project(myudf) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(GLUTEN_HOME /path/to/gluten) add_library(myudf SHARED \"MyUDF.cpp\") find_library(VELOX_LIBRARY REQUIRED NAMES velox HINTS ${GLUTEN_HOME}/cpp/build/releases NO_DEFAULT_PATH) target_include_directories(myudf PRIVATE ${GLUTEN_HOME}/cpp ${GLUTEN_HOME}/ep/build-velox/build/velox_ep) target_link_libraries(myudf PRIVATE ${VELOX_LIBRARY}) . The steps for creating and building a UDAF library are quite similar to those for a UDF library. The major difference lies in including and defining specific functions within the UDAF header file Udaf.h . | getNumUdaf() | getUdafEntries(gluten::UdafEntry* udafEntries) | registerUdaf() | . gluten::UdafEntry requires an additional field intermediateType, to specify the output type from partial aggregation. For detailed implementation, you can refer to the example code in MyUDAF.cc . ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#create-and-build-udfudaf-library",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#create-and-build-udfudaf-library"
  },"340": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Using UDF/UDAF in Gluten",
    "content": "Gluten loads the UDF libraries at runtime. You can upload UDF libraries via --files or --archives, and configure the library paths using the provided Spark configuration, which accepts comma separated list of library paths. Note if running on Yarn client mode, the uploaded files are not reachable on driver side. Users should copy those files to somewhere reachable for driver and set spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths. This configuration is also useful when the udfLibraryPaths is different between driver side and executor side. | Use the --files option to upload a library and configure its relative path | . --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so . | Use the --archives option to upload an archive and configure its relative path | . --archives /path/to/udf_archives.zip#udf_archives --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=udf_archives # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/udf_archives.zip . | Configure URI | . You can also specify the local or HDFS URIs to the UDF libraries or archives. Local URIs should exist on driver and every worker nodes. --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=file:///path/to/library_or_archive . ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#using-udfudaf-in-gluten",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#using-udfudaf-in-gluten"
  },"341": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Try the example",
    "content": "We provided Velox UDF examples in file MyUDF.cc and UDAF examples in file MyUDAF.cc. You need to build the gluten project with --build_example=ON to get the example libraries./dev/buildbundle-veloxbe.sh --build_examples=ON . Then, you can find the example libraries at /path/to/gluten/cpp/build/velox/udf/examples/ . Start spark-shell or spark-sql with below configuration . # Use the `--files` option to upload a library and configure its relative path --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so . or . # Only configure URI --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=file:///path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so . Run query. The functions myudf1 and myudf2 increment the input value by a constant of 5 . select myudf1(100L), myudf2(1) . The output from spark-shell will be like . +------------------+----------------+ |udfexpression(100)|udfexpression(1)| +------------------+----------------+ | 105| 6| +------------------+----------------+ . ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#try-the-example",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#try-the-example"
  },"342": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Configurations",
    "content": "| Parameters | Description | . | spark.gluten.sql.columnar.backend.velox.udfLibraryPaths | Path to the udf/udaf libraries. | . | spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths | Path to the udf/udaf libraries on driver node. Only applicable on yarn-client mode. | . | spark.gluten.sql.columnar.backend.velox.udfAllowTypeConversion | Whether to inject possible cast to convert mismatched data types from input to one registered signatures. | . ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#configurations",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#configurations"
  },"343": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Pandas UDFs (a.k.a. Vectorized UDFs)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#pandas-udfs-aka-vectorized-udfs",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#pandas-udfs-aka-vectorized-udfs"
  },"344": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Introduction",
    "content": "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf() as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general. For more details, you can refer doc. ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#introduction-1",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#introduction-1"
  },"345": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Using Pandas UDFs in Gluten with Velox Backend",
    "content": "Similar as in vanilla Spark, user needs to set up pyspark/arrow dependencies properly first. You may can refer following steps: . pip3 install pyspark==$SPARK_VERSION cython pip3 install pandas pyarrow . Gluten provides a config to control enable ColumnarArrowEvalPython or not, with true as defalt. spark.gluten.sql.columnar.arrowUdf . Then take following PySpark code for example: . from pyspark.sql.functions import pandas_udf, PandasUDFType import pyspark.sql.functions as F import os @pandas_udf('long') def pandas_plus_one(v): return (v + 1) df = spark.read.orc(\"path_to_file\").select(\"quantity\").withColumn(\"processed_quantity\", pandas_plus_one(\"quantity\")).select(\"quantity\") . The expected physical plan will be: . == Physical Plan == VeloxColumnarToRowExec +- ^(2) ProjectExecTransformer [pythonUDF0#45L AS processed_quantity#41L] +- ^(2) InputIteratorTransformer[quantity#2L, pythonUDF0#45L] +- ^(2) InputAdapter +- ^(2) ColumnarArrowEvalPython [pandas_plus_one(quantity#2L)#40L], [pythonUDF0#45L], 200 +- ^(1) NativeFileScan orc [quantity#2L] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/***], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;quantity:bigint&gt; . ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/#using-pandas-udfs-in-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/#using-pandas-udfs-in-gluten-with-velox-backend"
  },"346": {
    "doc": "Velox UDF(v1.2.1)",
    "title": "Velox UDF(v1.2.1)",
    "content": " ",
    "url": "/archives/v1.2.1/docs/velox-backend/udf/",
    
    "relUrl": "/archives/v1.2.1/docs/velox-backend/udf/"
  },"347": {
    "doc": "v1.2.1",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.2.1/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.2.1/#gluten-documents-by-version"
  },"348": {
    "doc": "v1.2.1",
    "title": "v1.2.1",
    "content": " ",
    "url": "/archives/v1.2.1/",
    
    "relUrl": "/archives/v1.2.1/"
  },"349": {
    "doc": "Configuration",
    "title": "Spark Configurations for Gluten Plugin",
    "content": "There are many configurations could impact the Gluten Plugin performance and can be fine-tuned in Spark. You can add these configurations into spark-defaults.conf to enable or disable the setting. ",
    "url": "/archives/v1.3.0/configuration#spark-configurations-for-gluten-plugin",
    
    "relUrl": "/archives/v1.3.0/configuration#spark-configurations-for-gluten-plugin"
  },"350": {
    "doc": "Configuration",
    "title": "Spark parameters",
    "content": "| Parameters | Description | Recommend Setting | . | spark.driver.extraClassPath | To add Gluten Plugin jar file in Spark Driver | /path/to/jar_file | . | spark.executor.extraClassPath | To add Gluten Plugin jar file in Spark Executor | /path/to/jar_file | . | spark.executor.memory | To set up how much memory to be used for Spark Executor. |   | . | spark.memory.offHeap.size | To set up how much memory to be used for Java OffHeap. Please notice Gluten Plugin will leverage this setting to allocate memory space for native usage even offHeap is disabled. The value is based on your system and it is recommended to set it larger if you are facing Out of Memory issue in Gluten Plugin | 30G | . | spark.sql.sources.useV1SourceList | Choose to use V1 source | avro | . | spark.sql.join.preferSortMergeJoin | To turn off preferSortMergeJoin in Spark | false | . | spark.plugins | To load Gluten’s components by Spark’s plug-in loader | org.apache.gluten.GlutenPlugin | . | spark.shuffle.manager | To turn on Gluten Columnar Shuffle Plugin | org.apache.spark.shuffle.sort.ColumnarShuffleManager | . | spark.gluten.enabled | Enable Gluten, default is true. Just an experimental property. Recommend to enable/disable Gluten through the setting for spark.plugins. | true | . | spark.gluten.memory.isolation | (Experimental) Enable isolated memory mode. If true, Gluten controls the maximum off-heap memory can be used by each task to X, X = executor memory / max task slots. It’s recommended to set true if Gluten serves concurrent queries within a single session, since not all memory Gluten allocated is guaranteed to be spillable. In the case, the feature should be enabled to avoid OOM. Note when true, setting spark.memory.storageFraction to a lower value is suggested since storage memory is considered non-usable by Gluten. | false | . | spark.gluten.ras.enabled | Experimental: Enables RAS (relation algebra selector) during physical planning to generate more efficient query plan. Note, this feature is still in development and may not bring performance profits. | false | . | spark.gluten.sql.columnar.maxBatchSize | Number of rows to be processed in each batch. Default value is 4096. | 4096 | . | spark.gluten.sql.columnar.scanOnly | When enabled, this config will overwrite all other operators’ enabling, and only Scan and Filter pushdown will be offloaded to native. | false | . | spark.gluten.sql.columnar.batchscan | Enable or Disable Columnar BatchScan, default is true | true | . | spark.gluten.sql.columnar.hashagg | Enable or Disable Columnar Hash Aggregate, default is true | true | . | spark.gluten.sql.columnar.project | Enable or Disable Columnar Project, default is true | true | . | spark.gluten.sql.columnar.filter | Enable or Disable Columnar Filter, default is true | true | . | spark.gluten.sql.columnar.sort | Enable or Disable Columnar Sort, default is true | true | . | spark.gluten.sql.columnar.window | Enable or Disable Columnar Window, default is true | true | . | spark.gluten.sql.columnar.shuffledHashJoin | Enable or Disable ShuffledHashJoin, default is true | true | . | spark.gluten.sql.columnar.forceShuffledHashJoin | Force to use ShuffledHashJoin over SortMergeJoin, default is true. For queries that can benefit from storaged patitioned join, please set it to false. | true | . | spark.gluten.sql.columnar.sortMergeJoin | Enable or Disable Columnar Sort Merge Join, default is true | true | . | spark.gluten.sql.columnar.union | Enable or Disable Columnar Union, default is true | true | . | spark.gluten.sql.columnar.expand | Enable or Disable Columnar Expand, default is true | true | . | spark.gluten.sql.columnar.generate | Enable or Disable Columnar Generate, default is true | true | . | spark.gluten.sql.columnar.limit | Enable or Disable Columnar Limit, default is true | true | . | spark.gluten.sql.columnar.tableCache | Enable or Disable Columnar Table Cache, default is false | true | . | spark.gluten.sql.columnar.broadcastExchange | Enable or Disable Columnar Broadcast Exchange, default is true | true | . | spark.gluten.sql.columnar.broadcastJoin | Enable or Disable Columnar BroadcastHashJoin, default is true | true | . | spark.gluten.sql.columnar.shuffle.sort.threshold | The threshold to determine whether to use sort-based columnar shuffle. Sort-based shuffle will be used if the number of partitions is greater than this threshold. | 100000 | . | spark.gluten.sql.columnar.shuffle.codec | Set up the codec to be used for Columnar Shuffle. If this configuration is not set, will check the value of spark.io.compression.codec. By default, Gluten use software compression. Valid options for software compression are lz4, zstd. Valid options for QAT and IAA is gzip. | lz4 | . | spark.gluten.sql.columnar.shuffle.codecBackend | Enable using hardware accelerators for shuffle de/compression. Valid options are QAT and IAA. |   | . | spark.gluten.sql.columnar.shuffle.compressionMode | Setting different compression mode in shuffle, Valid options are buffer and rowvector, buffer option compress each buffer of RowVector individually into one pre-allocated large buffer, rowvector option first copies each buffer of RowVector to a large buffer and then compress the entire buffer in one go. | buffer | . | spark.gluten.sql.columnar.shuffle.compression.threshold | If number of rows in a batch falls below this threshold, will copy all buffers into one buffer to compress. | 100 | . | spark.gluten.sql.columnar.shuffle.realloc.threshold | Set the threshold to dynamically adjust the size of shuffle split buffers. The size of each split buffer is recalculated for each incoming batch of data. If the new size deviates from the current partition buffer size by a factor outside the range of [1 - threshold, 1 + threshold], the split buffer will be re-allocated using the newly calculated size | 0.25 | . | spark.gluten.sql.columnar.shuffle.merge.threshold | Set the threshold control the minimum merged size. When a partition buffer is full, and the number of rows is below (threshold * spark.gluten.sql.columnar.maxBatchSize), it will be saved for merging. | 0.25 | . | spark.gluten.sql.columnar.numaBinding | Set up NUMABinding, default is false | true | . | spark.gluten.sql.columnar.coreRange | Set up the core range for NUMABinding, only works when numaBinding set to true. The setting is based on the number of cores in your system. Use 72 cores as an example. | 0-17,36-53 |18-35,54-71 | . | spark.gluten.sql.columnar.wholeStage.fallback.threshold | Configure the threshold for whether whole stage will fall back in AQE supported case by counting the number of ColumnarToRow &amp; vanilla leaf node | &gt;= 1 | . | spark.gluten.sql.columnar.query.fallback.threshold | Configure the threshold for whether query will fall back by counting the number of ColumnarToRow &amp; vanilla leaf node | &gt;= 1 | . | spark.gluten.sql.columnar.fallback.ignoreRowToColumnar | When true, the fallback policy ignores the RowToColumnar when counting fallback number. | true | . | spark.gluten.sql.columnar.fallback.preferColumnar | When true, the fallback policy prefers to use Gluten plan rather than vanilla Spark plan if the both of them contains ColumnarToRow and the vanilla Spark plan ColumnarToRow number is not smaller than Gluten plan. | true | . | spark.gluten.sql.columnar.force.hashagg | Force to use hash agg to replace sort agg. | true | . | spark.gluten.sql.columnar.vanillaReaders | Enable vanilla spark’s vectorized reader. Please note it may bring perf. overhead due to extra data transition. We recommend to disable it if most queries can be fully offloaded to gluten. | false | . | spark.gluten.sql.native.bloomFilter | Enable or Disable native runtime bloom filter. | true | . | spark.gluten.sql.native.arrow.reader.enabled | Enable or Disable native arrow read CSV file format | false | . | spark.gluten.shuffleWriter.bufferSize | Set the number of buffer rows for the shuffle writer | value of spark.gluten.sql.columnar.maxBatchSize | . | spark.gluten.loadLibFromJar | Controls whether to load dynamic link library from a packed jar for gluten/cpp. Not applicable to static build and clickhouse backend. | false | . | spark.gluten.loadLibOS | When spark.gluten.loadLibFromJar is true. Manually specify the system os to load library, e.g., CentOS |   | . | spark.gluten.loadLibOSVersion | Manually specify the system os version to load library, e.g., if spark.gluten.loadLibOS is CentOS, this config can be 7 |   | . | spark.gluten.expression.blacklist | A black list of expression to skip transform, multiple values separated by commas. |   | . | spark.gluten.sql.columnar.fallback.expressions.threshold | Fall back filter/project if the height of expression tree reaches this threshold, considering Spark codegen can bring better performance for such case. | 50 | . | spark.gluten.sql.cartesianProductTransformerEnabled | Config to enable CartesianProductExecTransformer. | true | . | spark.gluten.sql.broadcastNestedLoopJoinTransformerEnabled | Config to enable BroadcastNestedLoopJoinExecTransformer. | true | . | spark.gluten.sql.cacheWholeStageTransformerContext | When true, WholeStageTransformer will cache the WholeStageTransformerContext when executing. It is used to get substrait plan node and native plan string. | false | . | spark.gluten.sql.injectNativePlanStringToExplain | When true, Gluten will inject native plan tree to explain string inside WholeStageTransformerContext. | false | . | spark.gluten.sql.fallbackRegexpExpressions | When true, Gluten will fall back all regexp expressions to avoid any incompatibility risk. | false | . ",
    "url": "/archives/v1.3.0/configuration#spark-parameters",
    
    "relUrl": "/archives/v1.3.0/configuration#spark-parameters"
  },"351": {
    "doc": "Configuration",
    "title": "Velox Parameters",
    "content": "The following configurations are related to Velox settings. | Parameters | Description | Recommend Setting | . | spark.gluten.sql.columnar.backend.velox.bloomFilter.expectedNumItems | The default number of expected items for the velox bloomfilter. | 1000000L | . | spark.gluten.sql.columnar.backend.velox.bloomFilter.numBits | The default number of bits to use for the velox bloom filter. | 8388608L | . | spark.gluten.sql.columnar.backend.velox.bloomFilter.maxNumBits | The max number of bits to use for the velox bloom filter. | 4194304L | . | spark.gluten.sql.columnar.backend.velox.fileHandleCacheEnabled | Disables caching if false. File handle cache should be disabled if files are mutable, i.e. file content may change while file path stays the same. |   | . | spark.gluten.sql.columnar.backend.velox.directorySizeGuess | Set the directory size guess for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.filePreloadThreshold | Set the file preload threshold for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.prefetchRowGroups | Set the prefetch row groups for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.loadQuantum | Set the load quantum for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.maxCoalescedDistanceBytes | Set the max coalesced distance bytes for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.maxCoalescedBytes | Set the max coalesced bytes for velox file scan. |   | . | spark.gluten.sql.columnar.backend.velox.cachePrefetchMinPct | Set prefetch cache min pct for velox file scan. |   | . | spark.gluten.velox.awsSdkLogLevel | Log granularity of AWS C++ SDK in velox. | FATAL | . | spark.gluten.velox.fs.s3a.retry.mode | Retry mode for AWS s3 connection error, can be “legacy”, “standard” and “adaptive”. | legacy | . | spark.gluten.velox.fs.s3a.connect.timeout | Timeout for AWS s3 connection. | 1s | . | spark.gluten.sql.columnar.backend.velox.orc.scan.enabled | Enable velox orc scan. If disabled, vanilla spark orc scan will be used. | true | . | spark.gluten.sql.complexType.scan.fallback.enabled | Force fallback for complex type scan, including struct, map, array. | true | . Additionally, you can control the configurations of gluten at thread level by local property. | Parameters | Description | Recommend Setting | . | gluten.enabledForCurrentThread | Control the usage of gluten at thread level. | true | . Below is an example of developing an application using scala to set local properties. // Before executing the query, set local properties. sparkContext.setLocalProperty(key, value) spark.sql(\"select * from demo_tables\").show() . ",
    "url": "/archives/v1.3.0/configuration#velox-parameters",
    
    "relUrl": "/archives/v1.3.0/configuration#velox-parameters"
  },"352": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": " ",
    "url": "/archives/v1.3.0/configuration",
    
    "relUrl": "/archives/v1.3.0/configuration"
  },"353": {
    "doc": "CPP Code Style",
    "title": "Gluten CPP Core Guidelines",
    "content": "This is a set of CPP core guidelines for Gluten. The aim is to make the codebase simpler, more efficient, more maintainable by promoting consistency and according to best practices. ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#gluten-cpp-core-guidelines",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#gluten-cpp-core-guidelines"
  },"354": {
    "doc": "CPP Code Style",
    "title": "Philosophy",
    "content": "Philosophical rules are generally not measurable. However, they are valuable. For Gluten CPP coding, there are a few Philosophical rules as the following. | Write in ISO Standard C++. | Standard API first, the CPP programming APIs are priority to system calls. | Write code consistently. It’s good for understanding and maintaining. | Keep simple, making code clear and easy to read. | Optimize code for reader, not for writer. Thus, more time will be spent reading code than writing it. | Make it work, and then make it better or faster. | Don’t import any complexity if possible. Collaborate with minimal knowledge consensus. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#philosophy",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#philosophy"
  },"355": {
    "doc": "CPP Code Style",
    "title": "Code Formatting",
    "content": "Many aspects of C++ coding style will be covered by clang-format, such as spacing, line width, indentation and ordering (for includes, using directives and etc).  . | Always ensure your code is compatible with clang-format-12 for Velox backend. | dev/formatcppcode.sh is provided for formatting Velox CPP code. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#code-formatting",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#code-formatting"
  },"356": {
    "doc": "CPP Code Style",
    "title": "Naming Conventions",
    "content": ". | Use PascalCase for types (class, struct, enum, type alias, type template parameter) and file name. | Use camelCase for function, member and local variable, and non-type template parameter. | Use camelCase_ for private and protected member variable. | Use snake_case for namespace name and build target. | Use UPPER_SNAKE_CASE for macro. | Use kPascalCase for static constant and enumerator. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#naming-conventions",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#naming-conventions"
  },"357": {
    "doc": "CPP Code Style",
    "title": "Designs",
    "content": ". | No over design. | No negation of negation, isValid is better than isNotInvalid. | Avoid corner case, and common case first. | Express ideas directly, don’t let me think. | Make a check for the arguments in the interface between modules, and don’t make a check in the inner implementation, use assert in the private implementation instead of too much safe check. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#designs",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#designs"
  },"358": {
    "doc": "CPP Code Style",
    "title": "Source File &amp; Header File",
    "content": ". | All header files must have a single-inclusion guard using #pragma once | Always use .h as header file suffix, not .hpp. | Always use .cc as source file suffix, neither .cpp nor .cxx. | One file should contain one main class, and the file name should be consistent with the main class name. | Obvious exception: files used for defining various misc functions. | . | If a header file has a corresponding source file, they should have the same file name with different suffix, such as a.h vs a.cc. | If a function is declared in the file a.h, ensure it’s defined in the corrosponding source file a.cc, do not define it in other files. | No deep source directory for CPP files, not do it as JAVA. | Include header files should satisfy the following rules. | Include the necessary header files, which means the source file (.cc) containing the only one line #include \"test.h\" can be compiled successfully without including any other header files. | Do not include any unnecessary header files, the more including, the slower compiling. | In one word, no more, no less, just as needed. | . | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#source-file--header-file",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#source-file--header-file"
  },"359": {
    "doc": "CPP Code Style",
    "title": "Class",
    "content": ". | Base class name doesn’t end with Base, use Backend instead of BackendBase. | Ensure one class does one thing, and follows the single responsibility principle. | No big class, No huge class, No too much interfaces. | Distinguish interface from implementation, make implementations private. | When designing a class hierarchy, distinguish between interface inheritance and implementation inheritance. | Ensure that public inheritance represent the relation of is-a. | Ensure that private inheritance represent the relation of implements-with. | . | Don’t make a function virtual without reason. | Ensure the polymorphic base class has a virtual deconstructor. | Use override to make overriding explicit and to make the compiler work. | Use const to mark the member function read-only as far as possible. | When you try to define a copy constructor or a operator= for a class, remember the Rule of three/five/zero. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#class",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#class"
  },"360": {
    "doc": "CPP Code Style",
    "title": "Function",
    "content": ". | Make functions short and simple. | Calling a meaningful function is more readable than writing too many statements in place, but the performance-sensitive code path is an exception. | Give the function a good name, how to check whether the function name is good or not. | When you read it loudly, you feel smooth. | The information can be represented by arguments should not be encoded into the function name. such as. use get(size_t index) instead of getByIndex. | . | A function should focus on a single logic operation. | A function should do as the name meaning. | do everything converd by the function name | don’t do anything not convered by the function name | . | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#function",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#function"
  },"361": {
    "doc": "CPP Code Style",
    "title": "Variable",
    "content": ". | Make variable names simple and meaningful. | Don’t group all your variables at the top of the scope, it’s an outdated habit. | Declare variables as close to the usage point as possible. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#variable",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#variable"
  },"362": {
    "doc": "CPP Code Style",
    "title": "Constant",
    "content": ". | Prefer const variables to using preprocessor (#define) to define constant values. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#constant",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#constant"
  },"363": {
    "doc": "CPP Code Style",
    "title": "Macro",
    "content": ". | Macros downgrade readability, break mind, and affect debug. | Macros have side effects. | Use macros cautiously and carefully. | Consider using const variables or inline functions to replace macros. | Consider defining macros with the wrap of do {...} while (0) | Avoid using 3rd party library macros directly. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#macro",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#macro"
  },"364": {
    "doc": "CPP Code Style",
    "title": "Namespace",
    "content": ". | Don’t using namespace xxx in header files. Instead, you can do this in source files. But it’s still not encouraged. | Place all Gluten CPP codes under namespace gluten because one level namespace is enough. No nested namespace. Nested namespaces bring mess. | The anonymous namespace is recommended for defining file level classes, functions and variables. It’s used to place file scoped static functions and variables. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#namespace",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#namespace"
  },"365": {
    "doc": "CPP Code Style",
    "title": "Resource Management",
    "content": ". | Use handles and RAII to manage resources automatically. | Immediately give the result of an explicit resource allocation to a manager object. | Prefer scoped objects and stack objects. | Use raw pointers to denote individual objects. | Use pointer + size_t to denote array objects if you don’t want to use containers. | A raw pointer (a T*) is non-owning. | A raw reference (a T&amp;) is non-owning. | Understand the difference of unique_ptr, shared_ptr, weak_ptr. | unique_ptr represents ownership, but not share ownership. unique_ptr is equivalent to RAII, release the resource when the object is destructed. | shared_ptr represents shared ownership by use-count. It is more expensive that unqiue_ptr. | weak_ptr models temporary ownership. It is useful in breaking reference cycles formed by objects managed by shared_ptr. | . | Use unique_ptr or shared_ptr to represent ownership. | Prefer unique_ptr over shared_ptr unless you need to share ownership. | Use make_unique to make unique_ptrs. | Use make_shared to make shared_ptrs. | Take smart pointers as parameters only to explicitly express lifetime semantics. | For general use, take T* or T&amp; arguments rather than smart pointers. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#resource-management",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#resource-management"
  },"366": {
    "doc": "CPP Code Style",
    "title": "Exception",
    "content": ". | The exception specifications are changing always. The difference between various CPP standards is big, so we should use exception cautiously in Gluten. | Prefer return code to throwing exceptions. | Prefer compile-time checking to run-time checking. | Encapsulate messy constructors, rather than spreading through the code. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#exception",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#exception"
  },"367": {
    "doc": "CPP Code Style",
    "title": "Code Comment",
    "content": ". | Add necessary comments. The comment is not the more the better, also not the less the better. | Good comment makes obscure code easily understood. It’s unnecessary to add comments for quite obvious code. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#code-comment",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#code-comment"
  },"368": {
    "doc": "CPP Code Style",
    "title": "References",
    "content": ". | CppCoreGuidelines | Velox CODING_STYLE | Thanks Gluten developers for their wise suggestions and helps. | . ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/#references",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/#references"
  },"369": {
    "doc": "CPP Code Style",
    "title": "CPP Code Style",
    "content": " ",
    "url": "/archives/v1.3.0/developers/cpp-code-style/",
    
    "relUrl": "/archives/v1.3.0/developers/cpp-code-style/"
  },"370": {
    "doc": "How To Use Gluten",
    "title": "How to understand the key work of Gluten?",
    "content": "The Gluten worked as the role of bridge, it’s a middle layer between the Spark and the native execution library. The Gluten is responsibility for validating whether the operators of the Spark plan can be executed by the native engine or not. If yes, the Gluten transforms Spark plan to Substrait plan, and then send the Substrait plan to the native engine. The Gluten codes consist of two parts: the C++ codes and the Java/Scala codes. | All C++ codes are placed under the directory of gluten_home/cpp, the Java/Scala codes are placed under several directories, such as gluten_home/gluten-core gluten_home/gluten-data gluten_home/backends-velox. | The Java/Scala codes are responsibility for validating and transforming the execution plan. Source data should also be provided, the source data may come from files or other forms such as networks. | The C++ codes take the Substrait plan and the source data as inputs and transform the Substrait plan to the corresponding backend plan. If the backend is Velox, the Substrait plan will be transformed to the Velox plan, and then be executed. | . JNI is a programming technology of invoking C++ from Java. All JNI interfaces are defined in the file JniWrapper.cc under the directory jni. ",
    "url": "/archives/v1.3.0/developers/how-to-use/#how-to-understand-the-key-work-of-gluten",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#how-to-understand-the-key-work-of-gluten"
  },"371": {
    "doc": "How To Use Gluten",
    "title": "How to debug in Gluten?",
    "content": " ",
    "url": "/archives/v1.3.0/developers/how-to-use/#how-to-debug-in-gluten",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#how-to-debug-in-gluten"
  },"372": {
    "doc": "How To Use Gluten",
    "title": "1 How to debug C++",
    "content": "If you don’t concern about the Scala/Java codes and just want to debug the C++ codes executed in native engine, you may debug the C++ via benchmarks with GDB. To debug C++, you have to generate the example files, the example files consist of: . | A file contained Substrait plan in JSON format | One or more input data files in Parquet format | . You can generate the example files by the following steps: . | build Velox and Gluten CPP gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . | Compiling with --build_type=Debug is good for debugging. | The executable file generic_benchmark will be generated under the directory of gluten_home/cpp/build/velox/benchmarks/. | . | build Gluten and generate the example files cd gluten_home mvn clean package -Pspark-3.2 -Pbackends-velox -Prss mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . | After the above operations, the examples files are generated under gluten_home/backends-velox | You can check it by the command tree gluten_home/backends-velox/generated-native-benchmark/ | You may replace -Pspark-3.2 with -Pspark-3.3 if your spark’s version is 3.3 $ tree gluten_home/backends-velox/generated-native-benchmark/ gluten_home/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . | . | now, run benchmarks with GDB cd gluten_home/cpp/build/velox/benchmarks/ gdb generic_benchmark . | When GDB load generic_benchmark successfully, you can set breakpoint on the main function with command b main, and then run with command r, then the process generic_benchmark will start and stop at the main function. | You can check the variables’ state with command p variable_name, or execute the program line by line with command n, or step-in the function been called with command s. | Actually, you can debug generic_benchmark with any gdb commands as debugging normal C++ program, because the generic_benchmark is a pure C++ executable file in fact. | . | gdb-tui is a valuable feature and is worth trying. You can get more help from the online docs. gdb-tui . | you can start generic_benchmark with specific JSON plan and input files . | If you omit them, the example.json, example_lineitem + example_orders under the directory of gluten_home/backends-velox/generated-native-benchmark will be used as default. | You can also edit the file example.json to custom the Substrait plan or specify the inputs files placed in the other directory. | . | get more detail information about benchmarks from MicroBenchmarks | . ",
    "url": "/archives/v1.3.0/developers/how-to-use/#1-how-to-debug-c",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#1-how-to-debug-c"
  },"373": {
    "doc": "How To Use Gluten",
    "title": "2 How to debug plan validation process",
    "content": "Gluten will validate generated plan before execute it, and validation usually happens in native side, so we provide a utility to help debug validation process in native side. | Run query with conf spark.gluten.sql.debug=true, and you will find generated plan be printed in stderr with json format, save it as plan.json for example. | Compile cpp part with --build_benchmarks=ON, then check plan_validator_util executable file in gluten_home/cpp/build/velox/benchmarks/. | Run or debug with ./plan_validator_util &lt;path&gt;/plan.json | . ",
    "url": "/archives/v1.3.0/developers/how-to-use/#2-how-to-debug-plan-validation-process",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#2-how-to-debug-plan-validation-process"
  },"374": {
    "doc": "How To Use Gluten",
    "title": "3 How to debug Java/Scala",
    "content": "wait to add . ",
    "url": "/archives/v1.3.0/developers/how-to-use/#3-how-to-debug-javascala",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#3-how-to-debug-javascala"
  },"375": {
    "doc": "How To Use Gluten",
    "title": "4 How to debug with core-dump",
    "content": "wait to complete . cd the_directory_of_core_file_generated gdb gluten_home/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . | the core-Executor task l-2000883-1671542526 represents the core file name. | . ",
    "url": "/archives/v1.3.0/developers/how-to-use/#4-how-to-debug-with-core-dump",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#4-how-to-debug-with-core-dump"
  },"376": {
    "doc": "How To Use Gluten",
    "title": "How to run TPC-H on Velox backend",
    "content": "Now, both Parquet and DWRF format files are supported, related scripts and files are under the directory of gluten_home/backends-velox/workload/tpch. The file README.md under gluten_home/backends-velox/workload/tpch offers some useful help but it’s still not enough and exact. One way of run TPC-H test is to run velox with docker by workflow, you can refer to velox_docker.yml . Here will explain how to run TPC-H on Velox backend with the Parquet file format. | First step, prepare the datasets, you have two choices. | One way, generate Parquet datasets using the script under gluten_home/backends-velox/workload/tpch/gen_data/parquet_dataset, You can get help from the above mentioned README.md. | The other way, using the small dataset under gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox directly, If you just want to make simple TPC-H testing, this dataset is a good choice. | . | Second step, run TPC-H on Velox backend testing. | Modify gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.scala. | . | set var parquet_file_path to correct directory. If using the small dataset directly in the step one, then modify it as below var parquet_file_path = \"gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox\" . | set var gluten_root to correct directory. If gluten_home is the directory of /home/gluten, then modify it as below var gluten_root = \"/home/gluten\" . - Modify `gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.sh`. | Set GLUTEN_JAR correctly. Please refer to the section of Build Gluten with Velox Backend | Set SPARK_HOME correctly. | Set the memory configurations appropriately. - Execute tpch_parquet.sh using the below command. | cd gluten_home/backends-velox/workload/tpch/run_tpch/ | ./tpch_parquet.sh | . | . ",
    "url": "/archives/v1.3.0/developers/how-to-use/#how-to-run-tpc-h-on-velox-backend",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#how-to-run-tpc-h-on-velox-backend"
  },"377": {
    "doc": "How To Use Gluten",
    "title": "How to run TPC-DS",
    "content": "wait to add . ",
    "url": "/archives/v1.3.0/developers/how-to-use/#how-to-run-tpc-ds",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#how-to-run-tpc-ds"
  },"378": {
    "doc": "How To Use Gluten",
    "title": "How to track the memory exhaust problem",
    "content": "When your gluten spark jobs failed because of OOM, you can track the memory allocation’s call stack by configuring spark.gluten.backtrace.allocation = true. The above configuration will use BacktraceAllocationListener wrapping from SparkAllocationListener to create VeloxMemoryManager. BacktraceAllocationListener will check every allocation, if a single allocation bytes exceeds a fixed value or the accumulative allocation bytes exceeds 1/2/3…G, the call stack of memory allocation will be outputted to standard output, you can check the backtrace and get some valuable information about tracking the memory exhaust issues. You can also adjust the policy to decide when to backtrace, such as the fixed value. ",
    "url": "/archives/v1.3.0/developers/how-to-use/#how-to-track-the-memory-exhaust-problem",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/#how-to-track-the-memory-exhaust-problem"
  },"379": {
    "doc": "How To Use Gluten",
    "title": "How To Use Gluten",
    "content": "There are some common questions about developing, debugging and testing been asked again and again. In order to help the developers to contribute to Gluten as soon as possible, we collected these frequently asked questions, and organized them in the form of Q&amp;A. It’s convenient for the developers to check and learn. when you encountered a new problem and then resolved it, please add a new item to this document if you think it may be helpful to the other developers. We use gluten_home to represent the home directory of Gluten in this document. ",
    "url": "/archives/v1.3.0/developers/how-to-use/",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-use/"
  },"380": {
    "doc": "How To Release",
    "title": "How to Release",
    "content": "This section outlines the steps for releasing Apache Gluten (incubating) according to the Apache release guidelines. All projects under the Apache umbrella must adhere to the Apache Release Policy. This guide is designed to assist you in comprehending the policy and navigating the process of releasing projects at Apache. ",
    "url": "/docs/developers/how-to-release/#how-to-release",
    
    "relUrl": "/docs/developers/how-to-release/#how-to-release"
  },"381": {
    "doc": "How To Release",
    "title": "Release Process",
    "content": ". | Prepare the release artifacts. | Upload the release artifacts to the SVN repository. | Verify the release artifacts. | Initiate a release vote. | Announce the results and the release. | . Prepare the release artifacts. | Create a branch from the target git repository. | Tag a RC and draft the release notes. | Build and Sign the release artifacts (including source archives, binaries, …etc). | Generate checksums for the artifacts. | . How to Sign the release artifacts. | Create a GPG key . | Add the GPG key to the KEYS file . | Sign the release artifacts with the GPG key. | . # create a GPG key, after executing this command, select the first one RSA 和 RSA $ gpg --full-generate-key # list the GPG keys $ gpg --keyid-format SHORT --list-keys # upload the GPG key to the key server, xxx is the GPG key id # eg: pub rsa4096/4C21E346 2024-05-06 [SC], 4C21E346 is the GPG key id; $ gpg --keyserver keyserver.ubuntu.com --send-key xxx # append the GPG key to the KEYS file the svn repository # [IMPORTANT] Don't replace the KEYS file, just append the GPG key to the KEYS file. $ svn co https://dist.apache.org/repos/dist/release/incubator/gluten/ $ (gpg --list-sigs xxx@apache.org &amp;&amp; gpg --export --armor xxx@apache.org) &gt;&gt; KEYS $ svn ci -m \"add gpg key\" # sign the release artifacts, xxxx is xxx@apache.org $ for i in *.tar.gz; do echo $i; gpg --local-user xxxx --armor --output $i.asc --detach-sig $i ; done . How to Generate checksums for the release artifacts. # create the checksums $ for i in *.tar.gz; do echo $i; sha512sum $i &gt; $i.sha512 ; done . Upload the release artifacts to the SVN repository. | Create a project directory in the SVN repository (1st time only). https://dist.apache.org/repos/dist/dev/incubator/gluten/ . | Create a directory for the release artifacts in the SVN repository. https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} release-version format: apache-gluten-#.#.#-rc# . | Upload the release artifacts to the SVN repository. $ svn co https://dist.apache.org/repos/dist/dev/incubator/gluten/ $ cp /path/to/release/artifacts/* ./{release-version}/ $ svn add ./{release-version}/* $ svn commit -m \"add Apache Gluten release artifacts for {release-version}\" . | After the upload, please visit the link https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} to verify if the file upload is successful or not. The upload release artifacts should be include * apache-gluten-#.#.#-incubating-src.tar.gz * apache-gluten-#.#.#-incubating-src.tar.gz.asc * apache-gluten-#.#.#-incubating-src.tar.gz.sha512 . | . Verify the release artifacts. Please follow below steps to verify the release artifacts. | Check if the Download links are valid. | Check if the checksums and GPG signatures are valid. | Check if the release artifacts name is qualified and match with the current release. | Check if LICENSE and NOTICE files are correct. | Check if the License Headers are included in all files if necessary. | No unlicensed compiled archives bundled in source archive. | . How to Verify the Signatures . Please follow below steps to verify the signatures. # download KEYS $ curl https://dist.apache.org/repos/dist/release/incubator/gluten/KEYS &gt; KEYS # import KEYS and trust the key, please replace the email address with the one you want to trust. $ gpg --import KEYS $ gpg --edit-key xxx@apache.org gpg&gt; trust gpg&gt; 5 gpg&gt; y gpg&gt; quit # enter the directory where the release artifacts are located $ cd /path/to/release/artifacts # verify the signature $ for i in *.tar.gz; do echo $i; gpg --verify $i.asc $i ; done # if you see 'Good signature' in the output, it means the signature is valid. How to Verify the checksums . Please follow below steps to verify the checksums . # verify the checksums $ for i in *.tar.gz; do echo $i; sha512sum --check $i.sha512; done . Initiate a release vote. | Email a vote request to dev@gluten.apache.org, requiring at least 3 PPMC +1s. | Allow 72 hours or until enough votes are collected. | Share the vote outcome on the dev list. | If successful, request a vote on general@incubator.apache.org, needing 3 PMC +1s. | Wait 72 hours or for sufficient votes. | Announce the results on the general list. | . Vote Email Template . [VOTE] Release Apache Gluten (Incubating) {release-version} Hello, This is a call for vote to release Apache Gluten (Incubating) version {release-version}. The vote thread: https://lists.apache.org/thread/{id} Vote Result: https://lists.apache.org/thread/{id} The release candidates: https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version}/ Release notes: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Git tag for the release: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Git commit id for the release: https://github.com/apache/incubator-gluten/commit/{id} Keys to verify the Release Candidate: https://downloads.apache.org/incubator/gluten/KEYS The vote will be open for at least 72 hours or until the necessary number of votes are reached. Please vote accordingly: [ ] +1 approve [ ] +0 no opinion [ ] -1 disapprove with the reason Checklist for reference: [ ] Download links are valid. [ ] Checksums and PGP signatures are valid. [ ] Source code distributions have correct names matching the current release. [ ] LICENSE and NOTICE files are correct for each Apache Gluten repo. [ ] All files have license headers if necessary. [ ] No unlicensed compiled archives bundled in source archive. To compile from the source, please refer to: https://github.com/apache/incubator-gluten#building-from-source Thanks, &lt;YOUR NAME&gt; . Announce the results and the release. Announce Email Template . Hello everyone, The Apache Gluten (Incubating) {release-version} has been released! Apache Gluten is a Q&amp;A platform software for teams at any scale. Whether it's a community forum, help center, or knowledge management platform, you can always count on Apache Gluten. Download Links: https://downloads.apache.org/incubator/gluten/ Release Notes: https://github.com/apache/incubator-gluten/releases/tag/{release-version} Website: https://gluten.apache.org/ Resources: - Issue: https://github.com/apache/incubator-gluten/issues - Mailing list: dev@gluten.apache.org Thanks, &lt;YOUR NAME&gt; . Migrate candidate to the release Apache SVN . After the vote has passed, you need to migrate the RC build release to an official release by moving the artifacts from Apache SVN’s dev directory to the release directory. Please follow the steps below to upload the artifacts: . $ svn mv https://dist.apache.org/repos/dist/dev/incubator/gluten/{release-version} https://dist.apache.org/repos/dist/release/incubator/gluten/{release-version} -m \"transfer packages for gluten {release-version}\" . ",
    "url": "/docs/developers/how-to-release/#release-process",
    
    "relUrl": "/docs/developers/how-to-release/#release-process"
  },"382": {
    "doc": "How To Release",
    "title": "How To Release",
    "content": " ",
    "url": "/docs/developers/how-to-release/",
    
    "relUrl": "/docs/developers/how-to-release/"
  },"383": {
    "doc": "How To Scan Security issues",
    "title": "How to scan the security issues",
    "content": "This section outlines the steps to use tools to scan Apache Gluten (incubating) source code and make sure no vulnerability issues in the code. All projects under the Apache umbrella must adhere to the Apache Release Policy. This guide is designed to assist you in comprehending the policy and navigating the process of releasing projects at Apache. ",
    "url": "/archives/v1.3.0/developers/how-to-security-scan/#how-to-scan-the-security-issues",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-security-scan/#how-to-scan-the-security-issues"
  },"384": {
    "doc": "How To Scan Security issues",
    "title": "Scan Security Process",
    "content": "Before every Apache Gluten (incubating) release, we need to ensure there is no vulnerability issue in the source code. We use Trivy as the tool to scan all the security issues. | Install Trivy, please follow the steps to install Trivy: Trivy Installation . | Configuring Trivy, please follow the guide to configure Trivy for specific operation: Trivy Configuration . | Run Trivy File System Scan with the source code. Below is an example about how we run Trivy scan with Apache Gluten (incubating) source code. You can use your own tpl file as a template. | . trivy fs --list-all-pkgs --format template --template \"@/PATH/TO/csv.tpl\" --output ./trivy-report.csv /PATH/TO/GLUTEN_LOCATION/ . | Open the report file and check if there is any vulnerability issue highlighted. We must guarantee all the vulnerability issue has been solved before an official release. | . ",
    "url": "/archives/v1.3.0/developers/how-to-security-scan/#scan-security-process",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-security-scan/#scan-security-process"
  },"385": {
    "doc": "How To Scan Security issues",
    "title": "How To Scan Security issues",
    "content": " ",
    "url": "/archives/v1.3.0/developers/how-to-security-scan/",
    
    "relUrl": "/archives/v1.3.0/developers/how-to-security-scan/"
  },"386": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Generate Micro Benchmarks for Velox Backend",
    "content": "This document explains how to use the existing micro benchmark template in Gluten Cpp. A micro benchmark for Velox backend is provided in Gluten Cpp to simulate the execution of a first or middle stage in Spark. It serves as a more convenient alternative to debug in Gluten Cpp comparing with directly debugging in a Spark job. Developers can use it to create their own workloads, debug in native process, profile the hotspot and do optimizations. To simulate a first stage, you need to dump the Substrait plan and input split info into two JSON files. The input URIs of the splits should be exising file locations, which can be either local or HDFS paths. To simulate a middle stage, in addition to the JSON file, you also need to save the input data of this stage into Parquet files. The benchmark will load the data into Arrow format, then add Arrow2Velox to feed the data into Velox pipeline to reproduce the reducer stage. Shuffle exchange is not included. Please refer to the sections below to learn how to dump the Substrait plan and create the input data files. ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/#generate-micro-benchmarks-for-velox-backend",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/#generate-micro-benchmarks-for-velox-backend"
  },"387": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Try the example",
    "content": "To run a micro benchmark, user should provide one file that contains the Substrait plan in JSON format, and optional one or more input data files in parquet format. The commands below help to generate example input files: . cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_tests=ON --build_benchmarks=ON # Run test to generate input data files. If you are using spark 3.3, replace -Pspark-3.2 with -Pspark-3.3 mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am \\ -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . The generated example files are placed in gluten/backends-velox: . $ tree gluten/backends-velox/generated-native-benchmark/ gluten/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . Run micro benchmark with the generated files as input. You need to specify the absolute path to the input files: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example.json \\ --data /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_orders/part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet,\\ /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_lineitem/part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet \\ --threads 1 --iterations 1 --noprint-result --benchmark_filter=InputFromBatchStream . The output should be like: . 2022-11-18T16:49:56+08:00 Running ./generic_benchmark Run on (192 X 3800 MHz CPU s) CPU Caches: L1 Data 48 KiB (x96) L1 Instruction 32 KiB (x96) L2 Unified 2048 KiB (x96) L3 Unified 99840 KiB (x2) Load Average: 0.28, 1.17, 1.59 ***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead. -- Project[expressions: (n3_0:BIGINT, ROW[\"n1_0\"]), (n3_1:VARCHAR, ROW[\"n1_1\"])] -&gt; n3_0:BIGINT, n3_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 36.33us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 queuedWallNanos sum: 2.00us, count: 2, min: 0ns, max: 2.00us -- HashJoin[RIGHT SEMI (FILTER) n0_0=n1_0] -&gt; n1_0:BIGINT, n1_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 191.56us, Blocked wall time: 0ns, Peak memory: 2.00MB, Memory allocations: 8 HashBuild: Input: 582 rows (16.45KB, 1 batches), Output: 0 rows (0B, 0 batches), Cpu time: 1.84us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 distinctKey0 sum: 583, count: 1, min: 583, max: 583 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 59748, count: 1, min: 59748, max: 59748 HashProbe: Input: 37897 rows (296.07KB, 1 batches), Output: 535 rows (65.81KB, 1 batches), Cpu time: 189.71us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 5, Threads: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns -- ArrowStream[] -&gt; n0_0:BIGINT Input: 0 rows (0B, 0 batches), Output: 37897 rows (296.07KB, 1 batches), Cpu time: 1.29ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 -- ArrowStream[] -&gt; n1_0:BIGINT, n1_1:VARCHAR Input: 0 rows (0B, 0 batches), Output: 582 rows (16.45KB, 1 batches), Cpu time: 894.22us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 ----------------------------------------------------------------------------------------------------------------------------- Benchmark Time CPU Iterations UserCounters... ----------------------------------------------------------------------------------------------------------------------------- InputFromBatchVector/iterations:1/process_time/real_time/threads:1 41304520 ns 23740340 ns 1 collect_batch_time=34.7812M elapsed_time=41.3113M . ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/#try-the-example",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/#try-the-example"
  },"388": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Generate Substrait plan and input for any query",
    "content": "First, build Gluten with --build_benchmarks=ON. cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_benchmarks=ON # For debugging purpose, rebuild Gluten with build type `Debug`./dev/buildbundle-veloxbe.sh --build_benchmarks=ON --build_type=Debug . First, get the Stage Id from spark UI for the stage you want to simulate. And then re-run the query with below configurations to dump the inputs to micro benchmark. | Parameters | Description | Recommend Setting | . | spark.gluten.sql.benchmark_task.stageId | Spark task stage id | target stage id | . | spark.gluten.sql.benchmark_task.partitionId | Spark task partition id, default value -1 means all the partition of this stage | 0 | . | spark.gluten.sql.benchmark_task.taskId | If not specify partition id, use spark task attempt id, default value -1 means all the partition of this stage | target task attemp id | . | spark.gluten.saveDir | Directory to save the inputs to micro benchmark, should exist and be empty. | /path/to/saveDir | . Check the files in spark.gluten.saveDir. If the simulated stage is a first stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage, without input file splits. | Split file: JSON formatted, file name split_[stageId]_[partitionId]_[splitIndex].json. There can be more than one split file in a first stage task. Contains the substrait plan piece to the input file splits. | . Run benchmark. By default, the result will be printed to stdout. You can use --noprint-result to suppress this output. Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --split /absolut_path/to/split_[stageId]_[partitionId]_0.parquet,/absolut_path/to/split_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . If the simulated stage is a middle stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage. | Data file: Parquet formatted, file name data_[stageId]_[partitionId]_[iteratorIndex].json. There can be more than one input data file in a middle stage task. The input data files of a middle stage will be loaded as iterators to serve as the inputs for the pipeline: | . \"localFiles\": { \"items\": [ { \"uriFile\": \"iterator:0\" } ] } . Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --data /absolut_path/to/data_[stageId]_[partitionId]_0.parquet,/absolut_path/to/data_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . For some complex queries, stageId may cannot represent the Substrait plan input, please get the taskId from spark UI, and get your target parquet from saveDir. In this example, only one partition input with partition id 2, taskId is 36, iterator length is 2. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/complex_plan.json \\ --data /absolute_path/to/data_36_2_0.parquet,/absolute_path/to/data_36_2_1.parquet \\ --threads 1 --noprint-result . ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/#generate-substrait-plan-and-input-for-any-query",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/#generate-substrait-plan-and-input-for-any-query"
  },"389": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Save ouput to parquet to analyze",
    "content": "You can save the output to a parquet file to analyze. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --data /absolute_path/to/data.parquet --threads 1 --noprint-result --write-file=/absolute_path/to/result.parquet . ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/#save-ouput-to-parquet-to-analyze",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/#save-ouput-to-parquet-to-analyze"
  },"390": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Add shuffle write process",
    "content": "You can add the shuffle write process at the end of this stage. Note that this will ignore the --write-file option. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --split /absolute_path/to/split.json \\ --threads 1 --noprint-result --with-shuffle . By default, the compression codec for shuffle outputs is LZ4. You can switch to other codecs by adding one of the following argument flags to the command: . | –zstd: ZSTD codec, compression level 1 | –qat-gzip: QAT GZIP codec, compression level 1 | –qat-zstd: QAT ZSTD codec, compression level 1 | –iaa-gzip: IAA GZIP codec, compression level 1 | . Note using QAT or IAA codec requires Gluten cpp is built with these features. Please check the corresponding section in Velox document first for how to setup, build and enable these features in Gluten. For QAT support, please check Intel® QuickAssist Technology (QAT) support. For IAA support, please check Intel® In-memory Analytics Accelerator (IAA/IAX) support . ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/#add-shuffle-write-process",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/#add-shuffle-write-process"
  },"391": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Simulate Spark with multiple processes and threads",
    "content": "You can use below command to launch several processes and threads to simulate parallel execution on Spark. Each thread in the same process will be pinned to the core number starting from --cpu. Suppose running on a baremetal machine with 48C, 2-socket, HT-on, launching below command will utilize all vcores. processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --threads $threads --cpu $((i*threads)) &amp; done . If you want to add the shuffle write process, you can specify multiple direcotries by setting environment variable GLUTEN_SPARK_LOCAL_DIRS to a comma-separated string for shuffle write to spread the I/O pressure to multiple disks. mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done . Run Examples . We also provide some example inputs in cpp/velox/benchmarks/data. E.g. generic_q5/q5_first_stage_0.json simulates a first-stage in TPCH Q5, which has the the most heaviest table scan. You can follow below steps to run this example. | Open generic_q5/q5_first_stage_0.json with file editor. Search for \"uriFile\": \"LINEITEM\" and replace LINEITEM with the URI to one partition file in lineitem. In the next line, replace the number in \"length\": \"...\" with the actual file length. Suppose you are using the provided small TPCH table in cpp/velox/benchmarks/data/tpch_sf10m, the replaced JSON should be like: | . { \"items\": [ { \"uriFile\": \"file:///path/to/gluten/cpp/velox/benchmarks/data/tpch_sf10m/lineitem/part-00000-6c374e0a-7d76-401b-8458-a8e31f8ab704-c000.snappy.parquet\", \"length\": \"1863237\", \"parquet\": {} } ] } . | Launch multiple processes and multiple threads. Set GLUTEN_SPARK_LOCAL_DIRS and add –with-shuffle to the command. | . mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0.json --split /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0_split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done &gt;stdout.log 2&gt;stderr.log . You can find the “elapsed_time” and other metrics in stdout.log. In below output, the “elapsed_time” is ~10.75s. If you run TPCH Q5 with Gluten on Spark, a single task in the same Spark stage should take about the same time. ------------------------------------------------------------------------------------------------------------------ Benchmark Time CPU Iterations UserCounters... ------------------------------------------------------------------------------------------------------------------ SkipInput/iterations:1/process_time/real_time/threads:8 1317255379 ns 10061941861 ns 8 collect_batch_time=0 elapsed_time=10.7563G shuffle_compress_time=4.19964G shuffle_spill_time=0 shuffle_split_time=0 shuffle_write_time=1.91651G . ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/#simulate-spark-with-multiple-processes-and-threads",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/#simulate-spark-with-multiple-processes-and-threads"
  },"392": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Micro Benchmarks for Velox Backend",
    "content": " ",
    "url": "/archives/v1.3.0/developers/microbenchmarks/",
    
    "relUrl": "/archives/v1.3.0/developers/microbenchmarks/"
  },"393": {
    "doc": "New To Gluten",
    "title": "Environment",
    "content": "Now gluten supports Ubuntu20.04, Ubuntu22.04, centos8, centos7 and macOS. ",
    "url": "/archives/v1.3.0/developers/new-to/#environment",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#environment"
  },"394": {
    "doc": "New To Gluten",
    "title": "Openjdk8",
    "content": "Environment setting . For root user, the environment variables file is /etc/profile, it will make effect for all the users. For other user, you can set in ~/.bashrc. Guide for ubuntu . The default JDK version in ubuntu is java11, we need to set to java8. apt install openjdk-8-jdk update-alternatives --config java java -version . --config java to config java executable path, javac and other commands can also use this command to config. For some other uses, we suggest to set JAVA_HOME. export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # pay attention to $PATH double quote export PATH=\"$PATH:$JAVA_HOME/bin\" . Must set PATH with double quote in ubuntu. ",
    "url": "/archives/v1.3.0/developers/new-to/#openjdk8",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#openjdk8"
  },"395": {
    "doc": "New To Gluten",
    "title": "Maven 3.6.3 or above",
    "content": "Maven Dowload Page And then set the environment setting. ",
    "url": "/archives/v1.3.0/developers/new-to/#maven-363-or-above",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#maven-363-or-above"
  },"396": {
    "doc": "New To Gluten",
    "title": "GCC 9.4 or above",
    "content": " ",
    "url": "/archives/v1.3.0/developers/new-to/#gcc-94-or-above",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#gcc-94-or-above"
  },"397": {
    "doc": "New To Gluten",
    "title": "Compile gluten using debug mode",
    "content": "If you want to just debug java/scala code, there is no need to compile cpp code with debug mode. You can just refer to build-gluten-with-velox-backend. If you need to debug cpp code, please compile the backend code and gluten cpp code with debug mode. ## compile velox backend with benchmark and tests to debug gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . If you need to debug the tests in /gluten-ut, You need to compile java code with `-P spark-ut`. ",
    "url": "/archives/v1.3.0/developers/new-to/#compile-gluten-using-debug-mode",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#compile-gluten-using-debug-mode"
  },"398": {
    "doc": "New To Gluten",
    "title": "Java/scala code development with Intellij",
    "content": " ",
    "url": "/archives/v1.3.0/developers/new-to/#javascala-code-development-with-intellij",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#javascala-code-development-with-intellij"
  },"399": {
    "doc": "New To Gluten",
    "title": "Linux intellij local debug",
    "content": "Install the linux intellij version, and debug code locally. | Ask your linux maintainer to install the desktop, and then restart the server. | If you use Moba-XTerm to connect linux server, you don’t need to install x11 server, If not (e.g. putty), please follow this guide: X11 Forwarding: Setup Instructions for Linux and Mac . | Download intellij linux community version to linux server | Start Idea, bash &lt;idea_dir&gt;/idea.sh | . Notes: Sometimes, your desktop may stop accidently, left idea running. root@xx2:~bash idea-IC-221.5787.30/bin/idea.sh Already running root@xx2:~ps ux | grep intellij root@xx2:kill -9 &lt;pid&gt; . And then restart idea. ",
    "url": "/archives/v1.3.0/developers/new-to/#linux-intellij-local-debug",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#linux-intellij-local-debug"
  },"400": {
    "doc": "New To Gluten",
    "title": "Windows/Mac intellij remote debug",
    "content": "If you have Ultimate intellij, you can try to debug remotely. ",
    "url": "/archives/v1.3.0/developers/new-to/#windowsmac-intellij-remote-debug",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#windowsmac-intellij-remote-debug"
  },"401": {
    "doc": "New To Gluten",
    "title": "Set up gluten project",
    "content": ". | Make sure you have compiled gluten. | Load the gluten by File-&gt;Open, select &lt;gluten_home/pom.xml&gt;. | Activate your profiles such as , and Reload Maven Project, you will find all your need modules have been activated. | Create breakpoint and debug as you wish, maybe you can try CTRL+N to find TestOperator to start your test. | . ",
    "url": "/archives/v1.3.0/developers/new-to/#set-up-gluten-project",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#set-up-gluten-project"
  },"402": {
    "doc": "New To Gluten",
    "title": "Java/Scala code style",
    "content": "Intellij IDE supports importing settings for Java/Scala code style. You can import intellij-codestyle.xml to your IDE. See Intellij guide. To generate a fix for Java/Scala code style, you can run one or more of the below commands according to the code modules involved in your PR. For Velox backend: . mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.3 -Pspark-ut -DskipTests . For Clickhouse backend: . mvn spotless:apply -Pbackends-clickhouse -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-clickhouse -Pspark-3.3 -Pspark-ut -DskipTests . ",
    "url": "/archives/v1.3.0/developers/new-to/#javascala-code-style",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#javascala-code-style"
  },"403": {
    "doc": "New To Gluten",
    "title": "CPP code development with Visual Studio Code",
    "content": "This guide is for remote debug. We will connect the remote linux server by SSH. Download the windows vscode software The important leftside bar is: . | Explorer (Project structure) | Search | Run and Debug | Extensions (Install C/C++ Extension Pack, Remote Development, GitLens at least, C++ Test Mate is also suggested) | Remote Explorer (Connect linux server by ssh command, click +, then input ssh user@10.1.7.003) | Manage (Settings) | . Input your password in the above pop-up window, it will take a few minutes to install linux vscode server in remote machine folder ~/.vscode-server If download failed, delete this folder and try again. ",
    "url": "/archives/v1.3.0/developers/new-to/#cpp-code-development-with-visual-studio-code",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#cpp-code-development-with-visual-studio-code"
  },"404": {
    "doc": "New To Gluten",
    "title": "Usage",
    "content": "Set up project . File-&gt;Open Folder // select gluten folder Select cpp/CmakeList.txt as command prompt Select gcc version as command prompt . Settings . VSCode support 2 ways to set user setting. | Manage-&gt;Command Palette(Open settings.json, search by Preferences: Open Settings (JSON)) | Manage-&gt;Settings (Common setting) | . Build by vscode . VSCode will try to compile the debug version in /build. And we need to compile velox debug mode before, if you have compiled velox release mode, you just need to do. # Build the velox debug version in &lt;velox_home&gt;/_build/debug make debug EXTRA_CMAKE_FLAGS=\"-DVELOX_ENABLE_PARQUET=ON -DENABLE_HDFS=ON -DVELOX_BUILD_TESTING=OFF -DVELOX_ENABLE_DUCKDB=ON -DVELOX_BUILD_TEST_UTILS=ON\" . Then gluten will link velox debug library. Just click build in bottom bar, you will get intellisense search and link. Debug . The default compile command does not enable test and benchmark, so we cannot get any executable file Open the file in &lt;gluten_home&gt;/.vscode/settings.json (create if not exists) . { \"cmake.configureArgs\": [ \"-DBUILD_BENCHMARKS=ON\", \"-DBUILD_TESTS=ON\" ], \"C_Cpp.default.configurationProvider\": \"ms-vscode.cmake-tools\" } . Then we can get some executables, take velox_shuffle_writer_test as example . Click Run and Debug to create launch.json in &lt;gluten_home&gt;/.vscode/launch.json Click Add Configuration in the top of launch.json, select gdb launch or attach to exists program launch.json example . { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"velox shuffle writer test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/build/velox/tests/velox_shuffle_writer_test\", \"args\": [\"--gtest_filter=*TestSinglePartPartitioner*\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] }, { \"name\": \"benchmark test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/./generic_benchmark\", \"args\": [\"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/query.json\", \"--threads=1\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] } ] } . Change name, program, args to yours . Then you can create breakpoint and debug in Run and Debug section. Velox debug . For some velox tests such as ParquetReaderTest, tests need to read the parquet file in &lt;velox_home&gt;/velox/dwio/parquet/tests/examples, you should let the screen on ParquetReaderTest.cpp, then click Start Debuging, otherwise you will raise No such file or directory exception . ",
    "url": "/archives/v1.3.0/developers/new-to/#usage",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#usage"
  },"405": {
    "doc": "New To Gluten",
    "title": "Usefule notes",
    "content": "Upgrade vscode . No need to upgrade vscode version, if upgraded, will download linux server again, switch update mode to off Search update in Manage-&gt;Settings to turn off update mode . Colour setting . \"workbench.colorTheme\": \"Quiet Light\", \"files.autoSave\": \"afterDelay\", \"workbench.colorCustomizations\": { \"editor.wordHighlightBackground\": \"#063ef7\", // \"editor.selectionBackground\": \"#d1d1c6\", // \"tab.activeBackground\": \"#b8b9988c\", \"editor.selectionHighlightBackground\": \"#c5293e\" }, . Clang format . Now gluten uses clang-format 12 to format source files. apt-get install clang-format-12 . Set config in settings.json . \"clang-format.executable\": \"clang-format-12\", \"editor.formatOnSave\": true, . If exists multiple clang-format version, formatOnSave may not take effect, specify the default formatter Search default formatter in Settings, select Clang-Format. If your formatOnSave still make no effect, you can use shortcut SHIFT+ALT+F to format one file mannually. ",
    "url": "/archives/v1.3.0/developers/new-to/#usefule-notes",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#usefule-notes"
  },"406": {
    "doc": "New To Gluten",
    "title": "Debug cpp code with coredump",
    "content": "mkdir -p /mnt/DP_disk1/core sysctl -w kernel.core_pattern=/mnt/DP_disk1/core/core-%e-%p-%t cat /proc/sys/kernel/core_pattern # set the core file to unlimited size echo \"ulimit -c unlimited\" &gt;&gt; ~/.bashrc # then you will get the core file at `/mnt/DP_disk1/core` when the program crashes # gdb -c corefile # gdb &lt;gluten_home&gt;/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . ‘core-Executor task l-2000883-1671542526’ is the generated core file name. (gdb) bt (gdb) f7 (gdb) set print pretty on (gdb) p *this . | Get the backtrace | Switch to 7th stack | Print the variable in a more readable way | Print the variable fields | . Sometimes you only get the cpp exception message, you can generate core dump file by the following code: . char* p = nullptr; *p = 'a'; . or by the following commands: . | gcore &lt;pid&gt; | kill -s SIGSEGV &lt;pid&gt; | . ",
    "url": "/archives/v1.3.0/developers/new-to/#debug-cpp-code-with-coredump",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#debug-cpp-code-with-coredump"
  },"407": {
    "doc": "New To Gluten",
    "title": "Debug cpp with gdb",
    "content": "You can use gdb to debug tests and benchmarks. And also you can debug jni call. Place the following code to your debug path. pid_t pid = getpid(); printf(\"----------------------------------pid: %lun\", pid); sleep(10); . You can also get the pid by java command or grep java program when executing unit test. jps 1375551 ScalaTestRunner ps ux | grep TestOperator . Execute gdb command to debug: . gdb attach &lt;pid&gt; . gdb attach 1375551 wait to attach.... (gdb) b &lt;velox_home&gt;/velox/substrait/SubstraitToVeloxPlan.cpp:577 (gdb) c . ",
    "url": "/archives/v1.3.0/developers/new-to/#debug-cpp-with-gdb",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#debug-cpp-with-gdb"
  },"408": {
    "doc": "New To Gluten",
    "title": "Run TPC-H and TPC-DS",
    "content": "We supply &lt;gluten_home&gt;/tools/gluten-it to execute these queries Refer to velox_docker.yml . ",
    "url": "/archives/v1.3.0/developers/new-to/#run-tpc-h-and-tpc-ds",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#run-tpc-h-and-tpc-ds"
  },"409": {
    "doc": "New To Gluten",
    "title": "Run gluten+velox on clean machine",
    "content": "We can run gluten + velox on clean machine by one command (supported OS: Ubuntu20.04/22.04, Centos 7/8, etc.). spark-shell --name run_gluten \\ --master yarn --deploy-mode client \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --jars https://github.com/oap-project/gluten/releases/download/v1.0.0/gluten-velox-bundle-spark3.2_2.12-ubuntu_20.04_x86_64-1.0.0.jar \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager . ",
    "url": "/archives/v1.3.0/developers/new-to/#run-glutenvelox-on-clean-machine",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/#run-glutenvelox-on-clean-machine"
  },"410": {
    "doc": "New To Gluten",
    "title": "New To Gluten",
    "content": "Help users to debug and test with gluten. For intel internal developer, you could refer to internal wiki New Employee Guide to get more information such as proxy settings, Gluten has cpp code and java/scala code, we can use some useful IDE to read and debug. ",
    "url": "/archives/v1.3.0/developers/new-to/",
    
    "relUrl": "/archives/v1.3.0/developers/new-to/"
  },"411": {
    "doc": "Substrait Modifications",
    "title": "Substrait Modifications in Gluten",
    "content": "Substrait is a project aiming to create a well-defined, cross-language specification for data compute operations. Since it is still under active development, there are some lacking representations for Gluten needed computing operations. At the same time, some existing representations need to be modified a bit to satisfy the needs of computing. In Gluten, the base version of Substrait is v0.23.0. This page records all the Gluten changes to Substrait proto files for reference. It is preferred to upstream these changes to Substrait, but for those cannot be upstreamed, alternatives like AdvancedExtension could be considered. ",
    "url": "/archives/v1.3.0/developers/substrait/#substrait-modifications-in-gluten",
    
    "relUrl": "/archives/v1.3.0/developers/substrait/#substrait-modifications-in-gluten"
  },"412": {
    "doc": "Substrait Modifications",
    "title": "Modifications to algebra.proto",
    "content": ". | Added JsonReadOptions and TextReadOptions in FileOrFiles(#1584). | Changed join type JOIN_TYPE_SEMI to JOIN_TYPE_LEFT_SEMI and JOIN_TYPE_RIGHT_SEMI(#408). | Added WindowRel, added column_name and window_type in WindowFunction, changed Unbounded in WindowFunction into Unbounded_Preceding and Unbounded_Following, and added WindowType(#485). | Added output_schema in RelRoot(#1901). | Added ExpandRel(#1361). | Added GenerateRel(#574). | Added PartitionColumn in LocalFiles(#2405). | Added WriteRel (#3690). | . ",
    "url": "/archives/v1.3.0/developers/substrait/#modifications-to-algebraproto",
    
    "relUrl": "/archives/v1.3.0/developers/substrait/#modifications-to-algebraproto"
  },"413": {
    "doc": "Substrait Modifications",
    "title": "Modifications to type.proto",
    "content": ". | Added Nothing in Type(#791). | Added names in Struct(#1878). | Added PartitionColumns in NamedStruct(#320). | Remove PartitionColumns and add column_types in NamedStruct(#2405). | . ",
    "url": "/archives/v1.3.0/developers/substrait/#modifications-to-typeproto",
    
    "relUrl": "/archives/v1.3.0/developers/substrait/#modifications-to-typeproto"
  },"414": {
    "doc": "Substrait Modifications",
    "title": "Substrait Modifications",
    "content": " ",
    "url": "/archives/v1.3.0/developers/substrait/",
    
    "relUrl": "/archives/v1.3.0/developers/substrait/"
  },"415": {
    "doc": "Docker script for CentOS 7",
    "title": "Docker script for CentOS 7",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on CentOS 7: . Run on host as root user: . docker pull centos:7 docker run -itd --name gluten centos:7 /bin/bash docker attach gluten . Run in docker: . yum -y install epel-release centos-release-scl yum -y install \\ git236 \\ dnf \\ cmake3 \\ devtoolset-9 \\ java-1.8.0-openjdk \\ java-1.8.0-openjdk-devel \\ ninja-build \\ wget \\ autoconf \\ autoconf-archive \\ automake \\ perl-IPC-Cmd \\ libicu-devel \\ bison \\ libtool \\ patch \\ flex \\ sudo # gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin # cmake 3.x is required ln -s /usr/bin/cmake3 /usr/local/bin/cmake # enable gcc 9 . /opt/rh/devtoolset-9/enable || exit 1 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # If you have the same error with issue-3283, you need to add the parameter `--compile_arrow_java=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/archives/v1.3.0/developers/docker-centos7/",
    
    "relUrl": "/archives/v1.3.0/developers/docker-centos7/"
  },"416": {
    "doc": "Docker script for CentOS 8",
    "title": "Docker script for CentOS 8",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on Centos8: . Run on host as root user: . docker pull centos:8 docker run -itd --name gluten centos:8 /bin/bash docker attach gluten . Run in docker: . #update mirror sed -i -e \"s|mirrorlist=|#mirrorlist=|g\" /etc/yum.repos.d/CentOS-* sed -i -e \"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\" /etc/yum.repos.d/CentOS-* dnf install -y epel-release sudo yum install -y dnf-plugins-core yum config-manager --set-enabled powertools dnf --enablerepo=powertools install -y ninja-build dnf --enablerepo=powertools install -y libdwarf-devel dnf install -y --setopt=install_weak_deps=False ccache gcc-toolset-9 git wget which libevent-devel \\ openssl-devel re2-devel libzstd-devel lz4-devel double-conversion-devel \\ curl-devel cmake libicu-devel source /opt/rh/gcc-toolset-9/enable || exit 1 yum install -y java-1.8.0-openjdk-devel patch export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH #gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/archives/v1.3.0/developers/docker-centos8/",
    
    "relUrl": "/archives/v1.3.0/developers/docker-centos8/"
  },"417": {
    "doc": "Docker script for Ubuntu 22.04/20.04",
    "title": "Docker script for Ubuntu 22.04/20.04",
    "content": "To the first build, it’s suggested to build Gluten in a clean docker image. Otherwise it’s easy to run into library version conflict issues. Here is a docker script we verified to build Gluten+Velox backend on Ubuntu22.04/20.04: . Run on host as root user: . docker pull ubuntu:22.04 docker run -itd --network host --name gluten ubuntu:22.04 /bin/bash docker attach gluten . Run in docker: . apt-get update #install gcc and libraries to build arrow apt install software-properties-common apt install maven build-essential cmake libssl-dev libre2-dev libcurl4-openssl-dev clang lldb lld libz-dev git ninja-build uuid-dev autoconf-archive curl zip unzip tar pkg-config bison libtool flex vim #velox script needs sudo to install dependency libraries apt install sudo # make sure jemalloc is uninstalled, jemalloc will be build in vcpkg, which conflicts with the default jemalloc in system apt purge libjemalloc-dev libjemalloc2 librust-jemalloc-sys-dev #make sure jdk8 is used. New version of jdk is not supported apt install -y openjdk-8-jdk apt install -y default-jdk export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH #manually install tzdata to avoid the interactive timezone config ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata dpkg --configure -a #setup proxy on necessary #export http_proxy=xxxx #export https_proxy=xxxx #clone gluten git clone https://github.com/oap-project/gluten.git cd gluten/ #config maven proxy #mkdir ~/.m2/ #vim ~/.m2/settings.xml # the script download velox &amp; arrow and compile all dependency library automatically # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # It's suggested to build using static link, enabled by `--enable_vcpkg=ON` # For developer, it's suggested to enable Debug info, by --build_type=RelWithDebInfo. Note RelWithDebInfo uses -o2, release uses -o3 ./dev/buildbundle-veloxbe.sh --enable_vcpkg=ON --build_type=RelWithDebInfo . ",
    "url": "/archives/v1.3.0/developers/docker-ubuntu/",
    
    "relUrl": "/archives/v1.3.0/developers/docker-ubuntu/"
  },"418": {
    "doc": "Developers",
    "title": "Gluten Developers",
    "content": "This document provides a developer overview of the project and covers the following topics: . ",
    "url": "/archives/v1.3.0/developers#gluten-developers",
    
    "relUrl": "/archives/v1.3.0/developers#gluten-developers"
  },"419": {
    "doc": "Developers",
    "title": "Developers",
    "content": " ",
    "url": "/archives/v1.3.0/developers",
    
    "relUrl": "/archives/v1.3.0/developers"
  },"420": {
    "doc": "Velox Function Development",
    "title": "Developer Guide for Implementing Spark Built-in SQL Functions in Velox",
    "content": "In velox, two folders prestosql &amp; sparksql are holding most sql functions, respective for presto and spark. Gluten will ask velox to firstly register prestosql functions, then sparksql functions. So if prestosql and sparksql share same signature for a function, the sparksql function will overwrite the corresponding prestosql function. If the required function is lacking in both folders (exceptions are some common functions defined outside, like cast), we need to implement the missing function in sparksql folder. It is possible that a prestosql function has some semantic difference with the corresponding spark function, even though they share the same name and function signature. If so, we also need to do an implementation in sparksql folder, generally based on the original impl. for prestosql. There are a few spark functions that can behave differently for some special cases, depending on ANSI on or off. Currently, gluten does NOT support ANSI mode. So only ANSI off needs to be considered in implementing spark built-in functions in velox. Take BitwiseAndFunction as example: . template &lt;typename T&gt; struct BitwiseAndFunction { template &lt;typename TInput&gt; // For void return type, it indicates null result will never be obtained for non-null input. // For bool return type, it indicates null result can be obtained for non-null input (false for null). FOLLY_ALWAYS_INLINE void call(TInput&amp; result, TInput a, TInput b) { result = a &amp; b; } }; . It is templated, as well as the call function, to allow multiple types. In the above impl., the result will be null for null input. Please use callNullable if you need different behavior for null input, e.g., get a non-null result for null input. Also see callNullFree in velox document. It is used for fast evaluation in the case that any input has null. The below code will register the implemented function for all kinds of integer types. The specified name bitwise_and will be actually used in calling this function. registerBinaryIntegral&lt;BitwiseAndFunction&gt;({prefix + \"bitwise_and\"}); . Functions for complex types have similar implementations. See ArrayAverageFunction in velox/functions/prestosql/ArrayFunctions.h. Reference: . Velox’s official developer guide: . | velox/docs/develop/scalar-functions.rst | velox/examples/SimpleFunctions.cpp | . ",
    "url": "/archives/v1.3.0/developers/velox-function-dev/#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox",
    
    "relUrl": "/archives/v1.3.0/developers/velox-function-dev/#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox"
  },"421": {
    "doc": "Velox Function Development",
    "title": "Velox Function Development",
    "content": " ",
    "url": "/archives/v1.3.0/developers/velox-function-dev/",
    
    "relUrl": "/archives/v1.3.0/developers/velox-function-dev/"
  },"422": {
    "doc": "Getting start with ClickHouse Backend",
    "title": "ClickHouse Backend",
    "content": "ClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP), which supports best in the industry query performance, while significantly reducing storage requirements through its innovative use of columnar storage and compression. We port ClickHouse ( based on version 23.1 ) as a library, called ‘libch.so’, and Gluten loads this library through JNI as the native engine. In this way, we don’t need to deploy a standalone ClickHouse Cluster, Spark uses Gluten as SparkPlugin to read and write ClickHouse MergeTree data. Architecture . The architecture of the ClickHouse backend is shown below: . | On Spark driver, Spark uses Gluten SparkPlugin to transform the physical plan to the Substrait plan, and then pass the Substrait plan to ClickHouse backend through JNI call on executors. | Based on Spark DataSource V2 interface, implementing a ClickHouse Catalog to support operating the ClickHouse tables, and then using Delta to save some metadata about ClickHouse like the MergeTree parts information, and also provide ACID transactions. | When querying from a ClickHouse table, it will fetch MergeTree parts information from Delta metadata and assign these parts into Spark partitions according to some strategies. | When writing data into a ClickHouse table, it will use ClickHouse library to write MergeTree parts data and collect these MergeTree parts information after writing successfully, and then save these MergeTree parts information into Delta metadata. ( The feature of writing MergeTree parts is coming soon. ) | On Spark executors, each executor will load the ‘libch.so’ through JNI when starting, and then call the operators according to the Substrait plan which is passed from Spark Driver, like reading data from the MergeTree parts, writing the MergeTree parts, filtering data, aggregating data and so on. | Currently, the ClickHouse backend only supports reading the MergeTree parts from local storage, it needs to use a high-performance shared file system to share a root bucket on every node of the cluster from the object storage, like JuiceFS. | . Development environment setup . In general, we use IDEA for Gluten development and CLion for ClickHouse backend development on Ubuntu 20. Prerequisites . Install the software required for compilation, run sudo ./ep/build-clickhouse/src/install_ubuntu.sh. Under the hood, it will install the following software: . | Clang 16.0 | cmake 3.20 or higher version | ninja-build 1.8.2 | . You can also refer to How-to-Build-ClickHouse-on-Linux. You need to install the following software manually: . | Java 8 | Maven 3.6.3 or higher version | Spark 3.2.2 or Spark 3.3.1 | . Then, get Gluten code: . git clone https://github.com/apache/incubator-gluten.git . Setup ClickHouse backend development environment . If you don’t care about development environment, you can skip this part. Otherwise, do: . | clone Kyligence/ClickHouse repo cd /to/some/place/ git clone --recursive --shallow-submodules -b clickhouse_backend https://github.com/Kyligence/ClickHouse.git . | Configure cpp-ch ${GLUTEN_SOURCE}/cpp-ch can be treated as an add-on of Kyligence/Clickhouse . First, initialize some configuration for this add-on: . export GLUTEN_SOURCE=/path/to/gluten export CH_SOURCE_DIR=/path/to/ClickHouse cmake -G Ninja -S ${GLUTEN_SOURCE}/cpp-ch -B ${GLUTEN_SOURCE}/cpp-ch/build_ch -DCH_SOURCE_DIR=${CH_SOURCE_DIR} \"-DCMAKE_C_COMPILER=$(command -v clang-16)\" \"-DCMAKE_CXX_COMPILER=$(command -v clang++-16)\" \"-DCMAKE_BUILD_TYPE=RelWithDebInfo\" . Next, you need to compile Kyligence/Clickhouse. There are two options: . | (Option 1) Use CLion . | Open ClickHouse repo | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; Toolchains, and then choose Bundled CMake, clang-16 as C Compiler, clang++-16 as C++ Compiler: . | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; CMake: . And then add these options into CMake options: . -DENABLE_PROTOBUF=ON -DENABLE_TESTS=OFF -DENABLE_JEMALLOC=ON -DENABLE_MULTITARGET_CODE=ON -DENABLE_EXTERN_LOCAL_ENGINE=ON . | Build ‘ch’ target on ClickHouse Project with Debug mode or Release mode: . If it builds with Release mode successfully, there is a library file called ‘libch.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-release/utils/extern-local-engine/’. If it builds with Debug mode successfully, there is a library file called ‘libchd.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-debug/utils/extern-local-engine/’. | . | (Option 2) Use command line cmake --build ${GLUTEN_SOURCE}/cpp-ch/build_ch --target build_ch . If it builds successfully, there is a library file called ‘libch.so’ in path ‘${GLUTEN_SOURCE}/cpp-ch/build/utils/extern-local-engine/’. | . Directly Compile ClickHouse backend . In case you don’t want a develop environment, you can use the following command to compile ClickHouse backend directly: . git clone https://github.com/apache/incubator-gluten.git cd incubator-gluten bash ./ep/build-clickhouse/src/build_clickhouse.sh . This will download Clickhouse for you and build everything. The target file is /path/to/gluten/cpp-ch/build/utils/extern-local-engine/libch.so. Compile Gluten . The prerequisites are the same as the one mentioned above. Compile Gluten with ClickHouse backend through maven: . | for Spark 3.2.2 | . git clone https://github.com/apache/incubator-gluten.git cd incubator-gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.2 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.2-jar-with-dependencies.jar . | for Spark 3.3.1 | . git clone https://github.com/apache/incubator-gluten.git cd incubator-gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.3 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.3-jar-with-dependencies.jar . Gluten in local Spark Thrift Server . Prepare working directory . | for Spark 3.2.2 | . tar zxf spark-3.2.2-bin-hadoop2.7.tgz cd spark-3.2.2-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.0.1.jar and delta-storage-2.0.1.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.0.1/delta-core_2.12-2.0.1.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.0.1/delta-storage-2.0.1.jar -P ./jars cp gluten-XXXXX-spark-3.2-jar-with-dependencies.jar jars/ . | for Spark 3.3.1 | . tar zxf spark-3.3.1-bin-hadoop2.7.tgz cd spark-3.3.1-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.2.0.jar and delta-storage-2.2.0.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar -P ./jars cp gluten-XXXXX-spark-3.3-jar-with-dependencies.jar jars/ . Query local data . Start Spark Thriftserver on local . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master local[3] \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 #connect to Spark Thriftserver by beeline bin/beeline -u jdbc:hive2://localhost:10000/ -n root . Query local MergeTree files . | Prepare data | . Currently, the feature of writing ClickHouse MergeTree parts by Spark is developing, so you need to use command ‘clickhouse-local’ to generate MergeTree parts data manually. We provide a python script to call the command ‘clickhouse-local’ to convert parquet data to MergeTree parts: . #install ClickHouse community version sudo apt-get install -y apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754 echo \"deb https://packages.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt install -y --allow-downgrades clickhouse-server=22.5.1.2079 clickhouse-client=22.5.1.2079 clickhouse-common-static=22.5.1.2079 #generate MergeTree parts mkdir -p /path_clickhouse_database/table_path/ python3 /path_to_clickhouse_backend_src/utils/local-engine/tool/parquet_to_mergetree.py --path=/tmp --source=/path_to_parquet_data/tpch-data-sf100/lineitem --dst=/path_clickhouse_database/table_path/lineitem . This python script will convert one parquet data file to one MergeTree parts. | Create a TPC-H lineitem table using ClickHouse DataSource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION '/path_clickhouse_database/table_path/lineitem'; . | TPC-H Q6 test | . SELECT sum(l_extendedprice * l_discount) AS revenue FROM lineitem WHERE l_shipdate &gt;= date'1994-01-01' AND l_shipdate &lt; date'1994-01-01' + interval 1 year AND l_discount BETWEEN 0.06 - 0.01 AND 0.06 + 0.01 AND l_quantity &lt; 24; . | Result . The DAG is shown on Spark UI as below: . | . Query local Parquet files . You can query local parquet files directly. -- query on a single file select * from parquet.`/your_data_root_dir/1.parquet`; -- query on a directly which has multiple files select * from parquet.`/your_data_roo_dir/`; . You can also create a TEMPORARY VIEW for parquet files. create or replace temporary view your_table_name using org.apache.spark.sql.parquet options( path \"/your_data_root_dir/\" ) . Query Parquet files in S3 . If you have parquet files in S3(either AWS S3 or S3 compatible storages like MINIO), you can query them directly. You need to add these additional configs to spark: . --config spark.hadoop.fs.s3a.endpoint=S3_ENDPOINT --config spark.hadoop.fs.s3a.path.style.access=true --config spark.hadoop.fs.s3a.access.key=YOUR_ACCESS_KEY --config spark.hadoop.fs.s3a.secret.key=YOUR_SECRET_KEY . where S3_ENDPOINT must follow the format of https://s3.region-code.amazonaws.com, e.g. https://s3.us-east-1.amazonaws.com (or `http://hostname:39090 for MINIO) . When you query the parquet files in S3, you need to add the prefix s3a:// to the path, e.g. s3a://your_bucket_name/path_to_your_parquet. Additionally, you can add these configs to enable local caching of S3 data. Each spark executor will have its own cache. Cache stealing between executors is not supported yet. --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.enabled=true --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.cache_path=/executor_local_folder_for_cache . Use beeline to execute queries . After start a spark thriftserver, we can use the beeline to connect to this server. # run a file /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -f &lt;your_sql_file&gt; # run a query /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -e '&lt;your_sql&gt;' # enter a interactive mode /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 . Query Hive tables in HDFS . Suppose that you have set up hive and hdfs, you can query the data on hive directly. | Copy hive-site.xml into /path_to_spark/conf/ | Copy hdfs-site.xml into /path_to_spark/conf/, and edit spark-env.sh | . # add this line into spark-env.sh export HADOOP_CONF_DIR=/path_to_spark/conf . | Start spark thriftserver with hdfs configurations | . hdfs_conf_file=/your_local_path/hdfs-site.xml cd spark-3.2.2-bin-hadoop2.7 # add a new option: spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf ./sbin/start-thriftserver.sh \\ --master local[3] \\ --files $hdfs_conf_file \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=./hdfs-site.xml . For example, you have a table demo_database.demo_table on the hive, you can run queries as below. select * from demo_database.demo_talbe; . Gluten in YARN cluster . We can to run a Spark SQL task by gluten on a yarn cluster as following . #!/bin/bash # The file contains the sql you want to run sql_file=/path/to/spark/sql/file export SPARK_HOME=/path/to/spark/home spark_cmd=$SPARK_HOME/bin/spark-sql # Define the path to libch.so ch_lib=/path/to/libch.so export LD_PRELOAD=$ch_lib # copy gluten jar file to $SPARK_HOME/jar gluten_jar=/path/to/gluten/jar/file cp $gluten_jar $SPARK_HOME/jar batchsize=20480 hdfs_conf=/path/to/hdfs-site.xml $spark_cmd \\ --name gluten_on_yarn --master yarn \\ --deploy-mode client \\ --files $ch_lib \\ --executor-cores 1 \\ --num-executors 2 \\ --executor-memory 10g \\ --conf spark.default.parallelism=4 \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=7g \\ --conf spark.driver.maxResultSize=2g \\ --conf spark.sql.autoBroadcastJoinThreshold=-1 \\ --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} \\ --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} \\ --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} \\ --conf spark.sql.broadcastTimeout=4800 \\ --conf spark.task.maxFailures=1 \\ --conf spark.excludeOnFailure.enabled=false \\ --conf spark.driver.maxResultSize=4g \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.dynamicAllocation.executorIdleTimeout=0s \\ --conf spark.sql.shuffle.partitions=112 \\ --conf spark.sql.sources.useV1SourceList=avro \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.gluten.sql.columnar.columnartorow=true \\ --conf spark.gluten.sql.columnar.loadnative=true \\ --conf spark.gluten.sql.columnar.libpath=$ch_lib \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=$hdfs_conf \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.logger.level=debug \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.executorEnv.LD_PRELOAD=$LD_PRELOAD \\ --conf spark.hadoop.input.connect.timeout=600000 \\ --conf spark.hadoop.input.read.timeout=600000 \\ --conf spark.hadoop.input.write.timeout=600000 \\ --conf spark.hadoop.dfs.client.log.severity=\"DEBUG2\" \\ --files $ch_lib \\ -f $sql_file . We also can use spark-submit to run a task. Benchmark with TPC-H 100 Q6 on Gluten with ClickHouse backend . This benchmark is tested on AWS EC2 cluster, there are 7 EC2 instances: . | Node Role | EC2 Type | Instances Count | Resources | AMI | . | Master | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . | Worker | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . Deploy on Cloud . | Tested on Spark Standalone cluster, its resources are shown below: . |   | CPU cores | Memory | Instances Count | . | Spark Worker | 15 | 60G | 6 | . | Prepare jars . Refer to Deploy Spark 3.2.2 . | Deploy gluten-core-XXXXX-jar-with-dependencies.jar . | . #deploy 'gluten-core-XXXXX-jar-with-dependencies.jar' to every node, and then cp gluten-core-XXXXX-jar-with-dependencies.jar /path_to_spark/jars/ . | Deploy ClickHouse library . Deploy ClickHouse library ‘libch.so’ to every worker node. | . Deploy JuiceFS . | JuiceFS uses Redis to save metadata, install redis firstly: | . wget https://download.redis.io/releases/redis-6.0.14.tar.gz sudo apt install build-essential tar -zxvf redis-6.0.14.tar.gz cd redis-6.0.14 make make install PREFIX=/home/ubuntu/redis6 cd .. rm -rf redis-6.0.14 #start redis server /home/ubuntu/redis6/bin/redis-server /home/ubuntu/redis6/redis.conf . | Use JuiceFS to format a S3 bucket and mount a volumn on every node . Please refer to The-JuiceFS-Command-Reference . | . wget https://github.com/juicedata/juicefs/releases/download/v0.17.5/juicefs-0.17.5-linux-amd64.tar.gz tar -zxvf juicefs-0.17.5-linux-amd64.tar.gz ./juicefs format --block-size 4096 --storage s3 --bucket https://s3.cn-northwest-1.amazonaws.com.cn/s3-gluten-tpch100/ --access-key \"XXXXXXXX\" --secret-key \"XXXXXXXX\" redis://:123456@master-ip:6379/1 gluten-tables #mount a volumn on every node ./juicefs mount -d --no-usage-report --no-syslog --attr-cache 7200 --entry-cache 7200 --dir-entry-cache 7200 --buffer-size 500 --prefetch 1 --open-cache 86400 --log /home/ubuntu/juicefs-logs/mount1.log --cache-dir /home/ubuntu/juicefs-cache/ --cache-size 102400 redis://:123456@master-ip:6379/1 /home/ubuntu/gluten/gluten_table #create a directory for lineitem table path mkdir -p /home/ubuntu/gluten/gluten_table/lineitem . Preparation . Please refer to Data-preparation to generate MergeTree parts data to the lineitem table path: /home/ubuntu/gluten/gluten_table/lineitem. Run Spark Thriftserver . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master spark://master-ip:7070 --deploy-mode client \\ --driver-memory 16g --driver-cores 4 \\ --total-executor-cores 90 --executor-memory 60g --executor-cores 15 \\ --conf spark.driver.memoryOverhead=8G \\ --conf spark.default.parallelism=90 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=536870912 \\ --conf spark.sql.parquet.filterPushdown=true \\ --conf spark.sql.parquet.enableVectorizedReader=true \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=42949672960 \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 . Test TPC-H Q6 with JMeter . | Create a lineitem table using clickhouse datasource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION 'file:///home/ubuntu/gluten/gluten_table/lineitem'; . | Run TPC-H Q6 test with JMeter . | Run TPC-H Q6 test 100 times in the first round; | Run TPC-H Q6 test 1000 times in the second round; | . | . Performance . The performance of Gluten + ClickHouse backend increases by about 1/3. |   | 70% | 80% | 90% | 99% | Avg | . | Spark + Parquet | 590ms | 592ms | 597ms | 609ms | 588ms | . | Spark + Gluten + ClickHouse backend | 402ms | 405ms | 409ms | 425ms | 399ms | . New CI System . https://opencicd.kyligence.com/job/Gluten/job/gluten-ci/ public read-only account：gluten/hN2xX3uQ4m . Celeborn support . Gluten with clickhouse backend supports Celeborn as remote shuffle service. Currently, the supported Celeborn versions are 0.3.x, 0.4.x and 0.5.0. Below introduction is used to enable this feature. First refer to this URL(https://github.com/apache/celeborn) to setup a celeborn cluster. When compiling the Gluten Java module, it’s required to enable celeborn profile, as follows: . mvn clean package -Pbackends-clickhouse -Pspark-3.3 -Pceleborn -DskipTests . Then add the Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . Columnar shuffle mode . We have two modes of columnar shuffle . | prefer cache | prefer spill | . Switch through the configuration spark.gluten.sql.columnar.backend.ch.shuffle.preferSpill, the default is false, enable prefer cache shuffle. In the prefer cache mode, as much memory as possible will be used to cache the shuffle data. When the memory is insufficient, spark will actively trigger the memory spill. You can also specify the threshold size through spark.gluten.sql.columnar.backend.ch.spillThreshold to Limit memory usage. The default value is 0MB, which means no limit on memory usage. ",
    "url": "/archives/v1.3.0/getting-started/clickhouse-backend/#clickhouse-backend",
    
    "relUrl": "/archives/v1.3.0/getting-started/clickhouse-backend/#clickhouse-backend"
  },"423": {
    "doc": "Getting start with ClickHouse Backend",
    "title": "Getting start with ClickHouse Backend",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/clickhouse-backend/",
    
    "relUrl": "/archives/v1.3.0/getting-started/clickhouse-backend/"
  },"424": {
    "doc": "Getting Start with Velox Backend",
    "title": "Supported Version",
    "content": "| Type | Version | . | Spark | 3.2.2, 3.3.1, 3.4.2, 3.5.1 | . | OS | Ubuntu20.04/22.04, Centos7/8 | . | jdk | openjdk8/jdk17 | . | scala | 2.12 | . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#supported-version",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#supported-version"
  },"425": {
    "doc": "Getting Start with Velox Backend",
    "title": "Prerequisite",
    "content": "Currently, Gluten+Velox backend is only tested on Ubuntu20.04/Ubuntu22.04/Centos7/Centos8. Other kinds of OS support are still in progress. The long term goal is to support several common OS and conda env deployment. Currently, the officially supported Spark versions are 3.2.2, 3.3.1, 3.4.2 and 3.5.1. We need to set up the JAVA_HOME env. Currently, Gluten supports java 8 and java 17. For x86_64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH . For aarch64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64 export PATH=$JAVA_HOME/bin:$PATH . Get gluten . ## config maven, like proxy in ~/.m2/settings.xml ## fetch gluten code git clone https://github.com/apache/incubator-gluten.git . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#prerequisite",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#prerequisite"
  },"426": {
    "doc": "Getting Start with Velox Backend",
    "title": "Build Gluten with Velox Backend",
    "content": "It’s recommended to use buildbundle-veloxbe.sh to build gluten in one script. Gluten build guide listed the parameters and their default value of build command for your reference. For x86_64 build . First time build for all supported spark versions./dev/buildbundle-veloxbe.sh . After a complete build, if only some gluten code is changed, you can use the following command to skip building velox/arrow and setting up build dependencies./dev/buildbundle-veloxbe.sh --enable_ep_cache=ON --build_arrow=OFF --run_setup_script=OFF . For aarch64 build: . export CPU_TARGET=\"aarch64\" ./dev/builddeps-veloxbe.sh . Build Velox separately . Currently, Gluten is using a forked Velox which is daily updated based on upstream Velox. Scripts under /path/to/gluten/ep/build-velox/src provide get_velox.sh and build_velox.sh to build Velox separately, you could use these scripts with custom repo/branch/location. Velox provides arrow/parquet lib. Gluten cpp module need a required VELOX_HOME parsed by –velox_home, if you specify custom ep location, make sure these variables be passed correctly. ## fetch Velox and compile ./dev/builddeps-veloxbe.sh build_velox ## compile Gluten cpp module ./dev/builddeps-veloxbe.sh build_gluten_cpp ## compile Gluten java module and create package jar cd /path/to/gluten # For spark3.2.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.2 -DskipTests # For spark3.3.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.3 -DskipTests # For spark3.4.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.4 -DskipTests # For spark3.5.x mvn clean package -Pbackends-velox -Pceleborn -Puniffle -Pspark-3.5 -DskipTests . Notes： Building Velox may fail caused by oom. You can prevent this failure by adjusting NUM_THREADS (e.g., export NUM_THREADS=4) before building Gluten/Velox. Once building successfully, the Jar file will be generated in the directory: package/target/&lt;gluten-jar&gt; for Spark 3.2.x/Spark 3.3.x/Spark 3.4.x/Spark 3.5.x. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-velox-backend"
  },"427": {
    "doc": "Getting Start with Velox Backend",
    "title": "Dependency library deployment",
    "content": "With config enable_vcpkg=ON, the dependency libraries will be built and statically linked into libvelox.so and libgluten.so, which is packed into the gluten-jar. In this way, only the gluten-jar is needed to add to spark.&lt;driver|executor&gt;.extraClassPath and spark will deploy the jar to each worker node. It’s better to build the static version using a clean docker image without any extra libraries installed. On host with some libraries like jemalloc installed, the script may crash with odd message. You may need to uninstall those libraries to get a clean host. With config enable_vcpkg=OFF, not all dependency libraries will be statically linked, instead the script will install the libraries to system then pack the dependency libraries into another jar named gluten-package-${Maven-artifact-version}.jar. Then you need to add the jar to extraClassPath and set spark.gluten.loadLibFromJar=true. Otherwise, you need to install shared dependency libraries on each worker node. You may find the libraries list from the gluten-package jar. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#dependency-library-deployment",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#dependency-library-deployment"
  },"428": {
    "doc": "Getting Start with Velox Backend",
    "title": "HDFS support",
    "content": "Hadoop hdfs support is ready via the libhdfs3 library. The libhdfs3 provides native API for Hadoop I/O without the drawbacks of JNI. It also provides advanced authentication like Kerberos based. Please note this library has several dependencies which may require extra installations on Driver and Worker node. Build with HDFS support . To build Gluten with HDFS support, below command is suggested: . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_hdfs=ON . Configuration about HDFS support . HDFS uris (hdfs://host:port) will be extracted from a valid hdfs file path to initialize hdfs client, you do not need to specify it explicitly. libhdfs3 need a configuration file and example here, this file is a bit different from hdfs-site.xml and core-site.xml. Download that example config file to local and do some needed modifications to support HA or else, then set env variable like below to use it, or upload it to HDFS to use, more details here. // Spark local mode export LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode --conf spark.executorEnv.LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode and upload hdfs config file cp /path/to/hdfs-client.xml hdfs-client.xml --files hdfs-client.xml . One typical deployment on Spark/HDFS cluster is to enable short-circuit reading. Short-circuit reads provide a substantial performance boost to many applications. By default libhdfs3 does not set the default hdfs domain socket path to support HDFS short-circuit read. If this feature is required in HDFS setup, users may need to setup the domain socket path correctly by patching the libhdfs3 source code or by setting the correct config environment. In Gluten the short-circuit domain socket path is set to “/var/lib/hadoop-hdfs/dn_socket” in build_velox.sh So we need to make sure the folder existed and user has write access as below script. sudo mkdir -p /var/lib/hadoop-hdfs/ sudo chown &lt;sparkuser&gt;:&lt;sparkuser&gt; /var/lib/hadoop-hdfs/ . You also need to add configuration to the “hdfs-site.xml” as below: . &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt; . Kerberos support . Here are two steps to enable kerberos. | Make sure the hdfs-client.xml contains | . &lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;/property&gt; . | Specify the environment variable KRB5CCNAME and upload the kerberos ticket cache file | . --conf spark.executorEnv.KRB5CCNAME=krb5cc_0000 --files /tmp/krb5cc_0000 . The ticket cache file can be found by klist. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#hdfs-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#hdfs-support"
  },"429": {
    "doc": "Getting Start with Velox Backend",
    "title": "Azure Blob File System (ABFS) support",
    "content": "Velox supports ABFS with the open source Azure SDK for C++ and Gluten uses the Velox ABFS connector to connect with ABFS. The build option for ABFS (enable_abfs) must be set to enable this feature as listed below. cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_abfs=ON . Please refer Velox ABFS part for more detailed configurations. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#azure-blob-file-system-abfs-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#azure-blob-file-system-abfs-support"
  },"430": {
    "doc": "Getting Start with Velox Backend",
    "title": "AWS S3 support",
    "content": "Velox supports S3 with the open source AWS C++ SDK and Gluten uses Velox S3 connector to connect with S3. A new build option for S3(enable_s3) is added. Below command is used to enable this feature . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_s3=ON . Currently there are several ways to asscess S3 in Spark. Please refer Velox S3 part for more detailed configurations . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#aws-s3-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#aws-s3-support"
  },"431": {
    "doc": "Getting Start with Velox Backend",
    "title": "Celeborn support",
    "content": "Gluten with velox backend supports Celeborn as remote shuffle service. Currently, the supported Celeborn versions are 0.3.x, 0.4.x and 0.5.0. Below introduction is used to enable this feature. First refer to this URL(https://github.com/apache/celeborn) to setup a celeborn cluster. When compiling the Gluten Java module, it’s required to enable celeborn profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Pceleborn -DskipTests . Then add the Gluten and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar | Gluten: gluten-velox-bundle-spark3.x_2.12-xx_xx_xx-SNAPSHOT.jar, gluten-celeborn-package-xx-SNAPSHOT.jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#celeborn-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#celeborn-support"
  },"432": {
    "doc": "Getting Start with Velox Backend",
    "title": "Uniffle support",
    "content": "Uniffle with velox backend supports Uniffle as remote shuffle service. Currently, the supported Uniffle versions are 0.8.0. First refer to this URL(https://uniffle.apache.org/docs/intro) to get start with uniffle. When compiling the Gluten Java module, it’s required to enable uniffle profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Puniffle -DskipTests . Then add the Uniffle and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Uniffle: rss-client-spark3-shaded-[uniffleVersion].jar | Gluten: gluten-uniffle-velox-xxx-SNAPSHOT-3.x.jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.uniffle.UniffleShuffleManager # uniffle coordinator address spark.rss.coordinator.quorum ip:port # Support for Spark AQE spark.sql.adaptive.localShuffleReader.enabled false spark.shuffle.service.enabled false # Uniffle support mutilple storage types, you can choose one of them. # Such as MEMORY,LOCALFILE,MEMORY_LOCALFILE,HDFS,MEMORY_HDFS,LOCALFILE_HDFS,MEMORY_LOCALFILE_HDFS spark.rss.storage.type LOCALFILE_HDFS # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/incubator-uniffle/tree/master/patch/spark) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#uniffle-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#uniffle-support"
  },"433": {
    "doc": "Getting Start with Velox Backend",
    "title": "DeltaLake Support",
    "content": "Gluten with velox backend supports DeltaLake table. How to use . First of all, compile gluten-delta module by a delta profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Pdelta -DskipTests . Then, put the additional gluten-delta-XX-SNAPSHOT.jar to the class path (usually it’s $SPARK_HOME/jars). The gluten-delta jar is in gluten-delta/target directory. After the two steps, you can query delta table by gluten/velox without scan’s fallback. Gluten with velox backends also support the column mapping of delta tables. About column mapping, see more here. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#deltalake-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#deltalake-support"
  },"434": {
    "doc": "Getting Start with Velox Backend",
    "title": "Iceberg Support",
    "content": "Gluten with velox backend supports Iceberg table. Currently, only reading COW (Copy-On-Write) tables is supported. How to use . First of all, compile gluten-iceberg module by a iceberg profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Piceberg -DskipTests . Once built successfully, iceberg features will be included in gluten-velox-bundle-X jar. Then you can query iceberg table by gluten/velox without scan’s fallback. After the two steps, you can query iceberg table by gluten/velox without scan’s fallback. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#iceberg-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#iceberg-support"
  },"435": {
    "doc": "Getting Start with Velox Backend",
    "title": "Coverage",
    "content": "Spark3.3 has 387 functions in total. ~240 are commonly used. To get the support status of all Spark built-in functions, please refer to Velox Backend’s Supported Operators &amp; Functions. Velox doesn’t support ANSI mode), so as Gluten. Once ANSI mode is enabled in Spark config, Gluten will fallback to Vanilla Spark. To identify what can be offloaded in a query and detailed fallback reasons, user can follow below steps to retrieve corresponding logs. 1) Enable Gluten by proper [configuration](https://github.com/apache/incubator-gluten/blob/main/docs/Configuration.md). 2) Disable Spark AQE to trigger plan validation in Gluten spark.sql.adaptive.enabled = false 3) Check physical plan sparkSession.sql(\"your_sql\").explain() . With above steps, you will get a physical plan output like: . == Physical Plan == -Execute InsertIntoHiveTable (7) +- Coalesce (6) +- VeloxColumnarToRowExec (5) +- ^ ProjectExecTransformer (3) +- GlutenRowToArrowColumnar (2) +- Scan hive default.extracted_db_pins (1) . GlutenRowToArrowColumnar/VeloxColumnarToRowExec indicates there is a fallback operator before or after it. And you may find fallback reason like below in logs. native validation failed due to: in ProjectRel, Scalar function name not registered: get_struct_field, called with arguments: (ROW&lt;col_0:INTEGER,col_1:BIGINT,col_2:BIGINT&gt;, INTEGER). In the above, the symbol ^ indicates a plan is offloaded to Velox in a stage. In Spark DAG, all such pipelined plans (consecutive plans marked with ^) are plotted inside an umbrella node named WholeStageCodegenTransformer (It’s not codegen node. The naming is just for making it well plotted like Spark Whole Stage Codegen). ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#coverage",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#coverage"
  },"436": {
    "doc": "Getting Start with Velox Backend",
    "title": "Spill (Experimental)",
    "content": "Velox backend supports spilling-to-disk. Using the following configuration options to customize spilling: . | Name | Default Value | Description | . | spark.gluten.sql.columnar.backend.velox.spillStrategy | auto | none: Disable spill on Velox backend; auto: Let Spark memory manager manage Velox’s spilling | . | spark.gluten.sql.columnar.backend.velox.spillFileSystem | local | The filesystem used to store spill data. local: The local file system. heap-over-local: Write files to JVM heap if having extra heap space. Otherwise write to local file system. | . | spark.gluten.sql.columnar.backend.velox.aggregationSpillEnabled | true | Whether spill is enabled on aggregations | . | spark.gluten.sql.columnar.backend.velox.joinSpillEnabled | true | Whether spill is enabled on joins | . | spark.gluten.sql.columnar.backend.velox.orderBySpillEnabled | true | Whether spill is enabled on sorts | . | spark.gluten.sql.columnar.backend.velox.maxSpillLevel | 4 | The max allowed spilling level with zero being the initial spilling level | . | spark.gluten.sql.columnar.backend.velox.maxSpillFileSize | 1GB | The max allowed spill file size. If it is zero, then there is no limit | . | spark.gluten.sql.columnar.backend.velox.spillStartPartitionBit | 29 | The start partition bit which is used with ‘spillPartitionBits’ together to calculate the spilling partition number | . | spark.gluten.sql.columnar.backend.velox.spillPartitionBits | 2 | The number of bits used to calculate the spilling partition number. The number of spilling partitions will be power of two | . | spark.gluten.sql.columnar.backend.velox.spillableReservationGrowthPct | 25 | The spillable memory reservation growth percentage of the previous memory reservation size | . | spark.gluten.sql.columnar.backend.velox.spillThreadNum | 0 | (Experimental) The thread num of a dedicated thread pool to do spill | . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#spill-experimental",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#spill-experimental"
  },"437": {
    "doc": "Getting Start with Velox Backend",
    "title": "Velox User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF)",
    "content": "Please check the VeloxNativeUDF.md for more detailed usage and configurations. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf"
  },"438": {
    "doc": "Getting Start with Velox Backend",
    "title": "High-Bandwidth Memory (HBM) support",
    "content": "Gluten supports allocating memory on HBM. This feature is optional and is disabled by default. It is implemented on top of Memkind library. You can refer to memkind’s readme for more details. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#high-bandwidth-memory-hbm-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#high-bandwidth-memory-hbm-support"
  },"439": {
    "doc": "Getting Start with Velox Backend",
    "title": "Build Gluten with HBM",
    "content": "Gluten will internally build and link to a specific version of Memkind library and hwloc. Other dependencies should be installed on Driver and Worker node first: . sudo apt install -y autoconf automake g++ libnuma-dev libtool numactl unzip libdaxctl-dev . After the set-up, you can now build Gluten with HBM. Below command is used to enable this feature . cd /path/to/gluten ## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1./dev/buildbundle-veloxbe.sh --enable_hbm=ON . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-hbm",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-hbm"
  },"440": {
    "doc": "Getting Start with Velox Backend",
    "title": "Configure and enable HBM in Spark Application",
    "content": "At runtime, MEMKIND_HBW_NODES enviroment variable is detected for configuring HBM NUMA nodes. For the explaination to this variable, please refer to memkind’s manual page. This can be set for all executors through spark conf, e.g. --conf spark.executorEnv.MEMKIND_HBW_NODES=8-15. Note that memory allocation fallback is also supported and cannot be turned off. If HBM is unavailable or fills up, the allocator will use default(DDR) memory. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#configure-and-enable-hbm-in-spark-application",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#configure-and-enable-hbm-in-spark-application"
  },"441": {
    "doc": "Getting Start with Velox Backend",
    "title": "Intel® QuickAssist Technology (QAT) support",
    "content": "Gluten supports using Intel® QuickAssist Technology (QAT) for data compression during Spark Shuffle. It benefits from QAT Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on QAT driver library and QATzip library. Please manually download QAT driver for your system, and follow its README to build and install on all Driver and Worker node: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#intel-quickassist-technology-qat-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#intel-quickassist-technology-qat-support"
  },"442": {
    "doc": "Getting Start with Velox Backend",
    "title": "Software Requirements",
    "content": ". | Download QAT driver for your system, and follow its README to build and install on all Driver and Worker nodes: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. | Below compression libraries need to be installed on all Driver and Worker nodes: . | Zlib* library of version 1.2.7 or higher | ZSTD* library of version 1.5.4 or higher | LZ4* library | . | . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#software-requirements",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#software-requirements"
  },"443": {
    "doc": "Getting Start with Velox Backend",
    "title": "Build Gluten with QAT",
    "content": ". | Setup ICP_ROOT environment variable to the directory where QAT driver is extracted. This environment variable is required during building Gluten and running Spark applications. It’s recommended to put it in .bashrc on Driver and Worker nodes. | . echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc source ~/.bashrc # Also set for root if running as non-root user sudo su - echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc exit . | This step is required if your application is running as Non-root user. The users must be added to the ‘qat’ group after QAT drvier is installed. And change the amount of max locked memory for the username that is included in the group name. This can be done by specifying the limit in /etc/security/limits.conf. | . sudo su - usermod -aG qat username # need relogin to take effect # To set 500MB add a line like this in /etc/security/limits.conf echo \"@qat - memlock 500000\" &gt;&gt; /etc/security/limits.conf exit . | Enable huge page. This step is required to execute each time after system reboot. We recommend using systemctl to manage at system startup. You change the values for “max_huge_pages” and “max_huge_pages_per_process” to make sure there are enough resources for your workload. As for Spark applications, one process matches one executor. Within the executor, every task is allocated a maximum of 5 huge pages. | . sudo su - cat &lt;&lt; EOF &gt; /usr/local/bin/qat_startup.sh #!/bin/bash echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages rmmod usdm_drv insmod $ICP_ROOT/build/usdm_drv.ko max_huge_pages=1024 max_huge_pages_per_process=32 EOF chmod +x /usr/local/bin/qat_startup.sh cat &lt;&lt; EOF &gt; /etc/systemd/system/qat_startup.service [Unit] Description=Configure QAT [Service] ExecStart=/usr/local/bin/qat_startup.sh [Install] WantedBy=multi-user.target EOF systemctl enable qat_startup.service systemctl start qat_startup.service # setup immediately systemctl status qat_startup.service exit . | After the setup, you are now ready to build Gluten with QAT. Use the command below to enable this feature: | . cd /path/to/gluten ## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1./dev/buildbundle-veloxbe.sh --enable_qat=ON . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-qat",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-qat"
  },"444": {
    "doc": "Getting Start with Velox Backend",
    "title": "Enable QAT with Gzip/Zstd for shuffle compression",
    "content": ". | To offload shuffle compression into QAT, first make sure you have the right QAT configuration file at /etc/4xxx_devX.conf. We provide a example configuration file. This configuration sets up to 4 processes that can bind to 1 QAT, and each process can use up to 16 QAT DC instances. | . ## run as root ## Overwrite QAT configuration file. cd /etc for i in {0..7}; do echo \"4xxx_dev$i.conf\"; done | xargs -i cp -f /path/to/gluten/docs/qat/4x16.conf {} ## Restart QAT after updating configuration files. adf_ctl restart . | Check QAT status and make sure the status is up | . adf_ctl status . The output should be like: . Checking status of all devices. There is 8 QAT acceleration device(s) in the system: qat_dev0 - type: 4xxx, inst_id: 0, node_id: 0, bsf: 0000:6b:00.0, #accel: 1 #engines: 9 state: up qat_dev1 - type: 4xxx, inst_id: 1, node_id: 1, bsf: 0000:70:00.0, #accel: 1 #engines: 9 state: up qat_dev2 - type: 4xxx, inst_id: 2, node_id: 2, bsf: 0000:75:00.0, #accel: 1 #engines: 9 state: up qat_dev3 - type: 4xxx, inst_id: 3, node_id: 3, bsf: 0000:7a:00.0, #accel: 1 #engines: 9 state: up qat_dev4 - type: 4xxx, inst_id: 4, node_id: 4, bsf: 0000:e8:00.0, #accel: 1 #engines: 9 state: up qat_dev5 - type: 4xxx, inst_id: 5, node_id: 5, bsf: 0000:ed:00.0, #accel: 1 #engines: 9 state: up qat_dev6 - type: 4xxx, inst_id: 6, node_id: 6, bsf: 0000:f2:00.0, #accel: 1 #engines: 9 state: up qat_dev7 - type: 4xxx, inst_id: 7, node_id: 7, bsf: 0000:f7:00.0, #accel: 1 #engines: 9 state: up . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip # Valid options are gzip and zstd --conf spark.gluten.sql.columnar.shuffle.codecBackend=qat . | You can use below command to check whether QAT is working normally at run-time. The value of fw_counters should continue to increase during shuffle. | . while :; do cat /sys/kernel/debug/qat_4xxx_0000:6b:00.0/fw_counters; sleep 1; done . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#enable-qat-with-gzipzstd-for-shuffle-compression",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#enable-qat-with-gzipzstd-for-shuffle-compression"
  },"445": {
    "doc": "Getting Start with Velox Backend",
    "title": "QAT driver references",
    "content": "Documentation . README Text Files (README_QAT20.L.1.0.0-00021.txt) . Release Notes . Check out the Intel® QuickAssist Technology Software for Linux* - Release Notes for the latest changes in this release. Getting Started Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Getting Started Guide for detailed installation instructions. Programmer’s Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Programmer’s Guide for software usage guidelines. For more Intel® QuickAssist Technology resources go to Intel® QuickAssist Technology (Intel® QAT) . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#qat-driver-references",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#qat-driver-references"
  },"446": {
    "doc": "Getting Start with Velox Backend",
    "title": "Intel® In-memory Analytics Accelerator (IAA/IAX) support",
    "content": "Similar to Intel® QAT, Gluten supports using Intel® In-memory Analytics Accelerator (IAA, also called IAX) for data compression during Spark Shuffle. It benefits from IAA Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on Intel® QPL. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#intel-in-memory-analytics-accelerator-iaaiax-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#intel-in-memory-analytics-accelerator-iaaiax-support"
  },"447": {
    "doc": "Getting Start with Velox Backend",
    "title": "Build Gluten with IAA",
    "content": "Gluten will internally build and link to a specific version of QPL library, but extra environment setup is still required. Please refer to QPL Installation Guide to install dependencies and configure accelerators. This step is required if your application is running as Non-root user. Create a group for the users who have privilege to use IAA, and grant group iaa read/write access to the IAA Work-Queues. sudo groupadd iaa sudo usermod -aG iaa username # need to relogin sudo chgrp -R iaa /dev/iax sudo chmod -R g+rw /dev/iax . After the set-up, you can now build Gluten with QAT. Below command is used to enable this feature . cd /path/to/gluten ## The script builds four jars for spark 3.2.2, 3.3.1, 3.4.2 and 3.5.1./dev/buildbundle-veloxbe.sh --enable_iaa=ON . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-iaa",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#build-gluten-with-iaa"
  },"448": {
    "doc": "Getting Start with Velox Backend",
    "title": "Enable IAA with Gzip Compression for shuffle compression",
    "content": ". | To enable QAT at run-time, first make sure you have configured the IAA Work-Queues correctly, and the file permissions of /dev/iax/wqX.0 are correct. | . sudo ls -l /dev/iax . The output should be like: . total 0 crw-rw---- 1 root iaa 509, 0 Apr 5 18:54 wq1.0 crw-rw---- 1 root iaa 509, 5 Apr 5 18:54 wq11.0 crw-rw---- 1 root iaa 509, 6 Apr 5 18:54 wq13.0 crw-rw---- 1 root iaa 509, 7 Apr 5 18:54 wq15.0 crw-rw---- 1 root iaa 509, 1 Apr 5 18:54 wq3.0 crw-rw---- 1 root iaa 509, 2 Apr 5 18:54 wq5.0 crw-rw---- 1 root iaa 509, 3 Apr 5 18:54 wq7.0 crw-rw---- 1 root iaa 509, 4 Apr 5 18:54 wq9.0 . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip --conf spark.gluten.sql.columnar.shuffle.codecBackend=iaa . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#enable-iaa-with-gzip-compression-for-shuffle-compression",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#enable-iaa-with-gzip-compression-for-shuffle-compression"
  },"449": {
    "doc": "Getting Start with Velox Backend",
    "title": "IAA references",
    "content": "Intel® IAA Enabling Guide . Check out the Intel® In-Memory Analytics Accelerator (Intel® IAA) Enabling Guide . Intel® QPL Documentation . Check out the Intel® Query Processing Library (Intel® QPL) Documentation . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#iaa-references",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#iaa-references"
  },"450": {
    "doc": "Getting Start with Velox Backend",
    "title": "Test TPC-H or TPC-DS on Gluten with Velox backend",
    "content": "All TPC-H and TPC-DS queries are supported in Gluten Velox backend. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend"
  },"451": {
    "doc": "Getting Start with Velox Backend",
    "title": "Data preparation",
    "content": "The data generation scripts are TPC-H dategen script and TPC-DS dategen script. The used TPC-H and TPC-DS queries are the original ones, and can be accessed from TPC-DS queries and TPC-H queries. Some other versions of TPC-DS queries are also provided, but are not recommended for testing, including: . | the modified TPC-DS queries with “Decimal-to-Double”: TPC-DS non-decimal queries (outdated). | . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#data-preparation",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#data-preparation"
  },"452": {
    "doc": "Getting Start with Velox Backend",
    "title": "Submit the Spark SQL job",
    "content": "Submit test script from spark-shell. You can find the scala code to Run TPC-H as an example. Please remember to modify the location of TPC-H files as well as TPC-H queries before you run the testing. var parquet_file_path = \"/PATH/TO/TPCH_PARQUET_PATH\" var gluten_root = \"/PATH/TO/GLUTEN\" . Below script shows an example about how to run the testing, you should modify the parameters such as executor cores, memory, offHeap size based on your environment. export GLUTEN_JAR = /PATH/TO/GLUTEN/package/target/&lt;gluten-jar&gt; cat tpch_parquet.scala | spark-shell --name tpch_powertest_velox \\ --master yarn --deploy-mode client \\ --conf spark.plugins=org.apache.gluten.GlutenPlugin \\ --conf spark.driver.extraClassPath=${GLUTEN_JAR} \\ --conf spark.executor.extraClassPath=${GLUTEN_JAR} \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --num-executors 6 \\ --executor-cores 6 \\ --driver-memory 20g \\ --executor-memory 25g \\ --conf spark.executor.memoryOverhead=5g \\ --conf spark.driver.maxResultSize=32g . Refer to Gluten configuration for more details. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#submit-the-spark-sql-job",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#submit-the-spark-sql-job"
  },"453": {
    "doc": "Getting Start with Velox Backend",
    "title": "Result",
    "content": "wholestagetransformer indicates that the offloading works. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#result",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#result"
  },"454": {
    "doc": "Getting Start with Velox Backend",
    "title": "Performance",
    "content": "Below table shows the TPC-H Q1 and Q6 Performance in a multiple-thread test (–num-executors 6 –executor-cores 6) for Velox and vanilla Spark. Both Parquet and ORC datasets are sf1024. | Query Performance (s) | Velox (ORC) | Vanilla Spark (Parquet) | Vanilla Spark (ORC) | . | TPC-H Q6 | 13.6 | 21.6 | 34.9 | . | TPC-H Q1 | 26.1 | 76.7 | 84.9 | . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#performance",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#performance"
  },"455": {
    "doc": "Getting Start with Velox Backend",
    "title": "External reference setup",
    "content": "TO ease your first-hand experience of using Gluten, we have set up an external reference cluster. If you are interested, please contact Weiting.Chen@intel.com. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#external-reference-setup",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#external-reference-setup"
  },"456": {
    "doc": "Getting Start with Velox Backend",
    "title": "Gluten UI",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#gluten-ui",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#gluten-ui"
  },"457": {
    "doc": "Getting Start with Velox Backend",
    "title": "Gluten event",
    "content": "Gluten provides two events GlutenBuildInfoEvent and GlutenPlanFallbackEvent: . | GlutenBuildInfoEvent, it contains the Gluten build information so that we are able to be aware of the environment when doing some debug. It includes Java Version, Scala Version, GCC Version, Gluten Version, Spark Version, Hadoop Version, Gluten Revision, Backend, Backend Revision, etc. | GlutenPlanFallbackEvent, it contains the fallback information for each query execution. Note, if the query execution is in AQE, then Gluten will post it for each stage. | . Developers can register SparkListener to handle these two Gluten events. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#gluten-event",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#gluten-event"
  },"458": {
    "doc": "Getting Start with Velox Backend",
    "title": "SQL tab",
    "content": "Gluten provides a tab based on Spark UI, named Gluten SQL / DataFrame . This tab contains two parts: . | The Gluten build information. | SQL/Dataframe queries fallback information. | . If you want to disable Gluten UI, add a config when submitting --conf spark.gluten.ui.enabled=false. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#sql-tab",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#sql-tab"
  },"459": {
    "doc": "Getting Start with Velox Backend",
    "title": "History server",
    "content": "Gluten UI also supports Spark history server. Add gluten-ui jar into the history server classpath, e.g., $SPARK_HOME/jars, then restart history server. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#history-server",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#history-server"
  },"460": {
    "doc": "Getting Start with Velox Backend",
    "title": "Native plan string",
    "content": "Gluten supports inject native plan string into Spark explain with formatted mode by setting --conf spark.gluten.sql.injectNativePlanStringToExplain=true. Here is an example, how Gluten show the native plan string. (9) WholeStageCodegenTransformer (2) Input [6]: [c1#0L, c2#1L, c3#2L, c1#3L, c2#4L, c3#5L] Arguments: false Native Plan: -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#native-plan-string",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#native-plan-string"
  },"461": {
    "doc": "Getting Start with Velox Backend",
    "title": "Native plan with stats",
    "content": "Gluten supports print native plan with stats to executor system output stream by setting --conf spark.gluten.sql.debug=true. Note that, the plan string with stats is task level which may cause executor log size big. Here is an example, how Gluten show the native plan string with stats. I20231121 10:19:42.348845 90094332 WholeStageResultIterator.cc:220] Native Plan with stats for: [Stage: 1 TID: 16] -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 10.58us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 queuedWallNanos sum: 2.00us, count: 1, min: 2.00us, max: 2.00us runningAddInputWallNanos sum: 626ns, count: 1, min: 626ns, max: 626ns runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 5.54us, count: 1, min: 5.54us, max: 5.54us -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 223.00us, Blocked wall time: 0ns, Peak memory: 93.12KB, Memory allocations: 15 HashBuild: Input: 10 rows (960B, 10 batches), Output: 0 rows (0B, 0 batches), Cpu time: 185.67us, Blocked wall time: 0ns, Peak memory: 68.00KB, Memory allocations: 2, Threads: 1 distinctKey0 sum: 4, count: 1, min: 4, max: 4 hashtable.capacity sum: 4, count: 1, min: 4, max: 4 hashtable.numDistinct sum: 10, count: 1, min: 10, max: 10 hashtable.numRehashes sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 4, count: 1, min: 4, max: 4 runningAddInputWallNanos sum: 1.27ms, count: 1, min: 1.27ms, max: 1.27ms runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 1.29us, count: 1, min: 1.29us, max: 1.29us H23/11/21 10:19:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 13) in 335 ms on 10.221.97.35 (executor driver) (1/10) ashProbe: Input: 9 rows (864B, 3 batches), Output: 27 rows (3.56KB, 3 batches), Cpu time: 37.33us, Blocked wall time: 0ns, Peak memory: 25.12KB, Memory allocations: 13, Threads: 1 dynamicFiltersProduced sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningAddInputWallNanos sum: 4.54us, count: 1, min: 4.54us, max: 4.54us runningFinishWallNanos sum: 83ns, count: 1, min: 83ns, max: 83ns runningGetOutputWallNanos sum: 29.08us, count: 1, min: 29.08us, max: 29.08us -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT Input: 9 rows (864B, 3 batches), Output: 9 rows (864B, 3 batches), Cpu time: 630.75us, Blocked wall time: 0ns, Peak memory: 2.44KB, Memory allocations: 63, Threads: 1, Splits: 3 dataSourceWallNanos sum: 102.00us, count: 1, min: 102.00us, max: 102.00us dynamicFiltersAccepted sum: 1, count: 1, min: 1, max: 1 flattenStringDictionaryValues sum: 0, count: 1, min: 0, max: 0 ioWaitNanos sum: 312.00us, count: 1, min: 312.00us, max: 312.00us localReadBytes sum: 0B, count: 1, min: 0B, max: 0B numLocalRead sum: 0, count: 1, min: 0, max: 0 numPrefetch sum: 0, count: 1, min: 0, max: 0 numRamRead sum: 0, count: 1, min: 0, max: 0 numStorageRead sum: 6, count: 1, min: 6, max: 6 overreadBytes sum: 0B, count: 1, min: 0B, max: 0B prefetchBytes sum: 0B, count: 1, min: 0B, max: 0B queryThreadIoLatency sum: 12, count: 1, min: 12, max: 12 ramReadBytes sum: 0B, count: 1, min: 0B, max: 0B runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 125ns, count: 1, min: 125ns, max: 125ns runningGetOutputWallNanos sum: 1.07ms, count: 1, min: 1.07ms, max: 1.07ms skippedSplitBytes sum: 0B, count: 1, min: 0B, max: 0B skippedSplits sum: 0, count: 1, min: 0, max: 0 skippedStrides sum: 0, count: 1, min: 0, max: 0 storageReadBytes sum: 3.44KB, count: 1, min: 3.44KB, max: 3.44KB totalScanTime sum: 0ns, count: 1, min: 0ns, max: 0ns -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Input: 0 rows (0B, 0 batches), Output: 10 rows (960B, 10 batches), Cpu time: 1.03ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 54.62us, count: 1, min: 54.62us, max: 54.62us runningGetOutputWallNanos sum: 1.10ms, count: 1, min: 1.10ms, max: 1.10ms . ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#native-plan-with-stats",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#native-plan-with-stats"
  },"462": {
    "doc": "Getting Start with Velox Backend",
    "title": "Gluten Implicits",
    "content": "Gluten provides a helper class to get the fallback summary from a Spark Dataset. import org.apache.spark.sql.execution.GlutenImplicits._ val df = spark.sql(\"SELECT * FROM t\") df.fallbackSummary . Note that, if AQE is enabled, but the query is not materialized, then it will re-plan the query execution with disabled AQE. It is a workaround to get the final plan, and it may cause the inconsistent results with a materialized query. However, we have no choice. ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/#gluten-implicits",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/#gluten-implicits"
  },"463": {
    "doc": "Getting Start with Velox Backend",
    "title": "Getting Start with Velox Backend",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/velox-backend/",
    
    "relUrl": "/archives/v1.3.0/getting-started/velox-backend/"
  },"464": {
    "doc": "Using ABFS with Gluten",
    "title": "Working with ABFS",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/abfs/#working-with-abfs",
    
    "relUrl": "/archives/v1.3.0/getting-started/abfs/#working-with-abfs"
  },"465": {
    "doc": "Using ABFS with Gluten",
    "title": "Configuring ABFS Access Token",
    "content": "To configure access to your storage account, replace with the name of your account. This property aligns with Spark configurations. By setting this config multiple times using different storage account names, you can access multiple ABFS accounts. spark.hadoop.fs.azure.account.key.&lt;storage-account&gt;.dfs.core.windows.net XXXXXXXXX . Other authentatication methods are not yet supported. ",
    "url": "/archives/v1.3.0/getting-started/abfs/#configuring-abfs-access-token",
    
    "relUrl": "/archives/v1.3.0/getting-started/abfs/#configuring-abfs-access-token"
  },"466": {
    "doc": "Using ABFS with Gluten",
    "title": "Local Caching support",
    "content": "Velox supports a local cache when reading data from ABFS. Please refer Velox Local Cache part for more detailed configurations. ",
    "url": "/archives/v1.3.0/getting-started/abfs/#local-caching-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/abfs/#local-caching-support"
  },"467": {
    "doc": "Using ABFS with Gluten",
    "title": "Using ABFS with Gluten",
    "content": "ABFS is an important data store for big data users. This doc discusses config details and use cases of Gluten with ABFS. To use an ABFS account as your data source, please ensure you use the listed ABFS config in your spark-defaults.conf. If you would like to authenticate with ABFS using additional auth mechanisms, please reach out using the ‘Issues’ tab. ",
    "url": "/archives/v1.3.0/getting-started/abfs/",
    
    "relUrl": "/archives/v1.3.0/getting-started/abfs/"
  },"468": {
    "doc": "Using GCS with Gluten",
    "title": "Working with GCS",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/gcs/#working-with-gcs",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#working-with-gcs"
  },"469": {
    "doc": "Using GCS with Gluten",
    "title": "Installing the gcloud CLI",
    "content": "To access GCS Objects using Gluten and Velox, first you have to [download an install the gcloud CLI] (https://cloud.google.com/sdk/docs/install). ",
    "url": "/archives/v1.3.0/getting-started/gcs/#installing-the-gcloud-cli",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#installing-the-gcloud-cli"
  },"470": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS using a user account",
    "content": "This is recommended for regular users, follow the instructions to authorize a user account. After these steps, no specific configuration is required for Gluten, since the authorization was handled entirely by the gcloud tool. ",
    "url": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-using-a-user-account",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-using-a-user-account"
  },"471": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS using a credential file",
    "content": "For workloads that need to be fully automated, manually authorizing can be problematic. For such cases it is better to use a json file with the credentials. This is described in the [instructions to configure a service account]https://cloud.google.com/sdk/docs/authorizing#service-account. Such json file with the credetials can be passed to Gluten: . spark.hadoop.fs.gs.auth.type SERVICE_ACCOUNT_JSON_KEYFILE spark.hadoop.fs.gs.auth.service.account.json.keyfile // path to the json file with the credentials. ",
    "url": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-using-a-credential-file",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-using-a-credential-file"
  },"472": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS endpoints",
    "content": "For cases when a GCS mock is used, an optional endpoint can be provided: . spark.hadoop.fs.gs.storage.root.url // url to the mock gcs service including starting with http or https . ",
    "url": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-endpoints",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-endpoints"
  },"473": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS max retry count",
    "content": "For cases when a transient server error is detected, GCS can be configured to keep retrying until a number of transient error is detected. spark.hadoop.fs.gs.http.max.retry // number of times to keep retrying unless a non-transient error is detected . ",
    "url": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-max-retry-count",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-max-retry-count"
  },"474": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS max retry time",
    "content": "For cases when a transient server error is detected, GCS can be configured to keep retrying until the retry loop exceeds a prescribed duration. spark.hadoop.fs.gs.http.max.retry-time // a string representing the time keep retring (10s, 1m, etc). ",
    "url": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-max-retry-time",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/#configuring-gcs-max-retry-time"
  },"475": {
    "doc": "Using GCS with Gluten",
    "title": "Using GCS with Gluten",
    "content": "Object stores offered by CSPs such as GCS are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use a GCS endpoint as your data source, please ensure you are using the following GCS configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to GCS with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/archives/v1.3.0/getting-started/gcs/",
    
    "relUrl": "/archives/v1.3.0/getting-started/gcs/"
  },"476": {
    "doc": "Velox Local Caching",
    "title": "Velox Local Caching",
    "content": "Velox supports a local cache when reading data from HDFS/S3/ABFS. With this feature, Velox can asynchronously cache the data on local disk when reading from remote storage and future read requests on previously cached blocks will be serviced from local cache files. To enable the local caching feature, the following configurations are required: . spark.gluten.sql.columnar.backend.velox.cacheEnabled // enable or disable velox cache, default false. spark.gluten.sql.columnar.backend.velox.memCacheSize // the total size of in-mem cache, default is 128MB. spark.gluten.sql.columnar.backend.velox.ssdCachePath // the folder to store the cache files, default is \"/tmp\". spark.gluten.sql.columnar.backend.velox.ssdCacheSize // the total size of the SSD cache, default is 128MB. Velox will do in-mem cache only if this value is 0. spark.gluten.sql.columnar.backend.velox.ssdCacheShards // the shards of the SSD cache, default is 1. spark.gluten.sql.columnar.backend.velox.ssdCacheIOThreads // the IO threads for cache promoting, default is 1. Velox will try to do \"read-ahead\" if this value is bigger than 1 spark.gluten.sql.columnar.backend.velox.ssdODirect // enable or disable O_DIRECT on cache write, default false. It’s recommended to mount SSDs to the cache path to get the best performance of local caching. Cache files will be written to “spark.gluten.sql.columnar.backend.velox.cachePath”, with UUID based suffix, e.g. “/tmp/cache.13e8ab65-3af4-46ac-8d28-ff99b2a9ec9b0”. Gluten cannot reuse older caches for now, and the old cache files are left after Spark context shutdown. ",
    "url": "/archives/v1.3.0/getting-started/localcache/",
    
    "relUrl": "/archives/v1.3.0/getting-started/localcache/"
  },"477": {
    "doc": "Using S3 with Gluten",
    "title": "Working with S3",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/s3/#working-with-s3",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#working-with-s3"
  },"478": {
    "doc": "Using S3 with Gluten",
    "title": "Configuring S3 endpoint",
    "content": "S3 provides the endpoint based method to access the files, here’s the example configuration. Users may need to modify some values based on real setup. spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider spark.hadoop.fs.s3a.access.key XXXXXXXXX spark.hadoop.fs.s3a.secret.key XXXXXXXXX spark.hadoop.fs.s3a.endpoint https://s3.us-west-1.amazonaws.com spark.hadoop.fs.s3a.connection.ssl.enabled true spark.hadoop.fs.s3a.path.style.access false . ",
    "url": "/archives/v1.3.0/getting-started/s3/#configuring-s3-endpoint",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#configuring-s3-endpoint"
  },"479": {
    "doc": "Using S3 with Gluten",
    "title": "Configuring S3 instance credentials",
    "content": "S3 also provides other methods for accessing, you can also use instance credentials by setting the following config . spark.hadoop.fs.s3a.use.instance.credentials true . Note that in this case, “spark.hadoop.fs.s3a.endpoint” won’t take affect as Gluten will use the endpoint set during instance creation. ",
    "url": "/archives/v1.3.0/getting-started/s3/#configuring-s3-instance-credentials",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#configuring-s3-instance-credentials"
  },"480": {
    "doc": "Using S3 with Gluten",
    "title": "Configuring S3 IAM roles",
    "content": "You can also use iam role credentials by setting the following configurations. Instance credentials have higher priority than iam credentials. spark.hadoop.fs.s3a.iam.role xxxx spark.hadoop.fs.s3a.iam.role.session.name xxxx . Note that spark.hadoop.fs.s3a.iam.role.session.name is optional. ",
    "url": "/archives/v1.3.0/getting-started/s3/#configuring-s3-iam-roles",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#configuring-s3-iam-roles"
  },"481": {
    "doc": "Using S3 with Gluten",
    "title": "Other authentatication methods are not supported yet",
    "content": ". | AWS temporary credential | . ",
    "url": "/archives/v1.3.0/getting-started/s3/#other-authentatication-methods-are-not-supported-yet",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#other-authentatication-methods-are-not-supported-yet"
  },"482": {
    "doc": "Using S3 with Gluten",
    "title": "Log granularity of AWS C++ SDK in velox",
    "content": "You can change log granularity of AWS C++ SDK by setting the spark.gluten.velox.awsSdkLogLevel configuration. The Allowed values are: . | OFF | FATAL | ERROR | WARN | INFO | DEBUG | TRACE | . ",
    "url": "/archives/v1.3.0/getting-started/s3/#log-granularity-of-aws-c-sdk-in-velox",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#log-granularity-of-aws-c-sdk-in-velox"
  },"483": {
    "doc": "Using S3 with Gluten",
    "title": "Local Caching support",
    "content": "Velox supports a local cache when reading data from S3. Please refer Velox Local Cache part for more detailed configurations. ",
    "url": "/archives/v1.3.0/getting-started/s3/#local-caching-support",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/#local-caching-support"
  },"484": {
    "doc": "Using S3 with Gluten",
    "title": "Using S3 with Gluten",
    "content": "Object stores offered by CSPs such as AWS S3 are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use an S3 endpoint as your data source, please ensure you are using the following S3 configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to S3 with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/archives/v1.3.0/getting-started/s3/",
    
    "relUrl": "/archives/v1.3.0/getting-started/s3/"
  },"485": {
    "doc": "Build Parameters for Velox Backend",
    "title": "Build Parameters",
    "content": "Native build parameters for buildbundle-veloxbe.sh or builddeps-veloxbe.sh . Please set them via --, e.g. --build_type=Release. | Parameters | Description | Default | . | build_type | Build type for Velox &amp; gluten cpp, CMAKE_BUILD_TYPE. | Release | . | build_tests | Build gluten cpp tests. | OFF | . | build_examples | Build udf example. | OFF | . | build_benchmarks | Build gluten cpp benchmarks. | OFF | . | build_jemalloc | Build with jemalloc. | OFF | . | build_protobuf | Build protobuf lib. | OFF | . | enable_qat | Enable QAT for shuffle data de/compression. | OFF | . | enable_iaa | Enable IAA for shuffle data de/compression. | OFF | . | enable_hbm | Enable HBM allocator. | OFF | . | enable_s3 | Build with S3 support. | OFF | . | enable_gcs | Build with GCS support. | OFF | . | enable_hdfs | Build with HDFS support. | OFF | . | enable_abfs | Build with ABFS support. | OFF | . | enable_ep_cache | Enable caching for external project build (Velox). | OFF | . | enable_vcpkg | Enable vcpkg for static build. | OFF | . | run_setup_script | Run setup script to install Velox dependencies. | ON | . | velox_repo | Specify your own Velox repo to build. | ”” | . | velox_branch | Specify your own Velox branch to build. | ”” | . | velox_home | Specify your own Velox source path to build. | ”” | . | build_velox_tests | Build Velox tests. | OFF | . | build_velox_benchmarks | Build Velox benchmarks (velox_tests and connectors will be disabled if ON) | OFF | . | build_arrow | Build arrow java/cpp and install the libs in local. Can turn it OFF after first build. | ON | . | spark_version | Build for specified version of Spark(3.2, 3.3, 3.4, 3.5, ALL). ALL means build for all versions. | ALL | . Velox build parameters for build_velox.sh . Please set them via --, e.g., --velox_home=/YOUR/PATH. | Parameters | Description | Default | . | velox_home | Specify Velox source path to build. | GLUTEN_SRC/ep/build-velox/build/velox_ep | . | build_type | Velox build type, i.e., CMAKE_BUILD_TYPE. | Release | . | enable_s3 | Build Velox with S3 support. | OFF | . | enable_gcs | Build Velox with GCS support. | OFF | . | enable_hdfs | Build Velox with HDFS support. | OFF | . | enable_abfs | Build Velox with ABFS support. | OFF | . | run_setup_script | Run setup script to install Velox dependencies before build. | ON | . | enable_ep_cache | Enable and reuse cache of Velox build. | OFF | . | build_test_utils | Build Velox with cmake arg -DVELOX_BUILD_TEST_UTILS=ON if ON. | OFF | . | build_tests | Build Velox test. | OFF | . | build_benchmarks | Build Velox benchmarks. | OFF | . Maven build parameters . The below parameters can be set via -P for mvn. | Parameters | Description | Default state | . | backends-velox | Build Gluten Velox backend. | disabled | . | backends-clickhouse | Build Gluten ClickHouse backend. | disabled | . | celeborn | Build Gluten with Celeborn. | disabled | . | uniffle | Build Gluten with Uniffle. | disabled | . | delta | Build Gluten with Delta Lake support. | disabled | . | iceberg | Build Gluten with Iceberg support. | disabled | . | spark-3.2 | Build Gluten for Spark 3.2. | enabled | . | spark-3.3 | Build Gluten for Spark 3.3. | disabled | . | spark-3.4 | Build Gluten for Spark 3.4. | disabled | . | spark-3.5 | Build Gluten for Spark 3.5. | disabled | . ",
    "url": "/archives/v1.3.0/getting-started/build-guide/#build-parameters",
    
    "relUrl": "/archives/v1.3.0/getting-started/build-guide/#build-parameters"
  },"486": {
    "doc": "Build Parameters for Velox Backend",
    "title": "Gluten Jar for Deployment",
    "content": "The gluten jar built out is under GLUTEN_SRC/package/target/. It’s name pattern is gluten-&lt;backend_type&gt;-bundle-spark&lt;spark.bundle.version&gt;_&lt;scala.binary.version&gt;-&lt;os.detected.release&gt;_&lt;os.detected.release.version&gt;-&lt;project.version&gt;.jar. | Spark Version | spark.bundle.version | scala.binary.version | . | 3.2.2 | 3.2 | 2.12 | . | 3.3.1 | 3.3 | 2.12 | . | 3.4.2 | 3.4 | 2.12 | . | 3.5.1 | 3.5 | 2.12 | . ",
    "url": "/archives/v1.3.0/getting-started/build-guide/#gluten-jar-for-deployment",
    
    "relUrl": "/archives/v1.3.0/getting-started/build-guide/#gluten-jar-for-deployment"
  },"487": {
    "doc": "Build Parameters for Velox Backend",
    "title": "Build Parameters for Velox Backend",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started/build-guide/",
    
    "relUrl": "/archives/v1.3.0/getting-started/build-guide/"
  },"488": {
    "doc": "Getting-Started",
    "title": "Getting Started with Gluten for Apache Spark",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started#getting-started-with-gluten-for-apache-spark",
    
    "relUrl": "/archives/v1.3.0/getting-started#getting-started-with-gluten-for-apache-spark"
  },"489": {
    "doc": "Getting-Started",
    "title": "Getting-Started",
    "content": " ",
    "url": "/archives/v1.3.0/getting-started",
    
    "relUrl": "/archives/v1.3.0/getting-started"
  },"490": {
    "doc": "v1.3.0",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.3.0/#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.3.0/#gluten-documents-by-version"
  },"491": {
    "doc": "v1.3.0",
    "title": "v1.3.0",
    "content": " ",
    "url": "/archives/v1.3.0/",
    
    "relUrl": "/archives/v1.3.0/"
  },"492": {
    "doc": "Velox Backend",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend#gluten-documents-by-version",
    
    "relUrl": "/archives/v1.3.0/velox-backend#gluten-documents-by-version"
  },"493": {
    "doc": "Velox Backend",
    "title": "Velox Backend",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend",
    
    "relUrl": "/archives/v1.3.0/velox-backend"
  },"494": {
    "doc": "Limitations",
    "title": "Velox Backend Limitations",
    "content": "This document describes the limitations of velox backend by listing some known cases where exception will be thrown, gluten behaves incompatibly with spark, or certain plan’s execution must fall back to vanilla spark, etc. Override of Spark classes (For Spark3.2 and Spark3.3) . Gluten avoids to modify Spark’s existing code and use Spark APIs if possible. However, some APIs aren’t exposed in Vanilla spark and we have to copy the Spark file and do the hardcode changes. The list of override classes can be found as ignoreClasses in package/pom.xml . If you use customized Spark, you may check if the files are modified in your spark, otherwise your changes will be overrided. So you need to ensure preferentially load the Gluten jar to overwrite the jar of vanilla spark. Refer to How to prioritize loading Gluten jars in Spark. If not officially supported spark3.2/3.3 version is used, NoSuchMethodError can be thrown at runtime. More details see issue-4514. Fallbacks . Except the unsupported operators, functions, file formats, data sources listed in , there are some known cases also fall back to Vanilla Spark. ANSI . Gluten currently doesn’t support ANSI mode. If ANSI is enabled, Spark plan’s execution will always fall back to vanilla Spark. Runtime BloomFilter . Velox BloomFilter’s serialization format is different from Spark’s. BloomFilter binary generated by Velox can’t be deserialized by vanilla spark. So if might_contain falls back, we fall back bloom_filter_agg to vanilla spark also. Case Sensitive mode . Gluten only supports spark default case-insensitive mode. If case-sensitive mode is enabled, user may get incorrect result. Regexp functions . In Velox, regexp functions (rlike, regexp_extract, etc.) are implemented based on RE2, while in Spark they are based on java.util.regex. | Lookaround (lookahead/lookbehind) pattern is not supported in RE2. | When matching white space with pattern “\\s”, RE2 doesn’t treat “\\v” (or “\\x0b”) as white space, but java.util.regex does. | . There are a few unknown incompatible cases. If user cannot tolerate the incompatibility risk, please enable the below configuration property. spark.gluten.sql.fallbackRegexpExpressions . FileSource format . Currently, Gluten only fully supports parquet file format and partially support ORC. If other format is used, scan operator falls back to vanilla spark. Partitioned Table Scan . Gluten only support the partitioned table scan when the file path contain the partition info, otherwise will fall back to vanilla spark. Incompatible behavior . In certain cases, Gluten result may be different from Vanilla spark. JSON functions . Velox only supports double quotes surrounded strings, not single quotes, in JSON data. If single quotes are used, gluten will produce incorrect result. Velox doesn’t support [*] in path when get_json_object function is called and returns null instead. Parquet read conf . Gluten supports spark.files.ignoreCorruptFiles with default false, if true, the behavior is same as config false. Gluten ignores spark.sql.parquet.datetimeRebaseModeInRead, it only returns what write in parquet file. It does not consider the difference between legacy hybrid (Julian Gregorian) calendar and Proleptic Gregorian calendar. The result may be different with vanilla spark. Parquet write conf . Spark has spark.sql.parquet.datetimeRebaseModeInWrite config to decide whether legacy hybrid (Julian + Gregorian) calendar or Proleptic Gregorian calendar should be used during parquet writing for dates/timestamps. If the parquet to read is written by Spark with this config as true, Velox’s TableScan will output different result when reading it back. Partition write (For Spark3.2 and Spark3.3) . Gluten only supports static partition writes and does not support dynamic partition writes. spark.sql(\"CREATE TABLE t (c int, d long, e long) STORED AS PARQUET partitioned by (c, d)\") spark.sql(\"INSERT OVERWRITE TABLE t partition(c=1, d=2) SELECT 3 as e\") . Gluten does not support dynamic partition write and bucket write, Exception may be raised if you use. e.g., . spark.range(100).selectExpr(\"id as c1\", \"id % 7 as p\") .write .format(\"parquet\") .partitionBy(\"p\") .save(f.getCanonicalPath) . Partition write (For Spark3.4 and later) . Gluten supports static partition writes and dynamic partition writes. spark.sql(\"CREATE TABLE t (c int, d long, e long) STORED AS PARQUET partitioned by (c, d)\") spark.sql(\"INSERT OVERWRITE TABLE t partition(c=1, d) SELECT 2 as d, 3 as e\") . Gluten does not support bucket write, and will fall back to vanilla Spark. spark.range(100).selectExpr(\"id as c1\", \"id % 7 as p\") .write .format(\"parquet\") .bucketBy(2, \"c1\") .save(f.getCanonicalPath) . CTAS write (For Spark3.2 and Spark3.3) . Gluten does not create table as select. It may raise exception. e.g., . spark.range(100).toDF(\"id\") .write .format(\"parquet\") .saveAsTable(\"velox_ctas\") . CTAS write (For Spark3.4 and later) . Gluten supports create table as select with parquet file format. spark.range(100).toDF(\"id\") .write .format(\"parquet\") .saveAsTable(\"velox_ctas\") . HiveFileFormat write . Gluten supports writes of HiveFileFormat when the output file type is of type parquet only . NaN support . Velox does NOT support NaN. So unexpected result can be obtained for a few cases, e.g., comparing a number with NaN. Configuration . Parquet write only support three configs, other will not take effect. | compression code: . | sql conf: spark.sql.parquet.compression.codec | option: compression.codec | . | block size . | sql conf: spark.gluten.sql.columnar.parquet.write.blockSize | option: parquet.block.size | . | block rows . | sql conf: spark.gluten.sql.native.parquet.write.blockRows | option: parquet.block.rows | . | . Fetal error caused by Spark’s columnar reading . If the user enables Spark’s columnar reading, error can occur due to Spark’s columnar vector is not compatible with Gluten’s. Spill . OutOfMemoryExcetpion may still be triggered within current implementation of spill-to-disk feature, when shuffle partitions is set to a large number. When this case happens, please try to reduce the partition number to get rid of the OOM. Unsupported Data type support in ParquetScan . | Byte type causes fallback to vanilla spark | Timestamp type . Only reading with INT96 and dictionary encoding is supported. When reading INT64 represented millisecond/microsecond timestamps, or INT96 represented timestamps of other encodings, exceptions can occur. | Complex types . | Parquet scan of nested array with struct or array as element type is not supported in Velox (fallback behavior). | Parquet scan of nested map with struct as key type, or array type as value type is not supported in Velox (fallback behavior). | . | . CSV Read . The header option should be true. And now we only support DatasourceV1, i.e., user should set spark.sql.sources.useV1SourceList=csv. User defined read option is not supported, which will make CSV read fall back to vanilla Spark in most case. CSV read will also fall back to vanilla Spark and log warning when user specifies schema is different with file schema. ",
    "url": "/archives/v1.3.0/velox-backend/limitations/#velox-backend-limitations",
    
    "relUrl": "/archives/v1.3.0/velox-backend/limitations/#velox-backend-limitations"
  },"495": {
    "doc": "Limitations",
    "title": "Limitations",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend/limitations/",
    
    "relUrl": "/archives/v1.3.0/velox-backend/limitations/"
  },"496": {
    "doc": "Supported Operators & Functions",
    "title": "The Operators and Functions Support Progress",
    "content": "Gluten is still under active development. Here is a list of supported operators and functions. Since the same function may have different semantics between Presto and Spark, Velox implement the functions in Presto category, if we note a different semantics from Spark, then the function is implemented in Spark category. So Gluten will first try to find function in Velox’s spark category, if a function isn’t implemented then refer to Presto category. The total number of functions in Spark3.3 is 387, Gluten supports 189 of them. We use some notations to describe the supporting status of operators/functions in the tables below, they are: . | Value | Description | . | S | Supported. Gluten or Velox supports fully. | . | S* | Mark for foldable expression that will be converted to alias after spark’s optimization. | . | [Blank Cell] | Not applicable case or needs to confirm. | . | PS | Partial Support. Velox only partially supports it. | . | NS | Not Supported. Velox backend does not support it. | . And also some notations for the function implementation’s restrictions: . | Value | Description | . | Mismatched | Some functions are implemented by Velox, but have different semantics from Apache Spark, we mark them as “Mismatched”. | . | ANSI OFF | Gluten doesn’t support ANSI mode. If it is enabled, Gluten will fall back to Vanilla Spark. | . Operator Map . Gluten supports 28 operators (Drag to right to see all data types) . | Executor | Description | Gluten Name | Velox Name | BOOLEAN | BYTE | SHORT | INT | LONG | FLOAT | DOUBLE | STRING | NULL | BINARY | ARRAY | MAP | STRUCT(ROW) | DATE | TIMESTAMP | DECIMAL | CALENDAR | UDT | . | FileSourceScanExec | Reading data from files, often from Hive tables | FileSourceScanExecTransformer | TableScanNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | BatchScanExec | The backend for most file input | BatchScanExecTransformer | TableScanNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | FilterExec | The backend for most filter statements | FilterExecTransformer | FilterNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ProjectExec | The backend for most select, withColumn and dropColumn statements | ProjectExecTransformer | ProjectNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | HashAggregateExec | The backend for hash based aggregations | HashAggregateBaseTransformer | AggregationNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | BroadcastHashJoinExec | Implementation of join using broadcast data | BroadcastHashJoinExecTransformer | HashJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ShuffledHashJoinExec | Implementation of join using hashed shuffled data | ShuffleHashJoinExecTransformer | HashJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | SortExec | The backend for the sort operator | SortExecTransformer | OrderByNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | SortMergeJoinExec | Sort merge join, replacing with shuffled hash join | SortMergeJoinExecTransformer | MergeJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | WindowExec | Window operator backend | WindowExecTransformer | WindowNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | GlobalLimitExec | Limiting of results across partitions | LimitTransformer | LimitNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | LocalLimitExec | Per-partition limiting of results | LimitTransformer | LimitNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ExpandExec | The backend for the expand operator | ExpandExecTransformer | GroupIdNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | UnionExec | The backend for the union operator | UnionExecTransformer | N | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | DataWritingCommandExec | Writing data | Y | TableWriteNode | S | S | S | S | S | S | S | S | S | S | S | NS | S | S | NS | S | NS | NS | . | CartesianProductExec | Implementation of join using brute force | CartesianProductExecTransformer | NestedLoopJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | ShuffleExchangeExec | The backend for most data being exchanged between processes | ColumnarShuffleExchangeExec | ExchangeNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The unnest operation expands arrays and maps into separate columns | N | UnnestNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The top-n operation reorders a dataset based on one or more identified sort fields as well as a sorting order | N | TopNNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The partitioned output operation redistributes data based on zero or more distribution fields | N | PartitionedOutputNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The values operation returns specified data | N | ValuesNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | A receiving operation that merges multiple ordered streams to maintain orderedness | N | MergeExchangeNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | An operation that merges multiple ordered streams to maintain orderedness | N | LocalMergeNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | Partitions input data into multiple streams or combines data from multiple streams into a single stream | N | LocalPartitionNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The enforce single row operation checks that input contains at most one row and returns that row unmodified | N | EnforceSingleRowNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | . |   | The assign unique id operation adds one column at the end of the input columns with unique value per row | N | AssignUniqueIdNode | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | NS | S | S | S | S | S | . | ReusedExchangeExec | A wrapper for reused exchange to have different output | ReusedExchangeExec | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | CollectLimitExec | Reduce to single partition and apply limit | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | BroadcastExchangeExec | The backend for broadcast exchange of data | Y | Y | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | S | NS | NS | . | ObjectHashAggregateExec | The backend for hash based aggregations supporting TypedImperativeAggregate functions | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | SortAggregateExec | The backend for sort based aggregations | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | CoalesceExec | Reduce the partition numbers | CoalesceExecTransformer | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | GenerateExec | The backend for operations that generate more output rows than input rows like explode | GenerateExecTransformer | UnnestNode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | RangeExec | The backend for range operator | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | SampleExec | The backend for the sample operator | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | SubqueryBroadcastExec | Plan to collect and transform the broadcast key values | Y | Y | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | S | NS | NS | . | TakeOrderedAndProjectExec | Take the first limit elements as defined by the sortOrder, and do projection if needed | Y | Y | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | S | NS | NS | . | CustomShuffleReaderExec | A wrapper of shuffle query stage | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | InMemoryTableScanExec | Implementation of InMemory Table Scan | Y | Y |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | BroadcastNestedLoopJoinExec | Implementation of join using brute force. Full outer joins and joins where the broadcast side matches the join side (e.g.: LeftOuter with left broadcast) are not supported | BroadcastNestedLoopJoinExecTransformer | NestedLoopJoinNode | S | S | S | S | S | S | S | S | S | S | NS | NS | NS | S | NS | NS | NS | NS | . | AggregateInPandasExec | The backend for an Aggregation Pandas UDF, this accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ArrowEvalPythonExec | The backend of the Scalar Pandas UDFs. Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | FlatMapGroupsInPandasExec | The backend for Flat Map Groups Pandas UDF, Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | MapInPandasExec | The backend for Map Pandas Iterator UDF. Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | WindowInPandasExec | The backend for Window Aggregation Pandas UDF, Accelerates the data transfer between the Java process and the Python process | N | N |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | HiveTableScanExec | The Hive table scan operator. Column and partition pruning are both handled | Y | Y |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | InsertIntoHiveTable | Command for writing data out to a Hive table | Y | Y |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | Velox2Row | Convert Velox format to Row format | Y | Y | S | S | S | S | S | S | S | S | NS | S | NS | NS | NS | S | S | NS | NS | NS | . | Velox2Arrow | Convert Velox format to Arrow format | Y | Y | S | S | S | S | S | S | S | S | NS | S | S | S | S | S | NS | S | NS | NS | . Function support . Gluten supports 199 functions. (Drag to right to see all data types) . | Spark Functions | Velox/Presto Functions | Velox/Spark functions | Gluten | Restrictions | BOOLEAN | BYTE | SHORT | INT | LONG | FLOAT | DOUBLE | DATE | TIMESTAMP | STRING | DECIMAL | NULL | BINARY | CALENDAR | ARRAY | MAP | STRUCT | UDT | . | ! |   | not | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | != | neq |   | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | % | mod | remainder | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | &amp; | bitwise_and | bitwise_and | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | * | multiply | multiply | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | + | plus | add | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | - | minus | subtract | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | / | divide | divide | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | &lt; | lt | lessthan | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &lt;= | lte | lessthanorequa | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &lt;=&gt; |   | equalnullsafe | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | &lt;&gt; | neq | notequalto | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | = |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | == | eq | equalto | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &gt; | gt | greaterthan | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | &gt;= | gte | greaterthanorequal | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | ^ | bitwise_xor |   | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | | bitwise_or | bitwise_or | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . || |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ~ | bitwise_not |   | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | and |   |   | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | between | between | between | S |   | S | S | S | S | S | S | S | S |   | S |   |   |   |   |   |   |   |   | . | bit_and | bitwise_and_agg |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_count | bit_count | bit_count | S |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_get |   | bit_get | S |   |   | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_or |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_xor |   | bit_xor | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | case |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | div |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | getbit |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | if |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ifnull |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | in |   | in | S |   |   |   | S | S | S | S | S | S | S | S |   |   |   |   |   |   |   |   | . | isnan | is_nan | isnan | S |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   |   |   | . | isnotnull |   | isnotnull | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | isnull | is_null | isnull | S |   | S | S | S | S | S | S | S |   |   | S |   |   |   |   |   |   |   |   | . | mod | mod | remainder | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | negative | negate | unaryminus |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | not |   | not | S |   | S | S | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   | . | nullif |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | or |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | positive |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | when |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ascii |   | ascii | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | base64 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bin |   | bin |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bit_length |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | btrim |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | char, chr | chr | chr | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | char_length/character_length | length | length | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | character_lengt/char_length | length | length | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | chr, char | chr | chr | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | concat | concat | concat | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | concat_ws |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | contains |   | contains |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | decode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | elt |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | encode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | endswith |   | endsWith |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | find_in_set |   |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | format_number |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | format_string |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | initcap |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | instr |   | instr | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | lcase, lower | lower | lower | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | left |   |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | length | length | length | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | levenshtein |   | levenshtein | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | locate | strpos |   | S | Mismatched |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | lower | lower | lower | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | lpad | lpad |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | ltrim | ltrim | ltrim | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | octet_length |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | overlay |   | overlay | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | parse_url |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | position | strpos |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | printf |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | repeat |   | repeat | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | replace | replace | replace | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | reverse | reverse |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | right |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rpad | rpad |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | rtrim | rtrim | rtrim | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | sentences |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | soundex |   | soundex | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | space |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | split | split | split | S | Mismatched |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | split_part | split_part |   |   | Mismatched |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | startswith |   | startsWith |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | substr, substring | substr | substring | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | substring, substr | substr | substring | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | substring_index |   | substring_index | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | translate |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | trim | trim | trim | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | ucase, upper | upper | upper | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | unbase64 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unhex |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | upper, ucase | upper | upper | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | xpath |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_boolean |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_double |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_float |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_int |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_long |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_number |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_short |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | xpath_string |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | like | like |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp |   | rlike | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_extract | regexp_extract | regexp_extract | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_extract_all | regexp_extract_all |   | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_like | regexp_like | rlike | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | regexp_replace | regexp_replace |   | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | rlike |   | rlike | S | Lookaround unsupported |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | abs | abs | abs | S | ANSI OFF |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | acos | acos |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | acosh |   | acosh | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | asin | asin |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | asinh |   | asinh | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | atan | atan |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | atan2 | atan2 |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | atanh |   | atanh | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | bround |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | cbrt | cbrt |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | ceil | ceil | ceil | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | ceiling | ceiling |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | conv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | cos | cos |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | cosh | cosh |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | cot |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | degrees | degrees |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | e | e |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | exp | exp | exp | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | expm1 |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | factorial |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | floor | floor | floor | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | hex |   |   | S |   |   |   |   |   | S |   |   |   |   | S |   |   | S |   |   |   |   |   | . | hypot |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ln | ln |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | log | ln | log | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | log10 | log10 |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | log1p |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | log2 | log2 |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | pi | pi |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | pmod |   | pmod | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | pow, power | pow,power | power |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | power, pow | power,pow | power | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | radians | radians |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | rand | rand | rand | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rand | rand | rand |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | random | random |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rint |   | rint | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | round | round | round | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | shiftleft | bitwise_left_shift | shiftleft | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | shiftright | bitwise_right_shift | shiftright | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | shiftrightunsigned |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sign, signum | sign |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | signum, sign | sign |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | sin | sin |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | sinh |   | sinh |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sqrt | sqrt |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | tan | tan |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | tanh | tanh |   | S |   |   | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | width_bucket | width_bucket | width_bucket | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array |   | array | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | aggregate | aggregate | reduce | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_contains |   | array_contains | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_distinct | array_distinct |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_except | array_except |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_intersect | array_intersect | array_intersect | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_join | array_join |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_max | array_max |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_min | array_min |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_position | array_position |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   |   | . | array_remove | array_remove |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_repeat |   |   | S |   | S | S | S | S | S | S | S | S | S | S | S |   |   |   |   |   |   |   | . | array_sort | array_sort | array_sort | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_union |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | arrays_overlap | array_overlap | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | arrays_zip | zip |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | cardinality | cardinality |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | element_at | element_at | element_at | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S | S |   |   | . | exists | any_match |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | explode, explode_outer |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | explode_outer, explode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | filter | filter | filter | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | forall | all_match |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | flatten | flatten | flatten | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map | map | map | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_concat | map_concat |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_entries | map_entries |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_filter | map_filter | map_filter |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | get_map_value |   | element_at | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | map_from_arrays |   | map_from_arrays | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | map_from_entries | map_from_entries |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_keys | map_keys | map_keys | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | map_values | map_values | map_values | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | map_zip_with | map_zip_with |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   |   | . | named_struct,struct | row_construct | named_struct | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   | . | posexplode_outer,posexplode |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sequence |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | shuffle | shuffle | shuffle | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | size |   | size | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | array_size |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | slice | slice |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sort_array |   | sort_array | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | str_to_map |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | transform | transform | transofrm |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | transform_keys | transform_keys |   | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | transform_values | transform_values |   | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | zip_with | zip_with |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | add_months |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_date |   |   | S* |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_timestamp |   |   | S* |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_timezone |   |   | S* |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date | date |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_add | date_add | date_add | S |   |   | S | S | S |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | date_format | date_format |   | S |   |   |   |   | S |   |   |   |   | S |   |   |   |   |   |   |   |   |   | . | date_from_unix_date |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_part |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_sub |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | date_trunc | date_trunc |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | datediff | date_diff |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | day | day |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | dayofmonth | day_of_month |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | dayofweek | day_of_week,dow |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | dayofyear | day_of_year,doy |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | extract |   |   |   |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | from_unixtime | from_unixtime |   | S |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | from_utc_timestamp |   | from_utc_timestamp | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | hour | hour |   | S |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   | . | last_day |   | last_day | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_date |   | make_date | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_dt_interval |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_interval |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | make_ym_interval |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | minute | minute |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | month | month |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | months_between |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | next_day |   |   | S |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   |   | . | now |   |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | quarter | quarter |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | second | second |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | session_window |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp_micros |   | timestamp_micros | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp_millis |   | timestamp_millis | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | timestamp_seconds |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_date |   |   | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | to_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_unix_timestamp | to_unixtime | to_unix_timestamp | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_utc_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | trunc |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_timestamp |   | unix_timestamp |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_seconds |   | unix_seconds | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_millis |   | unix_millis | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | unix_micros |   | unix_micros | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | weekday |   |   | S |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   |   |   | . | weekofyear | week,week_of_year |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | window |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | year | year | year | S |   |   |   |   |   |   |   |   | S | S |   |   |   |   |   |   |   |   |   | . | aggregate |   | aggregate | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | any |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | approx_count_distinct | approx_distinct |   | S |   | S | S | S | S | S | S | S | S |   | S |   |   |   |   |   |   |   |   | . | approx_percentile |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | avg | avg |   | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | bool_and |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | bool_or |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | collect_list |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | collect_set |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | corr | corr |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | count | count |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | count_if | count_if |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | count_min_sketch |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | covar_pop | covar_pop |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | covar_samp | covar_samp |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | every |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | first |   | first | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | first_value |   | first_value | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | grouping |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | grouping_id |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | kurtosis | kurtosis | kurtosis | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | last |   | last | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | last_value |   | last_value | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | max | max |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | max_by |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | mean | avg |   | S | ANSI OFF |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | min | min |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | min_by |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | regr_avgx | regr_avgx | regr_avgx | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_avgy | regr_avgy | regr_avgy | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_count | regr_count | regr_count | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_r2 | regr_r2 | regr_r2 | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_intercept | regr_intercept | regr_intercept | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_slope | regr_slope | regr_slope | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_sxy | regr_sxy | regr_sxy | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_sxx | regr_sxx | regr_sxx | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | regr_syy | regr_syy | regr_syy | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | skewness | skewness | skewness | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | some |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | std,stddev | stddev |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | stddev,std | stddev |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | stddev_pop | stddev_pop |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | stddev_samp | stddev_samp |   | S |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | sum | sum |   | S | ANSI OFF |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | var_pop | var_pop |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | var_samp | var_samp |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | variance | variance |   | S |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   | . | cume_dist | cume_dist |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | dense_rank | dense_rank |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | lag |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | lead |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nth_value | nth_value | nth_value | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | ntile | ntile | ntile | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | percent_rank | percent_rank |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rank | rank |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | row_number | row_number |   | S |   |   |   | S | S | S |   |   |   |   |   |   |   |   |   |   |   |   |   | . | from_csv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | from_json |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | get_json_object | json_extract_scalar | get_json_object | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   | . | json_array_length | json_array_length |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | S |   | . | json_tuple |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | schema_of_csv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | schema_of_json |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_csv |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | to_json |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | assert_true |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | coalesce |   |   | PS |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | crc32 | crc32 |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | current_user |   |   | S* |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | current_catalog |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | current_database |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | greatest | greatest | greatest | S |   |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   | . | hash | hash | hash | S |   | S | S | S | S | S | S | S |   |   |   |   |   |   |   |   |   |   |   | . | inline |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | inline_outer |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | input_file_name |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | input_file_block_length |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | input_file_block_start |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | java_method |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | least | least | least | S |   |   |   |   |   | S | S | S | S | S |   |   |   |   |   |   |   |   |   | . | md5 | md5 |   | S |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | monotonically_increasing_id |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nanvl |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nvl |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | nvl2 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | raise_error |   | raise_error | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | reflect |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | sha |   |   | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | sha1 | sha1 | sha1 | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | sha2 |   | sha2 | S |   |   |   |   |   |   |   |   |   |   | S |   |   |   |   |   |   |   |   | . | spark_partition_id |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | stack |   |   | S |   | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | S | . | xxhash64 | xxhash64 | xxhash64 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | uuid | uuid | uuid | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | rand | rand | rand | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_add |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_substract |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_multiply |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . | try_divide |   |   | S |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   | . ",
    "url": "/archives/v1.3.0/velox-backend/support/#the-operators-and-functions-support-progress",
    
    "relUrl": "/archives/v1.3.0/velox-backend/support/#the-operators-and-functions-support-progress"
  },"497": {
    "doc": "Supported Operators & Functions",
    "title": "Supported Operators & Functions",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend/support/",
    
    "relUrl": "/archives/v1.3.0/velox-backend/support/"
  },"498": {
    "doc": "Troubleshooting",
    "title": "Velox Backend Troubleshooting",
    "content": "Fatal error after native exception is thrown . We depend on checking exceptions thrown from native code to validate whether a spark plan can be really offloaded to native engine. But if libunwind-dev is installed, native exception will not be caught and will interrupt the program. So far, we observed this fatal error can happen only on Ubuntu 20.04. Please remove libunwind-dev and then re-build the project to address this issue. sudo apt-get purge --auto-remove libunwind-dev . Jar conflict issue . With the latest version of Gluten, there should not be any jar conflict issue anymore. If you still get hit with such issue, please follow the below instructions. The potentially conflicting libraries include protobuf (Both Velox and CK backend), flatbuffers (Velox backend), and arrow-* (Velox backend). These libraries are compiled from source and packed into Gluten’s jar. Jvm should search them from Gluten.jar firstly and load them. But for some reason jvm loads the jars from spark_home/jars which causes conflict. You may use below commands to remove the jars from spark_home/jars. We are still investigating the root cause. Welcome to share if you have good solution. rm -rf $SPARK_HOME/jars/protobuf-* # velox backend only rm -rf $SPARK_HOME/jars/flatbuffers-* rm -rf $SPARK_HOME/jars/arrow-* . Incompatible class error when using native writer . Gluten native writer overwrite some vanilla spark classes. Therefore, when running a program that uses gluten, it is essential to ensure that the gluten jar is loaded prior to the vanilla spark jar. In this section, we will provide some configuration settings in $SPARK_HOME/conf/spark-defaults.conf for Yarn client, Yarn cluster, and Local&amp;Standalone mode to guarantee that the gluten jar is prioritized. Configurations for Yarn Client mode . // spark will upload the gluten jar to hdfs and then the nodemanager will fetch the gluten jar before start the executor process. Here also can set the spark.jars. spark.files = {absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The absolute path on running node spark.driver.extraClassPath={absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The relative path under the executor working directory spark.executor.extraClassPath=./gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar . Configurations for Yarn Cluster mode . spark.driver.userClassPathFirst = true spark.executor.userClassPathFirst = true spark.files = {absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The relative path under the executor working directory spark.driver.extraClassPath=./gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The relative path under the executor working directory spark.executor.extraClassPath=./gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar . Configurations for Local &amp; Standalone mode . // The absolute path on running node spark.driver.extraClassPath={absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar // The absolute path on running node spark.executor.extraClassPath={absolute_path}/gluten-&lt;spark-version&gt;-&lt;gluten-version&gt;-SNAPSHOT-jar-with-dependencies.jar . Invalid pointer error . If the below error is reported at runtime, please re-build gluten with --compile_arrow_java=ON, then redeploy Gluten jar. *** Error in `/usr/local/jdk1.8.0_381/bin/java': free(): invalid pointer: 0x00007f36cb5cec80 *** ======= Backtrace: ========= /lib64/libc.so.6(+0x7d1fd)[0x7f38c29da1fd] /lib64/libstdc++.so.6(_ZNSt6locale5_Impl16_M_install_facetEPKNS_2idEPKNS_5facetE+0x142)[0x7f36cb3370d2] /lib64/libstdc++.so.6(_ZNSt6locale5_ImplC1Em+0x1e3)[0x7f36cb337523] /lib64/libstdc++.so.6(+0x71495)[0x7f36cb338495] /lib64/libpthread.so.0(pthread_once+0x50)[0x7f38c3147be0] /lib64/libstdc++.so.6(+0x714e1)[0x7f36cb3384e1] /lib64/libstdc++.so.6(_ZNSt6localeC2Ev+0x13)[0x7f36cb338523] /lib64/libstdc++.so.6(_ZNSt8ios_base4InitC2Ev+0xbc)[0x7f36cb33537c] /tmp/jnilib-645156599284574767.tmp(+0x2a90)[0x7f375d235a90] /lib64/ld-linux-x86-64.so.2(+0xf4e3)[0x7f38c33664e3] /lib64/ld-linux-x86-64.so.2(+0x13b04)[0x7f38c336ab04] /lib64/ld-linux-x86-64.so.2(+0xf2f4)[0x7f38c33662f4] /lib64/ld-linux-x86-64.so.2(+0x1321b)[0x7f38c336a21b] /lib64/libdl.so.2(+0x102b)[0x7f38c2d1f02b] /lib64/ld-linux-x86-64.so.2(+0xf2f4)[0x7f38c33662f4] /lib64/libdl.so.2(+0x162d)[0x7f38c2d1f62d] /lib64/libdl.so.2(dlopen+0x31)[0x7f38c2d1f0c1] /usr/local/jdk1.8.0_381/jre/lib/amd64/server/libjvm.so(+0x9292b1)[0x7f38c22732b1] /usr/local/jdk1.8.0_381/jre/lib/amd64/server/libjvm.so(JVM_LoadLibrary+0xa1)[0x7f38c205e0c1] /usr/local/jdk1.8.0_381/jre/lib/amd64/libjava.so(Java_java_lang_ClassLoader_00024NativeLibrary_load+0x1ac) ... ",
    "url": "/archives/v1.3.0/velox-backend/troubleshooting/#velox-backend-troubleshooting",
    
    "relUrl": "/archives/v1.3.0/velox-backend/troubleshooting/#velox-backend-troubleshooting"
  },"499": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend/troubleshooting/",
    
    "relUrl": "/archives/v1.3.0/velox-backend/troubleshooting/"
  },"500": {
    "doc": "Velox UDF",
    "title": "Velox User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF)",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend/udf/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#velox-user-defined-functions-udf-and-user-defined-aggregate-functions-udaf"
  },"501": {
    "doc": "Velox UDF",
    "title": "Introduction",
    "content": "Velox backend supports User-Defined Functions (UDF) and User-Defined Aggregate Functions (UDAF). Users can create their own functions using the UDF interface provided in Velox backend and build libraries for these functions. At runtime, the UDF are registered at the start of applications. Once registered, Gluten will be able to parse and offload these UDF into Velox during execution. ",
    "url": "/archives/v1.3.0/velox-backend/udf/#introduction",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#introduction"
  },"502": {
    "doc": "Velox UDF",
    "title": "Create and Build UDF/UDAF library",
    "content": "The following steps demonstrate how to set up a UDF library project: . | Include the UDF Interface Header: First, include the UDF interface header file Udf.h in the project file. The header file defines the UdfEntry struct, along with the macros for declaring the necessary functions to integrate the UDF into Gluten and Velox. | Implement the UDF: Implement UDF. These functions should be able to register to Velox. | Implement the Interface Functions: Implement the following interface functions that integrate UDF into Project Gluten: . | getNumUdf(): This function should return the number of UDF in the library. This is used to allocating udfEntries array as the argument for the next function getUdfEntries. | getUdfEntries(gluten::UdfEntry* udfEntries): This function should populate the provided udfEntries array with the details of the UDF, including function names and signatures. | registerUdf(): This function is called to register the UDF to Velox function registry. This is where users should register functions by calling facebook::velox::exec::registerVecotorFunction or other Velox APIs. | The interface functions are mapped to marcos in Udf.h. Here’s an example of how to implement these functions: . | . // Filename MyUDF.cc #include &lt;velox/expression/VectorFunction.h&gt; #include &lt;velox/udf/Udf.h&gt; namespace { static const char* kInteger = \"integer\"; } const int kNumMyUdf = 1; const char* myUdfArgs[] = {kInteger}: gluten::UdfEntry myUdfSig = {\"myudf\", kInteger, 1, myUdfArgs}; class MyUdf : public facebook::velox::exec::VectorFunction { ... // Omit concrete implementation } static std::vector&lt;std::shared_ptr&lt;exec::FunctionSignature&gt;&gt; myUdfSignatures() { return {facebook::velox::exec::FunctionSignatureBuilder() .returnType(myUdfSig.dataType) .argumentType(myUdfSig.argTypes[0]) .build()}; } DEFINE_GET_NUM_UDF { return kNumMyUdf; } DEFINE_GET_UDF_ENTRIES { udfEntries[0] = myUdfSig; } DEFINE_REGISTER_UDF { facebook::velox::exec::registerVectorFunction( myUdf[0].name, myUdfSignatures(), std::make_unique&lt;MyUdf&gt;()); } . | . To build the UDF library, users need to compile the C++ code and link to libvelox.so. It’s recommended to create a CMakeLists.txt for the project. Here’s an example: . project(myudf) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(GLUTEN_HOME /path/to/gluten) add_library(myudf SHARED \"MyUDF.cpp\") find_library(VELOX_LIBRARY REQUIRED NAMES velox HINTS ${GLUTEN_HOME}/cpp/build/releases NO_DEFAULT_PATH) target_include_directories(myudf PRIVATE ${GLUTEN_HOME}/cpp ${GLUTEN_HOME}/ep/build-velox/build/velox_ep) target_link_libraries(myudf PRIVATE ${VELOX_LIBRARY}) . The steps for creating and building a UDAF library are quite similar to those for a UDF library. The major difference lies in including and defining specific functions within the UDAF header file Udaf.h . | getNumUdaf() | getUdafEntries(gluten::UdafEntry* udafEntries) | registerUdaf() | . gluten::UdafEntry requires an additional field intermediateType, to specify the output type from partial aggregation. For detailed implementation, you can refer to the example code in MyUDAF.cc . ",
    "url": "/archives/v1.3.0/velox-backend/udf/#create-and-build-udfudaf-library",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#create-and-build-udfudaf-library"
  },"503": {
    "doc": "Velox UDF",
    "title": "Using UDF/UDAF in Gluten",
    "content": "Gluten loads the UDF libraries at runtime. You can upload UDF libraries via --files or --archives, and configure the library paths using the provided Spark configuration, which accepts comma separated list of library paths. Note if running on Yarn client mode, the uploaded files are not reachable on driver side. Users should copy those files to somewhere reachable for driver and set spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths. This configuration is also useful when the udfLibraryPaths is different between driver side and executor side. | Use the --files option to upload a library and configure its relative path | . --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so . | Use the --archives option to upload an archive and configure its relative path | . --archives /path/to/udf_archives.zip#udf_archives --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=udf_archives # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/udf_archives.zip . | Configure URI | . You can also specify the local or HDFS URIs to the UDF libraries or archives. Local URIs should exist on driver and every worker nodes. --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=file:///path/to/library_or_archive . ",
    "url": "/archives/v1.3.0/velox-backend/udf/#using-udfudaf-in-gluten",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#using-udfudaf-in-gluten"
  },"504": {
    "doc": "Velox UDF",
    "title": "Try the example",
    "content": "We provided Velox UDF examples in file MyUDF.cc and UDAF examples in file MyUDAF.cc. You need to build the gluten project with --build_example=ON to get the example libraries./dev/buildbundle-veloxbe.sh --build_examples=ON . Then, you can find the example libraries at /path/to/gluten/cpp/build/velox/udf/examples/ . Start spark-shell or spark-sql with below configuration . # Use the `--files` option to upload a library and configure its relative path --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so . or . # Only configure URI --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=file:///path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so . Run query. The functions myudf1 and myudf2 increment the input value by a constant of 5 . select myudf1(100L), myudf2(1) . The output from spark-shell will be like . +------------------+----------------+ |udfexpression(100)|udfexpression(1)| +------------------+----------------+ | 105| 6| +------------------+----------------+ . ",
    "url": "/archives/v1.3.0/velox-backend/udf/#try-the-example",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#try-the-example"
  },"505": {
    "doc": "Velox UDF",
    "title": "Configurations",
    "content": "| Parameters | Description | . | spark.gluten.sql.columnar.backend.velox.udfLibraryPaths | Path to the udf/udaf libraries. | . | spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths | Path to the udf/udaf libraries on driver node. Only applicable on yarn-client mode. | . | spark.gluten.sql.columnar.backend.velox.udfAllowTypeConversion | Whether to inject possible cast to convert mismatched data types from input to one registered signatures. | . ",
    "url": "/archives/v1.3.0/velox-backend/udf/#configurations",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#configurations"
  },"506": {
    "doc": "Velox UDF",
    "title": "Pandas UDFs (a.k.a. Vectorized UDFs)",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend/udf/#pandas-udfs-aka-vectorized-udfs",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#pandas-udfs-aka-vectorized-udfs"
  },"507": {
    "doc": "Velox UDF",
    "title": "Introduction",
    "content": "Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf() as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general. For more details, you can refer doc. ",
    "url": "/archives/v1.3.0/velox-backend/udf/#introduction-1",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#introduction-1"
  },"508": {
    "doc": "Velox UDF",
    "title": "Using Pandas UDFs in Gluten with Velox Backend",
    "content": "Similar as in vanilla Spark, user needs to set up pyspark/arrow dependencies properly first. You may can refer following steps: . pip3 install pyspark==$SPARK_VERSION cython pip3 install pandas pyarrow . Gluten provides a config to control enable ColumnarArrowEvalPython or not, with true as defalt. spark.gluten.sql.columnar.arrowUdf . Then take following PySpark code for example: . from pyspark.sql.functions import pandas_udf, PandasUDFType import pyspark.sql.functions as F import os @pandas_udf('long') def pandas_plus_one(v): return (v + 1) df = spark.read.orc(\"path_to_file\").select(\"quantity\").withColumn(\"processed_quantity\", pandas_plus_one(\"quantity\")).select(\"quantity\") . The expected physical plan will be: . == Physical Plan == VeloxColumnarToRowExec +- ^(2) ProjectExecTransformer [pythonUDF0#45L AS processed_quantity#41L] +- ^(2) InputIteratorTransformer[quantity#2L, pythonUDF0#45L] +- ^(2) InputAdapter +- ^(2) ColumnarArrowEvalPython [pandas_plus_one(quantity#2L)#40L], [pythonUDF0#45L], 200 +- ^(1) NativeFileScan orc [quantity#2L] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/***], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;quantity:bigint&gt; . ",
    "url": "/archives/v1.3.0/velox-backend/udf/#using-pandas-udfs-in-gluten-with-velox-backend",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/#using-pandas-udfs-in-gluten-with-velox-backend"
  },"509": {
    "doc": "Velox UDF",
    "title": "Velox UDF",
    "content": " ",
    "url": "/archives/v1.3.0/velox-backend/udf/",
    
    "relUrl": "/archives/v1.3.0/velox-backend/udf/"
  }
}
