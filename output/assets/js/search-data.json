{"0": {
    "doc": "ClickHouse Backend",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/clickhouse/#gluten-documents-by-version",
    
    "relUrl": "/clickhouse/#gluten-documents-by-version"
  },"1": {
    "doc": "ClickHouse Backend",
    "title": "ClickHouse Backend",
    "content": " ",
    "url": "/clickhouse/",
    
    "relUrl": "/clickhouse/"
  },"2": {
    "doc": "CPP Code Style",
    "title": "Gluten CPP Core Guidelines",
    "content": "This is a set of CPP core guidelines for Gluten. The aim is to make the codebase simpler, more efficient, more maintainable by promoting consistency and according to best practices. ",
    "url": "/developers/CppCodingStyle.html#gluten-cpp-core-guidelines",
    
    "relUrl": "/developers/CppCodingStyle.html#gluten-cpp-core-guidelines"
  },"3": {
    "doc": "CPP Code Style",
    "title": "Philosophy",
    "content": "Philosophical rules are generally not measurable. However, they are valuable. For Gluten CPP coding, there are a few Philosophical rules as the following. | Write in ISO Standard C++. | Standard API first, the CPP programming APIs are priority to system calls. | Write code consistently. It’s good for understanding and maintaining. | Keep simple, making code clear and easy to read. | Optimize code for reader, not for writer. Thus, more time will be spent reading code than writing it. | Make it work, and then make it better or faster. | Don’t import any complexity if possible. Collaborate with minimal knowledge consensus. | . ",
    "url": "/developers/CppCodingStyle.html#philosophy",
    
    "relUrl": "/developers/CppCodingStyle.html#philosophy"
  },"4": {
    "doc": "CPP Code Style",
    "title": "Code Formatting",
    "content": "Many aspects of C++ coding style will be covered by clang-format, such as spacing, line width, indentation and ordering (for includes, using directives and etc).  . | Always ensure your code is compatible with clang-format-12 for Velox backend. | dev/formatcppcode.sh is provided for formatting Velox CPP code. | . ",
    "url": "/developers/CppCodingStyle.html#code-formatting",
    
    "relUrl": "/developers/CppCodingStyle.html#code-formatting"
  },"5": {
    "doc": "CPP Code Style",
    "title": "Naming Conventions",
    "content": ". | Use PascalCase for types (class, struct, enum, type alias, type template parameter) and file name. | Use camelCase for function, member and local variable, and non-type template parameter. | Use camelCase_ for private and protected member variable. | Use snake_case for namespace name and build target. | Use UPPER_SNAKE_CASE for macro. | Use kPascalCase for static constant and enumerator. | . ",
    "url": "/developers/CppCodingStyle.html#naming-conventions",
    
    "relUrl": "/developers/CppCodingStyle.html#naming-conventions"
  },"6": {
    "doc": "CPP Code Style",
    "title": "Designs",
    "content": ". | No over design. | No negation of negation, isValid is better than isNotInvalid. | Avoid corner case, and common case first. | Express ideas directly, don’t let me think. | Make a check for the arguments in the interface between modules, and don’t make a check in the inner implementation, use assert in the private implementation instead of too much safe check. | . ",
    "url": "/developers/CppCodingStyle.html#designs",
    
    "relUrl": "/developers/CppCodingStyle.html#designs"
  },"7": {
    "doc": "CPP Code Style",
    "title": "Source File &amp; Header File",
    "content": ". | All header files must have a single-inclusion guard using #pragma once | Always use .h as header file suffix, not .hpp. | Always use .cc as source file suffix, neither .cpp nor .cxx. | One file should contain one main class, and the file name should be consistent with the main class name. | Obvious exception: files used for defining various misc functions. | . | If a header file has a corresponding source file, they should have the same file name with different suffix, such as a.h vs a.cc. | If a function is declared in the file a.h, ensure it’s defined in the corrosponding source file a.cc, do not define it in other files. | No deep source directory for CPP files, not do it as JAVA. | Include header files should satisfy the following rules. | Include the necessary header files, which means the source file (.cc) containing the only one line #include \"test.h\" can be compiled successfully without including any other header files. | Do not include any unnecessary header files, the more including, the slower compiling. | In one word, no more, no less, just as needed. | . | . ",
    "url": "/developers/CppCodingStyle.html#source-file--header-file",
    
    "relUrl": "/developers/CppCodingStyle.html#source-file--header-file"
  },"8": {
    "doc": "CPP Code Style",
    "title": "Class",
    "content": ". | Base class name doesn’t end with Base, use Backend instead of BackendBase. | Ensure one class does one thing, and follows the single responsibility principle. | No big class, No huge class, No too much interfaces. | Distinguish interface from implementation, make implementations private. | When designing a class hierarchy, distinguish between interface inheritance and implementation inheritance. | Ensure that public inheritance represent the relation of is-a. | Ensure that private inheritance represent the relation of implements-with. | . | Don’t make a function virtual without reason. | Ensure the polymorphic base class has a virtual deconstructor. | Use override to make overriding explicit and to make the compiler work. | Use const to mark the member function read-only as far as possible. | When you try to define a copy constructor or a operator= for a class, remember the Rule of three/five/zero. | . ",
    "url": "/developers/CppCodingStyle.html#class",
    
    "relUrl": "/developers/CppCodingStyle.html#class"
  },"9": {
    "doc": "CPP Code Style",
    "title": "Function",
    "content": ". | Make functions short and simple. | Calling a meaningful function is more readable than writing too many statements in place, but the performance-sensitive code path is an exception. | Give the function a good name, how to check whether the function name is good or not. | When you read it loudly, you feel smooth. | The information can be represented by arguments should not be encoded into the function name. such as. use get(size_t index) instead of getByIndex. | . | A function should focus on a single logic operation. | A function should do as the name meaning. | do everything converd by the function name | don’t do anything not convered by the function name | . | . ",
    "url": "/developers/CppCodingStyle.html#function",
    
    "relUrl": "/developers/CppCodingStyle.html#function"
  },"10": {
    "doc": "CPP Code Style",
    "title": "Variable",
    "content": ". | Make variable names simple and meaningful. | Don’t group all your variables at the top of the scope, it’s an outdated habit. | Declare variables as close to the usage point as possible. | . ",
    "url": "/developers/CppCodingStyle.html#variable",
    
    "relUrl": "/developers/CppCodingStyle.html#variable"
  },"11": {
    "doc": "CPP Code Style",
    "title": "Constant",
    "content": ". | Prefer const variables to using preprocessor (#define) to define constant values. | . ",
    "url": "/developers/CppCodingStyle.html#constant",
    
    "relUrl": "/developers/CppCodingStyle.html#constant"
  },"12": {
    "doc": "CPP Code Style",
    "title": "Macro",
    "content": ". | Macros downgrade readability, break mind, and affect debug. | Macros have side effects. | Use macros cautiously and carefully. | Consider using const variables or inline functions to replace macros. | Consider defining macros with the wrap of do {...} while (0) | Avoid using 3rd party library macros directly. | . ",
    "url": "/developers/CppCodingStyle.html#macro",
    
    "relUrl": "/developers/CppCodingStyle.html#macro"
  },"13": {
    "doc": "CPP Code Style",
    "title": "Namespace",
    "content": ". | Don’t using namespace xxx in header files. Instead, you can do this in source files. But it’s still not encouraged. | Place all Gluten CPP codes under namespace gluten because one level namespace is enough. No nested namespace. Nested namespaces bring mess. | The anonymous namespace is recommended for defining file level classes, functions and variables. It’s used to place file scoped static functions and variables. | . ",
    "url": "/developers/CppCodingStyle.html#namespace",
    
    "relUrl": "/developers/CppCodingStyle.html#namespace"
  },"14": {
    "doc": "CPP Code Style",
    "title": "Resource Management",
    "content": ". | Use handles and RAII to manage resources automatically. | Immediately give the result of an explicit resource allocation to a manager object. | Prefer scoped objects and stack objects. | Use raw pointers to denote individual objects. | Use pointer + size_t to denote array objects if you don’t want to use containers. | A raw pointer (a T*) is non-owning. | A raw reference (a T&amp;) is non-owning. | Understand the difference of unique_ptr, shared_ptr, weak_ptr. | unique_ptr represents ownership, but not share ownership. unique_ptr is equivalent to RAII, release the resource when the object is destructed. | shared_ptr represents shared ownership by use-count. It is more expensive that unqiue_ptr. | weak_ptr models temporary ownership. It is useful in breaking reference cycles formed by objects managed by shared_ptr. | . | Use unique_ptr or shared_ptr to represent ownership. | Prefer unique_ptr over shared_ptr unless you need to share ownership. | Use make_unique to make unique_ptrs. | Use make_shared to make shared_ptrs. | Take smart pointers as parameters only to explicitly express lifetime semantics. | For general use, take T* or T&amp; arguments rather than smart pointers. | . ",
    "url": "/developers/CppCodingStyle.html#resource-management",
    
    "relUrl": "/developers/CppCodingStyle.html#resource-management"
  },"15": {
    "doc": "CPP Code Style",
    "title": "Exception",
    "content": ". | The exception specifications are changing always. The difference between various CPP standards is big, so we should use exception cautiously in Gluten. | Prefer return code to throwing exceptions. | Prefer compile-time checking to run-time checking. | Encapsulate messy constructors, rather than spreading through the code. | . ",
    "url": "/developers/CppCodingStyle.html#exception",
    
    "relUrl": "/developers/CppCodingStyle.html#exception"
  },"16": {
    "doc": "CPP Code Style",
    "title": "Code Comment",
    "content": ". | Add necessary comments. The comment is not the more the better, also not the less the better. | Good comment makes obscure code easily understood. It’s unnecessary to add comments for quite obvious code. | . ",
    "url": "/developers/CppCodingStyle.html#code-comment",
    
    "relUrl": "/developers/CppCodingStyle.html#code-comment"
  },"17": {
    "doc": "CPP Code Style",
    "title": "References",
    "content": ". | CppCoreGuidelines | Velox CODING_STYLE | Thanks Gluten developers for their wise suggestions and helps. | . ",
    "url": "/developers/CppCodingStyle.html#references",
    
    "relUrl": "/developers/CppCodingStyle.html#references"
  },"18": {
    "doc": "CPP Code Style",
    "title": "CPP Code Style",
    "content": " ",
    "url": "/developers/CppCodingStyle.html",
    
    "relUrl": "/developers/CppCodingStyle.html"
  },"19": {
    "doc": "Getting Started with ClickHouse Backend",
    "title": "ClickHouse Backend",
    "content": "ClickHouse is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP), which supports best in the industry query performance, while significantly reducing storage requirements through its innovative use of columnar storage and compression. We port ClickHouse ( based on version 23.1 ) as a library, called ‘libch.so’, and Gluten loads this library through JNI as the native engine. In this way, we don’t need to deploy a standalone ClickHouse Cluster, Spark uses Gluten as SparkPlugin to read and write ClickHouse MergeTree data. Architecture . The architecture of the ClickHouse backend is shown below: . | On Spark driver, Spark uses Gluten SparkPlugin to transform the physical plan to the Substrait plan, and then pass the Substrait plan to ClickHouse backend through JNI call on executors. | Based on Spark DataSource V2 interface, implementing a ClickHouse Catalog to support operating the ClickHouse tables, and then using Delta to save some metadata about ClickHouse like the MergeTree parts information, and also provide ACID transactions. | When querying from a ClickHouse table, it will fetch MergeTree parts information from Delta metadata and assign these parts into Spark partitions according to some strategies. | When writing data into a ClickHouse table, it will use ClickHouse library to write MergeTree parts data and collect these MergeTree parts information after writing successfully, and then save these MergeTree parts information into Delta metadata. ( The feature of writing MergeTree parts is coming soon. ) | On Spark executors, each executor will load the ‘libch.so’ through JNI when starting, and then call the operators according to the Substrait plan which is passed from Spark Driver, like reading data from the MergeTree parts, writing the MergeTree parts, filtering data, aggregating data and so on. | Currently, the ClickHouse backend only supports reading the MergeTree parts from local storage, it needs to use a high-performance shared file system to share a root bucket on every node of the cluster from the object storage, like JuiceFS. | . Development environment setup . In general, we use IDEA for Gluten development and CLion for ClickHouse backend development on Ubuntu 20. Prerequisites . Install the software required for compilation, run sudo ./ep/build-clickhouse/src/install_ubuntu.sh. Under the hood, it will install the following software: . | Clang 16.0 | cmake 3.20 or higher version | ninja-build 1.8.2 | . You can also refer to How-to-Build-ClickHouse-on-Linux. You need to install the following software manually: . | Java 8 | Maven 3.6.3 or higher version | Spark 3.2.2 or Spark 3.3.1 | . Then, get Gluten code: . git clone https://github.com/oap-project/gluten.git . Setup ClickHouse backend development environment . If you don’t care about development environment, you can skip this part. Otherwise, do: . | clone Kyligence/ClickHouse repo cd /to/some/place/ git clone --recursive --shallow-submodules -b clickhouse_backend https://github.com/Kyligence/ClickHouse.git . | Configure cpp-ch ${GLUTEN_SOURCE}/cpp-ch can be treated as an add-on of Kyligence/Clickhouse . First, initialize some configuration for this add-on: . export GLUTEN_SOURCE=/path/to/gluten export CH_SOURCE_DIR=/path/to/ClickHouse cmake -G Ninja -S ${GLUTEN_SOURCE}/cpp-ch -B ${GLUTEN_SOURCE}/cpp-ch/build_ch -DCH_SOURCE_DIR=${CH_SOURCE_DIR} \"-DCMAKE_C_COMPILER=$(command -v clang-16)\" \"-DCMAKE_CXX_COMPILER=$(command -v clang++-16)\" \"-DCMAKE_BUILD_TYPE=RelWithDebInfo\" . Next, you need to compile Kyligence/Clickhouse. There are two options: . | (Option 1) Use CLion . | Open ClickHouse repo | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; Toolchains, and then choose Bundled CMake, clang-16 as C Compiler, clang++-16 as C++ Compiler: . | Choose File -&gt; Settings -&gt; Build, Execution, Deployment -&gt; CMake: . And then add these options into CMake options: . -DENABLE_PROTOBUF=ON -DENABLE_TESTS=OFF -DENABLE_JEMALLOC=ON -DENABLE_MULTITARGET_CODE=ON -DENABLE_EXTERN_LOCAL_ENGINE=ON . | Build ‘ch’ target on ClickHouse Project with Debug mode or Release mode: . If it builds with Release mode successfully, there is a library file called ‘libch.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-release/utils/extern-local-engine/’. If it builds with Debug mode successfully, there is a library file called ‘libchd.so’ in path ‘${CH_SOURCE_DIR}/cmake-build-debug/utils/extern-local-engine/’. | . | (Option 2) Use command line cmake --build ${GLUTEN_SOURCE}/cpp-ch/build_ch --target build_ch . If it builds successfully, there is a library file called ‘libch.so’ in path ‘${GLUTEN_SOURCE}/cpp-ch/build/utils/extern-local-engine/’. | . Directly Compile ClickHouse backend . In case you don’t want a develop environment, you can use the following command to compile ClickHouse backend directly: . git clone https://github.com/oap-project/gluten.git cd gluten bash ./ep/build-clickhouse/src/build_clickhouse.sh . This will download Clickhouse for you and build everything. The target file is /path/to/gluten/cpp-ch/build/utils/extern-local-engine/libch.so. Compile Gluten . The prerequisites are the same as the one mentioned above. Compile Gluten with ClickHouse backend through maven: . | for Spark 3.2.2 | . git clone https://github.com/oap-project/gluten.git cd gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.2 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.2-jar-with-dependencies.jar . | for Spark 3.3.1 | . git clone https://github.com/oap-project/gluten.git cd gluten/ export MAVEN_OPTS=\"-Xmx8g -XX:ReservedCodeCacheSize=2g\" mvn clean install -Pbackends-clickhouse -Phadoop-2.7.4 -Pspark-3.3 -Dhadoop.version=2.8.5 -DskipTests -Dcheckstyle.skip ls -al backends-clickhouse/target/gluten-XXXXX-spark-3.3-jar-with-dependencies.jar . Gluten in local Spark Thrift Server . Prepare working directory . | for Spark 3.2.2 | . tar zxf spark-3.2.2-bin-hadoop2.7.tgz cd spark-3.2.2-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.0.1.jar and delta-storage-2.0.1.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.0.1/delta-core_2.12-2.0.1.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.0.1/delta-storage-2.0.1.jar -P ./jars cp gluten-XXXXX-spark-3.2-jar-with-dependencies.jar jars/ . | for Spark 3.3.1 | . tar zxf spark-3.3.1-bin-hadoop2.7.tgz cd spark-3.3.1-bin-hadoop2.7 rm -f jars/protobuf-java-2.5.0.jar #download protobuf-java-3.23.4.jar, delta-core_2.12-2.2.0.jar and delta-storage-2.2.0.jar wget https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.23.4/protobuf-java-3.23.4.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar -P ./jars wget https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar -P ./jars cp gluten-XXXXX-spark-3.3-jar-with-dependencies.jar jars/ . Query local data . Start Spark Thriftserver on local . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master local[3] \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 #connect to Spark Thriftserver by beeline bin/beeline -u jdbc:hive2://localhost:10000/ -n root . Query local MergeTree files . | Prepare data | . Currently, the feature of writing ClickHouse MergeTree parts by Spark is developing, so you need to use command ‘clickhouse-local’ to generate MergeTree parts data manually. We provide a python script to call the command ‘clickhouse-local’ to convert parquet data to MergeTree parts: . #install ClickHouse community version sudo apt-get install -y apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754 echo \"deb https://packages.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt install -y --allow-downgrades clickhouse-server=22.5.1.2079 clickhouse-client=22.5.1.2079 clickhouse-common-static=22.5.1.2079 #generate MergeTree parts mkdir -p /path_clickhouse_database/table_path/ python3 /path_to_clickhouse_backend_src/utils/local-engine/tool/parquet_to_mergetree.py --path=/tmp --source=/path_to_parquet_data/tpch-data-sf100/lineitem --dst=/path_clickhouse_database/table_path/lineitem . This python script will convert one parquet data file to one MergeTree parts. | Create a TPC-H lineitem table using ClickHouse DataSource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION '/path_clickhouse_database/table_path/lineitem'; . | TPC-H Q6 test | . SELECT sum(l_extendedprice * l_discount) AS revenue FROM lineitem WHERE l_shipdate &gt;= date'1994-01-01' AND l_shipdate &lt; date'1994-01-01' + interval 1 year AND l_discount BETWEEN 0.06 - 0.01 AND 0.06 + 0.01 AND l_quantity &lt; 24; . | Result . The DAG is shown on Spark UI as below: . | . Query local Parquet files . You can query local parquet files directly. -- query on a single file select * from parquet.`/your_data_root_dir/1.parquet`; -- query on a directly which has multiple files select * from parquet.`/your_data_roo_dir/`; . You can also create a TEMPORARY VIEW for parquet files. create or replace temporary view your_table_name using org.apache.spark.sql.parquet options( path \"/your_data_root_dir/\" ) . Query Parquet files in S3 . If you have parquet files in S3(either AWS S3 or S3 compatible storages like MINIO), you can query them directly. You need to add these additional configs to spark: . --config spark.hadoop.fs.s3a.endpoint=S3_ENDPOINT --config spark.hadoop.fs.s3a.path.style.access=true --config spark.hadoop.fs.s3a.access.key=YOUR_ACCESS_KEY --config spark.hadoop.fs.s3a.secret.key=YOUR_SECRET_KEY . where S3_ENDPOINT must follow the format of https://s3.region-code.amazonaws.com, e.g. https://s3.us-east-1.amazonaws.com (or `http://hostname:39090 for MINIO) . When you query the parquet files in S3, you need to add the prefix s3a:// to the path, e.g. s3a://your_bucket_name/path_to_your_parquet. Additionally, you can add these configs to enable local caching of S3 data. Each spark executor will have its own cache. Cache stealing between executors is not supported yet. --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.enabled=true --config spark.gluten.sql.columnar.backend.ch.runtime_config.s3.local_cache.cache_path=/executor_local_folder_for_cache . Use beeline to execute queries . After start a spark thriftserver, we can use the beeline to connect to this server. # run a file /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -f &lt;your_sql_file&gt; # run a query /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 -e '&lt;your_sql&gt;' # enter a interactive mode /path_to_spark/bin/beeline -u jdbc:hive2://localhost:10000 . Query Hive tables in HDFS . Suppose that you have set up hive and hdfs, you can query the data on hive directly. | Copy hive-site.xml into /path_to_spark/conf/ | Copy hdfs-site.xml into /path_to_spark/conf/, and edit spark-env.sh | . # add this line into spark-env.sh export HADOOP_CONF_DIR=/path_to_spark/conf . | Start spark thriftserver with hdfs configurations | . hdfs_conf_file=/your_local_path/hdfs-site.xml cd spark-3.2.2-bin-hadoop2.7 # add a new option: spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf ./sbin/start-thriftserver.sh \\ --master local[3] \\ --files $hdfs_conf_file \\ --driver-memory 10g \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.default.parallelism=1 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=6442450944 \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.executorEnv.LD_PRELOAD=/path_to_clickhouse_library/libch.so\\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=./hdfs-site.xml . For example, you have a table demo_database.demo_table on the hive, you can run queries as below. select * from demo_database.demo_talbe; . Gluten in YARN cluster . We can to run a Spark SQL task by gluten on a yarn cluster as following . #!/bin/bash # The file contains the sql you want to run sql_file=/path/to/spark/sql/file export SPARK_HOME=/path/to/spark/home spark_cmd=$SPARK_HOME/bin/spark-sql # Define the path to libch.so ch_lib=/path/to/libch.so export LD_PRELOAD=$ch_lib # copy gluten jar file to $SPARK_HOME/jar gluten_jar=/path/to/gluten/jar/file cp $gluten_jar $SPARK_HOME/jar batchsize=20480 hdfs_conf=/path/to/hdfs-site.xml $spark_cmd \\ --name gluten_on_yarn --master yarn \\ --deploy-mode client \\ --files $ch_lib \\ --executor-cores 1 \\ --num-executors 2 \\ --executor-memory 10g \\ --conf spark.default.parallelism=4 \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=7g \\ --conf spark.driver.maxResultSize=2g \\ --conf spark.sql.autoBroadcastJoinThreshold=-1 \\ --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} \\ --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} \\ --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} \\ --conf spark.sql.broadcastTimeout=4800 \\ --conf spark.task.maxFailures=1 \\ --conf spark.excludeOnFailure.enabled=false \\ --conf spark.driver.maxResultSize=4g \\ --conf spark.sql.adaptive.enabled=false \\ --conf spark.dynamicAllocation.executorIdleTimeout=0s \\ --conf spark.sql.shuffle.partitions=112 \\ --conf spark.sql.sources.useV1SourceList=avro \\ --conf spark.sql.files.maxPartitionBytes=1073741824 \\ --conf spark.gluten.sql.columnar.columnartorow=true \\ --conf spark.gluten.sql.columnar.loadnative=true \\ --conf spark.gluten.sql.columnar.libpath=$ch_lib \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.hdfs.libhdfs3_conf=$hdfs_conf \\ --conf spark.gluten.sql.columnar.backend.ch.runtime_config.logger.level=debug \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.executorEnv.LD_PRELOAD=$LD_PRELOAD \\ --conf spark.hadoop.input.connect.timeout=600000 \\ --conf spark.hadoop.input.read.timeout=600000 \\ --conf spark.hadoop.input.write.timeout=600000 \\ --conf spark.hadoop.dfs.client.log.severity=\"DEBUG2\" \\ --files $ch_lib \\ -f $sql_file . We also can use spark-submit to run a task. Benchmark with TPC-H 100 Q6 on Gluten with ClickHouse backend . This benchmark is tested on AWS EC2 cluster, there are 7 EC2 instances: . | Node Role | EC2 Type | Instances Count | Resources | AMI | . | Master | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . | Worker | m5.4xlarge | 1 | 16 cores 64G memory per node | ubuntu-focal-20.04 | . Deploy on Cloud . | Tested on Spark Standalone cluster, its resources are shown below: . |   | CPU cores | Memory | Instances Count | . | Spark Worker | 15 | 60G | 6 | . | Prepare jars . Refer to Deploy Spark 3.2.2 . | Deploy gluten-core-XXXXX-jar-with-dependencies.jar . | . #deploy 'gluten-core-XXXXX-jar-with-dependencies.jar' to every node, and then cp gluten-core-XXXXX-jar-with-dependencies.jar /path_to_spark/jars/ . | Deploy ClickHouse library . Deploy ClickHouse library ‘libch.so’ to every worker node. | . Deploy JuiceFS . | JuiceFS uses Redis to save metadata, install redis firstly: | . wget https://download.redis.io/releases/redis-6.0.14.tar.gz sudo apt install build-essential tar -zxvf redis-6.0.14.tar.gz cd redis-6.0.14 make make install PREFIX=/home/ubuntu/redis6 cd .. rm -rf redis-6.0.14 #start redis server /home/ubuntu/redis6/bin/redis-server /home/ubuntu/redis6/redis.conf . | Use JuiceFS to format a S3 bucket and mount a volumn on every node . Please refer to The-JuiceFS-Command-Reference . | . wget https://github.com/juicedata/juicefs/releases/download/v0.17.5/juicefs-0.17.5-linux-amd64.tar.gz tar -zxvf juicefs-0.17.5-linux-amd64.tar.gz ./juicefs format --block-size 4096 --storage s3 --bucket https://s3.cn-northwest-1.amazonaws.com.cn/s3-gluten-tpch100/ --access-key \"XXXXXXXX\" --secret-key \"XXXXXXXX\" redis://:123456@master-ip:6379/1 gluten-tables #mount a volumn on every node ./juicefs mount -d --no-usage-report --no-syslog --attr-cache 7200 --entry-cache 7200 --dir-entry-cache 7200 --buffer-size 500 --prefetch 1 --open-cache 86400 --log /home/ubuntu/juicefs-logs/mount1.log --cache-dir /home/ubuntu/juicefs-cache/ --cache-size 102400 redis://:123456@master-ip:6379/1 /home/ubuntu/gluten/gluten_table #create a directory for lineitem table path mkdir -p /home/ubuntu/gluten/gluten_table/lineitem . Preparation . Please refer to Data-preparation to generate MergeTree parts data to the lineitem table path: /home/ubuntu/gluten/gluten_table/lineitem. Run Spark Thriftserver . cd spark-3.2.2-bin-hadoop2.7 ./sbin/start-thriftserver.sh \\ --master spark://master-ip:7070 --deploy-mode client \\ --driver-memory 16g --driver-cores 4 \\ --total-executor-cores 90 --executor-memory 60g --executor-cores 15 \\ --conf spark.driver.memoryOverhead=8G \\ --conf spark.default.parallelism=90 \\ --conf spark.sql.shuffle.partitions=1 \\ --conf spark.sql.files.minPartitionNum=1 \\ --conf spark.sql.files.maxPartitionBytes=536870912 \\ --conf spark.sql.parquet.filterPushdown=true \\ --conf spark.sql.parquet.enableVectorizedReader=true \\ --conf spark.locality.wait=0 \\ --conf spark.locality.wait.node=0 \\ --conf spark.locality.wait.process=0 \\ --conf spark.sql.columnVector.offheap.enabled=true \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=42949672960 \\ --conf spark.serializer=org.apache.spark.serializer.JavaSerializer \\ --conf spark.sql.sources.ignoreDataLocality=true \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.gluten.sql.columnar.columnarToRow=true \\ --conf spark.gluten.sql.columnar.libpath=/path_to_clickhouse_library/libch.so \\ --conf spark.gluten.sql.columnar.iterator=true \\ --conf spark.gluten.sql.columnar.loadarrow=false \\ --conf spark.gluten.sql.columnar.hashagg.enablefinal=true \\ --conf spark.gluten.sql.enable.native.validation=false \\ --conf spark.io.compression.codec=snappy \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.execution.datasources.v2.clickhouse.ClickHouseSparkCatalog \\ --conf spark.databricks.delta.maxSnapshotLineageLength=20 \\ --conf spark.databricks.delta.snapshotPartitions=1 \\ --conf spark.databricks.delta.properties.defaults.checkpointInterval=5 \\ --conf spark.databricks.delta.stalenessLimit=3600000 . Test TPC-H Q6 with JMeter . | Create a lineitem table using clickhouse datasource | . DROP TABLE IF EXISTS lineitem; CREATE TABLE IF NOT EXISTS lineitem ( l_orderkey bigint, l_partkey bigint, l_suppkey bigint, l_linenumber bigint, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate date, l_commitdate date, l_receiptdate date, l_shipinstruct string, l_shipmode string, l_comment string) USING clickhouse TBLPROPERTIES (engine='MergeTree' ) LOCATION 'file:///home/ubuntu/gluten/gluten_table/lineitem'; . | Run TPC-H Q6 test with JMeter . | Run TPC-H Q6 test 100 times in the first round; | Run TPC-H Q6 test 1000 times in the second round; | . | . Performance . The performance of Gluten + ClickHouse backend increases by about 1/3. |   | 70% | 80% | 90% | 99% | Avg | . | Spark + Parquet | 590ms | 592ms | 597ms | 609ms | 588ms | . | Spark + Gluten + ClickHouse backend | 402ms | 405ms | 409ms | 425ms | 399ms | . New CI System . https://opencicd.kyligence.com/job/Gluten/job/gluten-ci/ public read-only account：gluten/hN2xX3uQ4m . Celeborn support . Gluten with clickhouse backend has not yet supportted Celeborn natively as remote shuffle service using columar shuffle. However, you can still use Celeborn with row shuffle, which means a ColumarBatch will be converted to a row during shuffle. Below introduction is used to enable this feature: . First refer to this URL(https://github.com/apache/incubator-celeborn) to setup a celeborn cluster. Then add the Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-0.3.0-incubating.jar | . Currently to use Celeborn following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.celeborn.SparkShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/incubator-celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . Celeborn Columnar Shuffle Support . The native Celeborn support can be enabled by the following configuration . spark.shuffle.manager=org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager . quickly start a celeborn cluster . wget https://dlcdn.apache.org/incubator/celeborn/celeborn-0.3.0-incubating/apache-celeborn-0.3.0-incubating-bin.tgz &amp;&amp; \\ tar -zxvf apache-celeborn-0.3.0-incubating-bin.tgz &amp;&amp; \\ mv apache-celeborn-0.3.0-incubating-bin/conf/celeborn-defaults.conf.template apache-celeborn-0.3.0-incubating-bin/conf/celeborn-defaults.conf &amp;&amp; \\ mv apache-celeborn-0.3.0-incubating-bin/conf/log4j2.xml.template apache-celeborn-0.3.0-incubating-bin/conf/log4j2.xml &amp;&amp; \\ mkdir /opt/hadoop &amp;&amp; chmod 777 /opt/hadoop &amp;&amp; \\ echo -e \"celeborn.worker.flusher.threads 4\\nceleborn.worker.storage.dirs /tmp\\nceleborn.worker.monitor.disk.enabled false\" &gt; apache-celeborn-0.3.0-incubating-bin/conf/celeborn-defaults.conf &amp;&amp; \\ bash apache-celeborn-0.3.0-incubating-bin/sbin/start-master.sh &amp;&amp; bash apache-celeborn-0.3.0-incubating-bin/sbin/start-worker.sh . Columnar shuffle mode . We have two modes of columnar shuffle . | prefer cache | prefer spill | . Switch through the configuration spark.gluten.sql.columnar.backend.ch.shuffle.preferSpill, the default is false, enable prefer cache shuffle. In the prefer cache mode, as much memory as possible will be used to cache the shuffle data. When the memory is insufficient, spark will actively trigger the memory spill. You can also specify the threshold size through spark.gluten.sql.columnar.backend.ch.spillThreshold to Limit memory usage. The default value is 0MB, which means no limit on memory usage. ",
    "url": "/docs/clickhouse/getting-started#clickhouse-backend",
    
    "relUrl": "/docs/clickhouse/getting-started#clickhouse-backend"
  },"20": {
    "doc": "Getting Started with ClickHouse Backend",
    "title": "Getting Started with ClickHouse Backend",
    "content": " ",
    "url": "/docs/clickhouse/getting-started",
    
    "relUrl": "/docs/clickhouse/getting-started"
  },"21": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Supported Version",
    "content": "| Type | Version |——-|——————————| Spark | 3.2.2, 3.3.1 | OS | Ubuntu20.04/22.04, Centos7/8 | jdk | openjdk8 | scala | 2.12 . Spark3.4.0 support is still WIP. TPCH/DS can pass, UT is not yet passed. There are pending PRs for jdk11 support. Currently, the mvn script can automatically fetch and build all dependency libraries incluing Velox. Our nightly build still use Velox under oap-project. ",
    "url": "/docs/velox/getting-started#supported-version",
    
    "relUrl": "/docs/velox/getting-started#supported-version"
  },"22": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Prerequisite",
    "content": "Currently, Gluten+Velox backend is only tested on Ubuntu20.04/Ubuntu22.04/Centos8. Other kinds of OS support are still in progress. The long term goal is to support several common OS and conda env deployment. Gluten builds with Spark3.2.x and Spark3.3.x now but only fully tested in CI with 3.2.2 and 3.3.1. We will add/update supported/tested versions according to the upstream changes. we need to set up the JAVA_HOME env. Currently, java 8 is required and the support for java 11/17 is not ready. For x86_64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH . For aarch64 . ## make sure jdk8 is used export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64 export PATH=$JAVA_HOME/bin:$PATH . Get gluten . ## config maven, like proxy in ~/.m2/settings.xml ## fetch gluten code git clone https://github.com/oap-project/gluten.git . ",
    "url": "/docs/velox/getting-started#prerequisite",
    
    "relUrl": "/docs/velox/getting-started#prerequisite"
  },"23": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Build Gluten with Velox Backend",
    "content": "It’s recommended to use buildbundle-veloxbe.sh to build gluten in one script. Gluten build guide listed the parameters and their default value of build command for your reference. For x86_64 build . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh ## After a complete build, if you need to re-build the project and only some gluten code is changed, ## you can use the following command to skip building velox and protobuf. # ./dev/buildbundle-veloxbe.sh --enable_ep_cache=ON --build_protobuf=OFF ## If you have the same error with issue-3283, you need to add the parameter `--compile_arrow_java=ON` . For aarch64 build: . export CPU_TARGET=\"aarch64\" cd /path/to/gluten ./dev/builddeps-veloxbe.sh . Build Velox separately . Scripts under /path/to/gluten/ep/build-velox/src provide get_velox.sh and build_velox.sh to build Velox separately, you could use these scripts with custom repo/branch/location. Velox provides arrow/parquet lib. Gluten cpp module need a required VELOX_HOME parsed by –velox_home, if you specify custom ep location, make sure these variables be passed correctly. ## fetch Velox and compile cd /path/to/gluten/ep/build-velox/src/ ## you could use custom ep location by --velox_home=custom_path, make sure specify --velox_home in build_velox.sh too./get_velox.sh ## make sure specify --velox_home if you have specified it in get_velox.sh./build_velox.sh ## compile Gluten cpp module cd /path/to/gluten/cpp ## if you use custom velox_home, make sure specified here by --velox_home ./compile.sh --build_velox_backend=ON ## compile Gluten java module and create package jar cd /path/to/gluten # For spark3.2.x mvn clean package -Pbackends-velox -Prss -Pspark-3.2 -DskipTests # For spark3.3.x mvn clean package -Pbackends-velox -Prss -Pspark-3.3 -DskipTests . notes：The compilation of Velox using the script of build_velox.sh may fail caused by oom, you can prevent this failure by using the user command of export NUM_THREADS=4 before executing the above scripts. Once building successfully, the Jar file will be generated in the directory: package/target/&lt;gluten-jar&gt; for Spark 3.2.x/Spark 3.3.x. ",
    "url": "/docs/velox/getting-started#build-gluten-with-velox-backend",
    
    "relUrl": "/docs/velox/getting-started#build-gluten-with-velox-backend"
  },"24": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Dependency library deployment",
    "content": "With config enable_vcpkg=ON, the dependency libraries will be built and statically linked into libvelox.so and libgluten.so, which is packed into the gluten-jar. In this way, only the gluten-jar is needed to add to spark.&lt;driver|executor&gt;.extraClassPath and spark will deploy the jar to each worker node. It’s better to build the static version using a clean docker image without any extra libraries installed. On host with some libraries like jemalloc installed, the script may crash with odd message. You may need to uninstall those libraries to get a clean host. With config enable_vcpkg=OFF, the dependency libraries won’t be statically linked, instead the script will install the libraries to system then pack the dependency libraries into another jar named gluten-package-${Maven-artifact-version}.jar. Then you need to add the jar to extraClassPath then set spark.gluten.loadLibFromJar=true. Or you already manually deployed the dependency libraries on each worker node. You may find the libraries list from the gluten-package jar. ",
    "url": "/docs/velox/getting-started#dependency-library-deployment",
    
    "relUrl": "/docs/velox/getting-started#dependency-library-deployment"
  },"25": {
    "doc": "Getting-Started with Velox Backend",
    "title": "HDFS support",
    "content": "Hadoop hdfs support is ready via the libhdfs3 library. The libhdfs3 provides native API for Hadoop I/O without the drawbacks of JNI. It also provides advanced authentication like Kerberos based. Please note this library has several dependencies which may require extra installations on Driver and Worker node. Build with HDFS support . To build Gluten with HDFS support, below command is suggested: . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_hdfs=ON . Configuration about HDFS support . HDFS uris (hdfs://host:port) will be extracted from a valid hdfs file path to initialize hdfs client, you do not need to specify it explicitly. libhdfs3 need a configuration file and example here, this file is a bit different from hdfs-site.xml and core-site.xml. Download that example config file to local and do some needed modifications to support HA or else, then set env variable like below to use it, or upload it to HDFS to use, more details here. // Spark local mode export LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode --conf spark.executorEnv.LIBHDFS3_CONF=\"/path/to/hdfs-client.xml\" // Spark Yarn cluster mode and upload hdfs config file cp /path/to/hdfs-client.xml hdfs-client.xml --files hdfs-client.xml . One typical deployment on Spark/HDFS cluster is to enable short-circuit reading. Short-circuit reads provide a substantial performance boost to many applications. By default libhdfs3 does not set the default hdfs domain socket path to support HDFS short-circuit read. If this feature is required in HDFS setup, users may need to setup the domain socket path correctly by patching the libhdfs3 source code or by setting the correct config environment. In Gluten the short-circuit domain socket path is set to “/var/lib/hadoop-hdfs/dn_socket” in build_velox.sh So we need to make sure the folder existed and user has write access as below script. sudo mkdir -p /var/lib/hadoop-hdfs/ sudo chown &lt;sparkuser&gt;:&lt;sparkuser&gt; /var/lib/hadoop-hdfs/ . You also need to add configuration to the “hdfs-site.xml” as below: . &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/lib/hadoop-hdfs/dn_socket&lt;/value&gt; &lt;/property&gt; . Kerberos support . Here are two steps to enable kerberos. | Make sure the hdfs-client.xml contains | . &lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;/property&gt; . | Specify the environment variable KRB5CCNAME and upload the kerberos ticket cache file | . --conf spark.executorEnv.KRB5CCNAME=krb5cc_0000 --files /tmp/krb5cc_0000 . The ticket cache file can be found by klist. ",
    "url": "/docs/velox/getting-started#hdfs-support",
    
    "relUrl": "/docs/velox/getting-started#hdfs-support"
  },"26": {
    "doc": "Getting-Started with Velox Backend",
    "title": "AWS S3 support",
    "content": "Velox supports S3 with the open source AWS C++ SDK and Gluten uses Velox S3 connector to connect with S3. A new build option for S3(enable_s3) is added. Below command is used to enable this feature . cd /path/to/gluten ./dev/buildbundle-veloxbe.sh --enable_s3=ON . Currently there are several ways to asscess S3 in Spark. Please refer Velox S3 part for more detailed configurations . ",
    "url": "/docs/velox/getting-started#aws-s3-support",
    
    "relUrl": "/docs/velox/getting-started#aws-s3-support"
  },"27": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Celeborn support",
    "content": "Gluten with velox backend supports Celeborn as remote shuffle service. Below introduction is used to enable this feature . First refer to this URL(https://github.com/apache/incubator-celeborn) to setup a celeborn cluster. When compiling the Gluten Java module, it’s required to enable rss profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Prss -DskipTests . Then add the Gluten and Spark Celeborn Client packages to your Spark application’s classpath(usually add them into $SPARK_HOME/jars). | Celeborn: celeborn-client-spark-3-shaded_2.12-0.3.0-incubating.jar | Gluten: gluten-velox-bundle-spark3.x_2.12-xx_xx_xx-SNAPSHOT.jar, gluten-thirdparty-lib-xx-xx.jar | . Currently to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs://&lt;namenode&gt;/celeborn # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/incubator-celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false . ",
    "url": "/docs/velox/getting-started#celeborn-support",
    
    "relUrl": "/docs/velox/getting-started#celeborn-support"
  },"28": {
    "doc": "Getting-Started with Velox Backend",
    "title": "DeltaLake Support",
    "content": "Gluten with velox backend supports DeltaLake table. How to use . First of all, compile gluten-delta module by a delta profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Pdelta -DskipTests . Then, put the additional gluten-delta jar to the class path (usually it’s $SPARK_HOME/jars). The gluten-delta jar is in gluten-delta/target directory. After the two steps, you can query delta table by gluten/velox without scan’s fallback. Gluten with velox backends also support the column mapping of delta tables. About column mapping, see more here. ",
    "url": "/docs/velox/getting-started#deltalake-support",
    
    "relUrl": "/docs/velox/getting-started#deltalake-support"
  },"29": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Iceberg Support",
    "content": "Gluten with velox backend supports Iceberg table. Currently, only reading COW (Copy-On-Write) tables is supported. How to use . First of all, compile gluten-iceberg module by a iceberg profile, as follows: . mvn clean package -Pbackends-velox -Pspark-3.3 -Piceberg -DskipTests . Then, put the additional gluten-iceberg jar to the class path (usually it’s $SPARK_HOME/jars). The gluten-iceberg jar is in gluten-iceberg/target directory. After the two steps, you can query iceberg table by gluten/velox without scan’s fallback. ",
    "url": "/docs/velox/getting-started#iceberg-support",
    
    "relUrl": "/docs/velox/getting-started#iceberg-support"
  },"30": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Coverage",
    "content": "Spark3.3 has 387 functions in total. ~240 are commonly used. Velox’s functions have two category, Presto and Spark. Presto has 124 functions implemented. Spark has 62 functions. Spark functions are verified to have the same result as Vanilla Spark. Some Presto functions have the same result as Vanilla Spark but some others have different. Gluten prefer to use Spark functions firstly. If it’s not in Spark’s list but implemented in Presto, we currently offload to Presto one until we noted some result mismatch, then we need to reimplement the function in Spark category. Gluten currently offloads 94 functions and 14 operators, more details refer to Velox Backend’s Supported Operators &amp; Functions. Velox doesn’t support ANSI mode), so as Gluten. Once ANSI mode is enabled in Spark config, Gluten will fallback to Vanilla Spark. To identify what can be offloaded in a query and detailed fallback reasons, user can follow below steps to retrieve corresponding logs. 1) Enable Gluten by proper [configuration](https://github.com/oap-project/gluten/blob/main/docs/Configuration.md). 2) Disable Spark AQE to trigger plan validation in Gluten spark.sql.adaptive.enabled = false 3) Check physical plan sparkSession.sql(\"your_sql\").explain() . With above steps, you will get a physical plan output like: . == Physical Plan == -Execute InsertIntoHiveTable (7) +- Coalesce (6) +- VeloxColumnarToRowExec (5) +- ^ ProjectExecTransformer (3) +- GlutenRowToArrowColumnar (2) +- Scan hive default.extracted_db_pins (1) . GlutenRowToArrowColumnar/VeloxColumnarToRowExec indicates there is a fallback operator before or after it. And you may find fallback reason like below in logs. native validation failed due to: in ProjectRel, Scalar function name not registered: get_struct_field, called with arguments: (ROW&lt;col_0:INTEGER,col_1:BIGINT,col_2:BIGINT&gt;, INTEGER). In the above, the symbol ^ indicates a plan is offloaded to Velox in a stage. In Spark DAG, all such pipelined plans (consecutive plans marked with ^) are plotted inside an umbrella node named WholeStageCodegenTransformer (It’s not codegen node. The naming is just for making it well plotted like Spark Whole Stage Codegen). ",
    "url": "/docs/velox/getting-started#coverage",
    
    "relUrl": "/docs/velox/getting-started#coverage"
  },"31": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Spill (Experimental)",
    "content": "Velox backend supports spilling-to-disk. Using the following configuration options to customize spilling: . | Name | Default Value | Description | . | spark.gluten.sql.columnar.backend.velox.spillStrategy | auto | none: Disable spill on Velox backend; auto: Let Spark memory manager manage Velox’s spilling | . | spark.gluten.sql.columnar.backend.velox.spillFileSystem | local | The filesystem used to store spill data. local: The local file system. heap-over-local: Write files to JVM heap if having extra heap space. Otherwise write to local file system. | . | spark.gluten.sql.columnar.backend.velox.aggregationSpillEnabled | true | Whether spill is enabled on aggregations | . | spark.gluten.sql.columnar.backend.velox.joinSpillEnabled | true | Whether spill is enabled on joins | . | spark.gluten.sql.columnar.backend.velox.orderBySpillEnabled | true | Whether spill is enabled on sorts | . | spark.gluten.sql.columnar.backend.velox.aggregationSpillMemoryThreshold | 0 | Memory limit before spilling to disk for aggregations, per Spark task. Unit: byte | . | spark.gluten.sql.columnar.backend.velox.joinSpillMemoryThreshold | 0 | Memory limit before spilling to disk for joins, per Spark task. Unit: byte | . | spark.gluten.sql.columnar.backend.velox.orderBySpillMemoryThreshold | 0 | Memory limit before spilling to disk for sorts, per Spark task. Unit: byte | . | spark.gluten.sql.columnar.backend.velox.maxSpillLevel | 4 | The max allowed spilling level with zero being the initial spilling level | . | spark.gluten.sql.columnar.backend.velox.maxSpillFileSize | 20MB | The max allowed spill file size. If it is zero, then there is no limit | . | spark.gluten.sql.columnar.backend.velox.minSpillRunSize | 268435456 | The min spill run size limit used to select partitions for spilling | . | spark.gluten.sql.columnar.backend.velox.spillStartPartitionBit | 29 | The start partition bit which is used with ‘spillPartitionBits’ together to calculate the spilling partition number | . | spark.gluten.sql.columnar.backend.velox.spillPartitionBits | 2 | The number of bits used to calculate the spilling partition number. The number of spilling partitions will be power of two | . | spark.gluten.sql.columnar.backend.velox.spillableReservationGrowthPct | 25 | The spillable memory reservation growth percentage of the previous memory reservation size | . ",
    "url": "/docs/velox/getting-started#spill-experimental",
    
    "relUrl": "/docs/velox/getting-started#spill-experimental"
  },"32": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Velox User-Defined Functions (UDF)",
    "content": " ",
    "url": "/docs/velox/getting-started#velox-user-defined-functions-udf",
    
    "relUrl": "/docs/velox/getting-started#velox-user-defined-functions-udf"
  },"33": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Introduction",
    "content": "Velox backend supports User-Defined Functions (UDF). Users can create their own functions using the UDF interface provided in Velox backend and build libraries for these functions. At runtime, the UDF are registered at the start of applications. Once registered, Gluten will be able to parse and offload these UDF into Velox during execution. ",
    "url": "/docs/velox/getting-started#introduction",
    
    "relUrl": "/docs/velox/getting-started#introduction"
  },"34": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Creating a UDF library",
    "content": "The following steps demonstrate how to set up a UDF library project: . | Include the UDF Interface Header: First, include the UDF interface header file Udf.h in the project file. The header file defines the UdfEntry struct, along with the macros for declaring the necessary functions to integrate the UDF into Gluten and Velox. | Implement the UDF: Implement UDF. These functions should be able to register to Velox. | Implement the Interface Functions: Implement the following interface functions that integrate UDF into Project Gluten: . | getNumUdf(): This function should return the number of UDF in the library. This is used to allocating udfEntries array as the argument for the next function getUdfEntries. | getUdfEntries(gluten::UdfEntry* udfEntries): This function should populate the provided udfEntries array with the details of the UDF, including function names and return types. | registerUdf(): This function is called to register the UDF to Velox function registry. This is where users should register functions by calling facebook::velox::exec::registerVecotorFunction or other Velox APIs. | The interface functions are mapping to marcos in Udf.h. Here’s an example of how to implement these functions: . | . // Filename MyUDF.cpp #include &lt;velox/expression/VectorFunction.h&gt; #include &lt;velox/udf/Udf.h&gt; const int kNumMyUdf = 1; gluten::UdfEntry myUdf[kNumMyUdf] = myudf1; class MyUdf : public facebook::velox::exec::VectorFunction { ... // Omit concrete implementation } static std::vector&lt;std::shared_ptr&lt;exec::FunctionSignature&gt;&gt; myUdfSignatures() { return {facebook::velox::exec::FunctionSignatureBuilder() .returnType(myUdf[0].dataType) .argumentType(\"integer\") .build()}; } DEFINE_GET_NUM_UDF { return kNumMyUdf; } DEFINE_GET_UDF_ENTRIES { udfEntries[0] = myUdf[0]; } DEFINE_REGISTER_UDF { facebook::velox::exec::registerVectorFunction( myUdf[0].name, myUdfSignatures(), std::make_unique&lt;MyUdf&gt;()); } . | . ",
    "url": "/docs/velox/getting-started#creating-a-udf-library",
    
    "relUrl": "/docs/velox/getting-started#creating-a-udf-library"
  },"35": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Building the UDF library",
    "content": "To build the UDF library, users need to compile the C++ code and link to libvelox.so. It’s recommended to create a CMakeLists.txt for the project. Here’s an example: . project(myudf) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(GLUTEN_HOME /path/to/gluten) add_library(myudf SHARED \"MyUDF.cpp\") find_library(VELOX_LIBRARY REQUIRED NAMES velox HINTS ${GLUTEN_HOME}/cpp/build/releases NO_DEFAULT_PATH) target_include_directories(myudf PRIVATE ${GLUTEN_HOME}/cpp ${GLUTEN_HOME}/ep/build-velox/build/velox_ep) target_link_libraries(myudf PRIVATE ${VELOX_LIBRARY}) . ",
    "url": "/docs/velox/getting-started#building-the-udf-library",
    
    "relUrl": "/docs/velox/getting-started#building-the-udf-library"
  },"36": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Using UDF in Gluten",
    "content": "Gluten loads the UDF libraries at runtime. You can upload UDF libraries via --files or --archives, and configure the libray paths using the provided Spark configuration, which accepts comma separated list of library paths. Note if running on Yarn client mode, the uploaded files are not reachable on driver side. Users should copy those files to somewhere reachable for driver and set spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths. This configuration is also useful when the udfLibraryPaths is different between driver side and executor side. | Use --files --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/libmyudf.so . | Use --archives --archives /path/to/udf_archives.zip#udf_archives --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=udf_archives # Needed for Yarn client mode --conf spark.gluten.sql.columnar.backend.velox.driver.udfLibraryPaths=file:///path/to/udf_archives.zip . | Specify URI | . You can also specify the local or HDFS URIs to the UDF libraries or archives. Local URIs should exist on driver and every worker nodes. --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=hdfs://path/to/library_or_archive . ",
    "url": "/docs/velox/getting-started#using-udf-in-gluten",
    
    "relUrl": "/docs/velox/getting-started#using-udf-in-gluten"
  },"37": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Try the example",
    "content": "We provided an Velox UDF example file MyUDF.cpp. After building gluten cpp, you can find the example library at /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so . Start spark-shell or spark-sql with below configuration . --files /path/to/gluten/cpp/build/velox/udf/examples/libmyudf.so --conf spark.gluten.sql.columnar.backend.velox.udfLibraryPaths=libmyudf.so . Run query. The functions myudf1 and myudf2 increment the input value by a constant of 5 . select myudf1(1), myudf2(100L) . The output from spark-shell will be like . +----------------+------------------+ |udfexpression(1)|udfexpression(100)| +----------------+------------------+ | 6| 105| +----------------+------------------+ . ",
    "url": "/docs/velox/getting-started#try-the-example",
    
    "relUrl": "/docs/velox/getting-started#try-the-example"
  },"38": {
    "doc": "Getting-Started with Velox Backend",
    "title": "High-Bandwidth Memory (HBM) support",
    "content": "Gluten supports allocating memory on HBM. This feature is optional and is disabled by default. It is implemented on top of Memkind library. You can refer to memkind’s readme for more details. ",
    "url": "/docs/velox/getting-started#high-bandwidth-memory-hbm-support",
    
    "relUrl": "/docs/velox/getting-started#high-bandwidth-memory-hbm-support"
  },"39": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Build Gluten with HBM",
    "content": "Gluten will internally build and link to a specific version of Memkind library and hwloc. Other dependencies should be installed on Driver and Worker node first: . sudo apt install -y autoconf automake g++ libnuma-dev libtool numactl unzip libdaxctl-dev . After the set-up, you can now build Gluten with HBM. Below command is used to enable this feature . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh --enable_hbm=ON . ",
    "url": "/docs/velox/getting-started#build-gluten-with-hbm",
    
    "relUrl": "/docs/velox/getting-started#build-gluten-with-hbm"
  },"40": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Configure and enable HBM in Spark Application",
    "content": "At runtime, MEMKIND_HBW_NODES enviroment variable is detected for configuring HBM NUMA nodes. For the explaination to this variable, please refer to memkind’s manual page. This can be set for all executors through spark conf, e.g. --conf spark.executorEnv.MEMKIND_HBW_NODES=8-15. Note that memory allocation fallback is also supported and cannot be turned off. If HBM is unavailable or fills up, the allocator will use default(DDR) memory. ",
    "url": "/docs/velox/getting-started#configure-and-enable-hbm-in-spark-application",
    
    "relUrl": "/docs/velox/getting-started#configure-and-enable-hbm-in-spark-application"
  },"41": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Intel® QuickAssist Technology (QAT) support",
    "content": "Gluten supports using Intel® QuickAssist Technology (QAT) for data compression during Spark Shuffle. It benefits from QAT Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on QAT driver library and QATzip library. Please manually download QAT driver for your system, and follow its README to build and install on all Driver and Worker node: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. ",
    "url": "/docs/velox/getting-started#intel-quickassist-technology-qat-support",
    
    "relUrl": "/docs/velox/getting-started#intel-quickassist-technology-qat-support"
  },"42": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Software Requirements",
    "content": ". | Download QAT driver for your system, and follow its README to build and install on all Driver and Worker nodes: Intel® QuickAssist Technology Driver for Linux* – HW Version 2.0. | Below compression libraries need to be installed on all Driver and Worker nodes: . | Zlib* library of version 1.2.7 or higher | ZSTD* library of version 1.5.4 or higher | LZ4* library | . | . ",
    "url": "/docs/velox/getting-started#software-requirements",
    
    "relUrl": "/docs/velox/getting-started#software-requirements"
  },"43": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Build Gluten with QAT",
    "content": ". | Setup ICP_ROOT environment variable to the directory where QAT driver is extracted. This environment variable is required during building Gluten and running Spark applications. It’s recommended to put it in .bashrc on Driver and Worker nodes. | . echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc source ~/.bashrc # Also set for root if running as non-root user sudo su - echo \"export ICP_ROOT=/path/to/QAT_driver\" &gt;&gt; ~/.bashrc exit . | This step is required if your application is running as Non-root user. The users must be added to the ‘qat’ group after QAT drvier is installed. And change the amount of max locked memory for the username that is included in the group name. This can be done by specifying the limit in /etc/security/limits.conf. | . sudo su - usermod -aG qat username # need relogin to take effect # To set 500MB add a line like this in /etc/security/limits.conf echo \"@qat - memlock 500000\" &gt;&gt; /etc/security/limits.conf exit . | Enable huge page. This step is required to execute each time after system reboot. We recommend using systemctl to manage at system startup. You change the values for “max_huge_pages” and “max_huge_pages_per_process” to make sure there are enough resources for your workload. As for Spark applications, one process matches one executor. Within the executor, every task is allocated a maximum of 5 huge pages. | . sudo su - cat &lt;&lt; EOF &gt; /usr/local/bin/qat_startup.sh #!/bin/bash echo 1024 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages rmmod usdm_drv insmod $ICP_ROOT/build/usdm_drv.ko max_huge_pages=1024 max_huge_pages_per_process=32 EOF chmod +x /usr/local/bin/qat_startup.sh cat &lt;&lt; EOF &gt; /etc/systemd/system/qat_startup.service [Unit] Description=Configure QAT [Service] ExecStart=/usr/local/bin/qat_startup.sh [Install] WantedBy=multi-user.target EOF systemctl enable qat_startup.service systemctl start qat_startup.service # setup immediately systemctl status qat_startup.service exit . | After the setup, you are now ready to build Gluten with QAT. Use the command below to enable this feature: | . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh --enable_qat=ON . ",
    "url": "/docs/velox/getting-started#build-gluten-with-qat",
    
    "relUrl": "/docs/velox/getting-started#build-gluten-with-qat"
  },"44": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Enable QAT with Gzip/Zstd for shuffle compression",
    "content": ". | To offload shuffle compression into QAT, first make sure you have the right QAT configuration file at /etc/4xxx_devX.conf. We provide a example configuration file. This configuration sets up to 4 processes that can bind to 1 QAT, and each process can use up to 16 QAT DC instances. | . ## run as root ## Overwrite QAT configuration file. cd /etc for i in {0..7}; do echo \"4xxx_dev$i.conf\"; done | xargs -i cp -f /path/to/gluten/docs/qat/4x16.conf {} ## Restart QAT after updating configuration files. adf_ctl restart . | Check QAT status and make sure the status is up | . adf_ctl status . The output should be like: . Checking status of all devices. There is 8 QAT acceleration device(s) in the system: qat_dev0 - type: 4xxx, inst_id: 0, node_id: 0, bsf: 0000:6b:00.0, #accel: 1 #engines: 9 state: up qat_dev1 - type: 4xxx, inst_id: 1, node_id: 1, bsf: 0000:70:00.0, #accel: 1 #engines: 9 state: up qat_dev2 - type: 4xxx, inst_id: 2, node_id: 2, bsf: 0000:75:00.0, #accel: 1 #engines: 9 state: up qat_dev3 - type: 4xxx, inst_id: 3, node_id: 3, bsf: 0000:7a:00.0, #accel: 1 #engines: 9 state: up qat_dev4 - type: 4xxx, inst_id: 4, node_id: 4, bsf: 0000:e8:00.0, #accel: 1 #engines: 9 state: up qat_dev5 - type: 4xxx, inst_id: 5, node_id: 5, bsf: 0000:ed:00.0, #accel: 1 #engines: 9 state: up qat_dev6 - type: 4xxx, inst_id: 6, node_id: 6, bsf: 0000:f2:00.0, #accel: 1 #engines: 9 state: up qat_dev7 - type: 4xxx, inst_id: 7, node_id: 7, bsf: 0000:f7:00.0, #accel: 1 #engines: 9 state: up . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip # Valid options are gzip and zstd --conf spark.gluten.sql.columnar.shuffle.codecBackend=qat . | You can use below command to check whether QAT is working normally at run-time. The value of fw_counters should continue to increase during shuffle. | . while :; do cat /sys/kernel/debug/qat_4xxx_0000:6b:00.0/fw_counters; sleep 1; done . ",
    "url": "/docs/velox/getting-started#enable-qat-with-gzipzstd-for-shuffle-compression",
    
    "relUrl": "/docs/velox/getting-started#enable-qat-with-gzipzstd-for-shuffle-compression"
  },"45": {
    "doc": "Getting-Started with Velox Backend",
    "title": "QAT driver references",
    "content": "Documentation . README Text Files (README_QAT20.L.1.0.0-00021.txt) . Release Notes . Check out the Intel® QuickAssist Technology Software for Linux* - Release Notes for the latest changes in this release. Getting Started Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Getting Started Guide for detailed installation instructions. Programmer’s Guide . Check out the Intel® QuickAssist Technology Software for Linux* - Programmer’s Guide for software usage guidelines. For more Intel® QuickAssist Technology resources go to Intel® QuickAssist Technology (Intel® QAT) . ",
    "url": "/docs/velox/getting-started#qat-driver-references",
    
    "relUrl": "/docs/velox/getting-started#qat-driver-references"
  },"46": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Intel® In-memory Analytics Accelerator (IAA/IAX) support",
    "content": "Similar to Intel® QAT, Gluten supports using Intel® In-memory Analytics Accelerator (IAA, also called IAX) for data compression during Spark Shuffle. It benefits from IAA Hardware-based acceleration on compression/decompression, and uses Gzip as compression format for higher compression ratio to reduce the pressure on disks and network transmission. This feature is based on Intel® QPL. ",
    "url": "/docs/velox/getting-started#intel-in-memory-analytics-accelerator-iaaiax-support",
    
    "relUrl": "/docs/velox/getting-started#intel-in-memory-analytics-accelerator-iaaiax-support"
  },"47": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Build Gluten with IAA",
    "content": "Gluten will internally build and link to a specific version of QPL library, but extra environment setup is still required. Please refer to QPL Installation Guide to install dependencies and configure accelerators. This step is required if your application is running as Non-root user. Create a group for the users who have privilege to use IAA, and grant group iaa read/write access to the IAA Work-Queues. sudo groupadd iaa sudo usermod -aG iaa username # need to relogin sudo chgrp -R iaa /dev/iax sudo chmod -R g+rw /dev/iax . After the set-up, you can now build Gluten with QAT. Below command is used to enable this feature . cd /path/to/gluten ## The script builds two jars for spark 3.2.2 and 3.3.1./dev/buildbundle-veloxbe.sh --enable_iaa=ON . ",
    "url": "/docs/velox/getting-started#build-gluten-with-iaa",
    
    "relUrl": "/docs/velox/getting-started#build-gluten-with-iaa"
  },"48": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Enable IAA with Gzip Compression for shuffle compression",
    "content": ". | To enable QAT at run-time, first make sure you have configured the IAA Work-Queues correctly, and the file permissions of /dev/iax/wqX.0 are correct. | . sudo ls -l /dev/iax . The output should be like: . total 0 crw-rw---- 1 root iaa 509, 0 Apr 5 18:54 wq1.0 crw-rw---- 1 root iaa 509, 5 Apr 5 18:54 wq11.0 crw-rw---- 1 root iaa 509, 6 Apr 5 18:54 wq13.0 crw-rw---- 1 root iaa 509, 7 Apr 5 18:54 wq15.0 crw-rw---- 1 root iaa 509, 1 Apr 5 18:54 wq3.0 crw-rw---- 1 root iaa 509, 2 Apr 5 18:54 wq5.0 crw-rw---- 1 root iaa 509, 3 Apr 5 18:54 wq7.0 crw-rw---- 1 root iaa 509, 4 Apr 5 18:54 wq9.0 . | Extra Gluten configurations are required when starting Spark application | . --conf spark.gluten.sql.columnar.shuffle.codec=gzip --conf spark.gluten.sql.columnar.shuffle.codecBackend=iaa . ",
    "url": "/docs/velox/getting-started#enable-iaa-with-gzip-compression-for-shuffle-compression",
    
    "relUrl": "/docs/velox/getting-started#enable-iaa-with-gzip-compression-for-shuffle-compression"
  },"49": {
    "doc": "Getting-Started with Velox Backend",
    "title": "IAA references",
    "content": "Intel® IAA Enabling Guide . Check out the Intel® In-Memory Analytics Accelerator (Intel® IAA) Enabling Guide . Intel® QPL Documentation . Check out the Intel® Query Processing Library (Intel® QPL) Documentation . ",
    "url": "/docs/velox/getting-started#iaa-references",
    
    "relUrl": "/docs/velox/getting-started#iaa-references"
  },"50": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Test TPC-H or TPC-DS on Gluten with Velox backend",
    "content": "All TPC-H and TPC-DS queries are supported in Gluten Velox backend. ",
    "url": "/docs/velox/getting-started#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend",
    
    "relUrl": "/docs/velox/getting-started#test-tpc-h-or-tpc-ds-on-gluten-with-velox-backend"
  },"51": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Data preparation",
    "content": "The data generation scripts are TPC-H dategen script and TPC-DS dategen script. The used TPC-H and TPC-DS queries are the original ones, and can be accessed from TPC-DS queries and TPC-H queries. Some other versions of TPC-DS queries are also provided, but are not recommended for testing, including: . | the modified TPC-DS queries with “Decimal-to-Double”: TPC-DS non-decimal queries (outdated). | . ",
    "url": "/docs/velox/getting-started#data-preparation",
    
    "relUrl": "/docs/velox/getting-started#data-preparation"
  },"52": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Submit the Spark SQL job",
    "content": "Submit test script from spark-shell. You can find the scala code to Run TPC-H as an example. Please remember to modify the location of TPC-H files as well as TPC-H queries before you run the testing. var parquet_file_path = \"/PATH/TO/TPCH_PARQUET_PATH\" var gluten_root = \"/PATH/TO/GLUTEN\" . Below script shows an example about how to run the testing, you should modify the parameters such as executor cores, memory, offHeap size based on your environment. export GLUTEN_JAR = /PATH/TO/GLUTEN/package/target/&lt;gluten-jar&gt; cat tpch_parquet.scala | spark-shell --name tpch_powertest_velox \\ --master yarn --deploy-mode client \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.driver.extraClassPath=${GLUTEN_JAR} \\ --conf spark.executor.extraClassPath=${GLUTEN_JAR} \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --conf spark.gluten.sql.columnar.forceShuffledHashJoin=true \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager \\ --num-executors 6 \\ --executor-cores 6 \\ --driver-memory 20g \\ --executor-memory 25g \\ --conf spark.executor.memoryOverhead=5g \\ --conf spark.driver.maxResultSize=32g . Refer to Gluten configuration for more details. ",
    "url": "/docs/velox/getting-started#submit-the-spark-sql-job",
    
    "relUrl": "/docs/velox/getting-started#submit-the-spark-sql-job"
  },"53": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Result",
    "content": "wholestagetransformer indicates that the offload works. ",
    "url": "/docs/velox/getting-started#result",
    
    "relUrl": "/docs/velox/getting-started#result"
  },"54": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Performance",
    "content": "Below table shows the TPC-H Q1 and Q6 Performance in a multiple-thread test (–num-executors 6 –executor-cores 6) for Velox and vanilla Spark. Both Parquet and ORC datasets are sf1024. | Query Performance (s) | Velox (ORC) | Vanilla Spark (Parquet) | Vanilla Spark (ORC) | . | TPC-H Q6 | 13.6 | 21.6 | 34.9 | . | TPC-H Q1 | 26.1 | 76.7 | 84.9 | . ",
    "url": "/docs/velox/getting-started#performance",
    
    "relUrl": "/docs/velox/getting-started#performance"
  },"55": {
    "doc": "Getting-Started with Velox Backend",
    "title": "External reference setup",
    "content": "TO ease your first-hand experience of using Gluten, we have set up an external reference cluster. If you are interested, please contact Weiting.Chen@intel.com. ",
    "url": "/docs/velox/getting-started#external-reference-setup",
    
    "relUrl": "/docs/velox/getting-started#external-reference-setup"
  },"56": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Gluten UI",
    "content": " ",
    "url": "/docs/velox/getting-started#gluten-ui",
    
    "relUrl": "/docs/velox/getting-started#gluten-ui"
  },"57": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Gluten event",
    "content": "Gluten provides two events GlutenBuildInfoEvent and GlutenPlanFallbackEvent: . | GlutenBuildInfoEvent, it contains the Gluten build information so that we are able to be aware of the environment when doing some debug. It includes Java Version, Scala Version, GCC Version, Gluten Version, Spark Version, Hadoop Version, Gluten Revision, Backend, Backend Revision, etc. | GlutenPlanFallbackEvent, it contains the fallback information for each query execution. Note, if the query execution is in AQE, then Gluten will post it for each stage. | . Developers can register SparkListener to handle these two Gluten events. ",
    "url": "/docs/velox/getting-started#gluten-event",
    
    "relUrl": "/docs/velox/getting-started#gluten-event"
  },"58": {
    "doc": "Getting-Started with Velox Backend",
    "title": "SQL tab",
    "content": "Gluten provides a tab based on Spark UI, named Gluten SQL / DataFrame . This tab contains two parts: . | The Gluten build information. | SQL/Dataframe queries fallback information. | . If you want to disable Gluten UI, add a config when submitting --conf spark.gluten.ui.enabled=false. ",
    "url": "/docs/velox/getting-started#sql-tab",
    
    "relUrl": "/docs/velox/getting-started#sql-tab"
  },"59": {
    "doc": "Getting-Started with Velox Backend",
    "title": "History server",
    "content": "Gluten UI also supports Spark history server. Add gluten-ui jar into the history server classpath, e.g., $SPARK_HOME/jars, then restart history server. ",
    "url": "/docs/velox/getting-started#history-server",
    
    "relUrl": "/docs/velox/getting-started#history-server"
  },"60": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Native plan string",
    "content": "Gluten supports inject native plan string into Spark explain with formatted mode by setting --conf spark.gluten.sql.injectNativePlanStringToExplain=true. Here is an example, how Gluten show the native plan string. (9) WholeStageCodegenTransformer (2) Input [6]: [c1#0L, c2#1L, c3#2L, c1#3L, c2#4L, c3#5L] Arguments: false Native Plan: -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT . ",
    "url": "/docs/velox/getting-started#native-plan-string",
    
    "relUrl": "/docs/velox/getting-started#native-plan-string"
  },"61": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Native plan with stats",
    "content": "Gluten supports print native plan with stats to executor system output stream by setting --conf spark.gluten.sql.debug=true. Note that, the plan string with stats is task level which may cause executor log size big. Here is an example, how Gluten show the native plan string with stats. I20231121 10:19:42.348845 90094332 WholeStageResultIterator.cc:220] Native Plan with stats for: [Stage: 1 TID: 16] -- Project[expressions: (n3_6:BIGINT, \"n0_0\"), (n3_7:BIGINT, \"n0_1\"), (n3_8:BIGINT, \"n0_2\"), (n3_9:BIGINT, \"n1_0\"), (n3_10:BIGINT, \"n1_1\"), (n3_11:BIGINT, \"n1_2\")] -&gt; n3_6:BIGINT, n3_7:BIGINT, n3_8:BIGINT, n3_9:BIGINT, n3_10:BIGINT, n3_11:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 10.58us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 queuedWallNanos sum: 2.00us, count: 1, min: 2.00us, max: 2.00us runningAddInputWallNanos sum: 626ns, count: 1, min: 626ns, max: 626ns runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 5.54us, count: 1, min: 5.54us, max: 5.54us -- HashJoin[INNER n1_1=n0_1] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT, n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Output: 27 rows (3.56KB, 3 batches), Cpu time: 223.00us, Blocked wall time: 0ns, Peak memory: 93.12KB, Memory allocations: 15 HashBuild: Input: 10 rows (960B, 10 batches), Output: 0 rows (0B, 0 batches), Cpu time: 185.67us, Blocked wall time: 0ns, Peak memory: 68.00KB, Memory allocations: 2, Threads: 1 distinctKey0 sum: 4, count: 1, min: 4, max: 4 hashtable.capacity sum: 4, count: 1, min: 4, max: 4 hashtable.numDistinct sum: 10, count: 1, min: 10, max: 10 hashtable.numRehashes sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 4, count: 1, min: 4, max: 4 runningAddInputWallNanos sum: 1.27ms, count: 1, min: 1.27ms, max: 1.27ms runningFinishWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningGetOutputWallNanos sum: 1.29us, count: 1, min: 1.29us, max: 1.29us H23/11/21 10:19:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 13) in 335 ms on 10.221.97.35 (executor driver) (1/10) ashProbe: Input: 9 rows (864B, 3 batches), Output: 27 rows (3.56KB, 3 batches), Cpu time: 37.33us, Blocked wall time: 0ns, Peak memory: 25.12KB, Memory allocations: 13, Threads: 1 dynamicFiltersProduced sum: 1, count: 1, min: 1, max: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningAddInputWallNanos sum: 4.54us, count: 1, min: 4.54us, max: 4.54us runningFinishWallNanos sum: 83ns, count: 1, min: 83ns, max: 83ns runningGetOutputWallNanos sum: 29.08us, count: 1, min: 29.08us, max: 29.08us -- TableScan[table: hive_table, range filters: [(c2, Filter(IsNotNull, deterministic, null not allowed))]] -&gt; n1_0:BIGINT, n1_1:BIGINT, n1_2:BIGINT Input: 9 rows (864B, 3 batches), Output: 9 rows (864B, 3 batches), Cpu time: 630.75us, Blocked wall time: 0ns, Peak memory: 2.44KB, Memory allocations: 63, Threads: 1, Splits: 3 dataSourceWallNanos sum: 102.00us, count: 1, min: 102.00us, max: 102.00us dynamicFiltersAccepted sum: 1, count: 1, min: 1, max: 1 flattenStringDictionaryValues sum: 0, count: 1, min: 0, max: 0 ioWaitNanos sum: 312.00us, count: 1, min: 312.00us, max: 312.00us localReadBytes sum: 0B, count: 1, min: 0B, max: 0B numLocalRead sum: 0, count: 1, min: 0, max: 0 numPrefetch sum: 0, count: 1, min: 0, max: 0 numRamRead sum: 0, count: 1, min: 0, max: 0 numStorageRead sum: 6, count: 1, min: 6, max: 6 overreadBytes sum: 0B, count: 1, min: 0B, max: 0B prefetchBytes sum: 0B, count: 1, min: 0B, max: 0B queryThreadIoLatency sum: 12, count: 1, min: 12, max: 12 ramReadBytes sum: 0B, count: 1, min: 0B, max: 0B runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 125ns, count: 1, min: 125ns, max: 125ns runningGetOutputWallNanos sum: 1.07ms, count: 1, min: 1.07ms, max: 1.07ms skippedSplitBytes sum: 0B, count: 1, min: 0B, max: 0B skippedSplits sum: 0, count: 1, min: 0, max: 0 skippedStrides sum: 0, count: 1, min: 0, max: 0 storageReadBytes sum: 3.44KB, count: 1, min: 3.44KB, max: 3.44KB totalScanTime sum: 0ns, count: 1, min: 0ns, max: 0ns -- ValueStream[] -&gt; n0_0:BIGINT, n0_1:BIGINT, n0_2:BIGINT Input: 0 rows (0B, 0 batches), Output: 10 rows (960B, 10 batches), Cpu time: 1.03ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 runningAddInputWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns runningFinishWallNanos sum: 54.62us, count: 1, min: 54.62us, max: 54.62us runningGetOutputWallNanos sum: 1.10ms, count: 1, min: 1.10ms, max: 1.10ms . ",
    "url": "/docs/velox/getting-started#native-plan-with-stats",
    
    "relUrl": "/docs/velox/getting-started#native-plan-with-stats"
  },"62": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Gluten Implicits",
    "content": "Gluten provides a helper class to get the fallback summary from a Spark Dataset. import org.apache.spark.sql.execution.GlutenImplicits._ val df = spark.sql(\"SELECT * FROM t\") df.fallbackSummary . Note that, if AQE is enabled, but the query is not materialized, then it will re-plan the query execution with disabled AQE. It is a workaround to get the final plan, and it may cause the inconsistent results with a materialized query. However, we have no choice. ",
    "url": "/docs/velox/getting-started#gluten-implicits",
    
    "relUrl": "/docs/velox/getting-started#gluten-implicits"
  },"63": {
    "doc": "Getting-Started with Velox Backend",
    "title": "Getting-Started with Velox Backend",
    "content": " ",
    "url": "/docs/velox/getting-started",
    
    "relUrl": "/docs/velox/getting-started"
  },"64": {
    "doc": "How To Use Gluten",
    "title": "How to understand the key work of Gluten?",
    "content": "The Gluten worked as the role of bridge, it’s a middle layer between the Spark and the native execution library. The Gluten is responsibility for validating whether the operators of the Spark plan can be executed by the native engine or not. If yes, the Gluten transforms Spark plan to Substrait plan, and then send the Substrait plan to the native engine. The Gluten codes consist of two parts: the C++ codes and the Java/Scala codes. | All C++ codes are placed under the directory of gluten_home/cpp, the Java/Scala codes are placed under several directories, such as gluten_home/gluten-core gluten_home/gluten-data gluten_home/backends-velox. | The Java/Scala codes are responsibility for validating and transforming the execution plan. Source data should also be provided, the source data may come from files or other forms such as networks. | The C++ codes take the Substrait plan and the source data as inputs and transform the Substrait plan to the corresponding backend plan. If the backend is Velox, the Substrait plan will be transformed to the Velox plan, and then be executed. | . JNI is a programming technology of invoking C++ from Java. All JNI interfaces are defined in the file JniWrapper.cc under the directory jni. ",
    "url": "/developers/HowTo.html#how-to-understand-the-key-work-of-gluten",
    
    "relUrl": "/developers/HowTo.html#how-to-understand-the-key-work-of-gluten"
  },"65": {
    "doc": "How To Use Gluten",
    "title": "How to debug in Gluten?",
    "content": " ",
    "url": "/developers/HowTo.html#how-to-debug-in-gluten",
    
    "relUrl": "/developers/HowTo.html#how-to-debug-in-gluten"
  },"66": {
    "doc": "How To Use Gluten",
    "title": "1 How to debug C++",
    "content": "If you don’t concern about the Scala/Java codes and just want to debug the C++ codes executed in native engine, you may debug the C++ via benchmarks with GDB. To debug C++, you have to generate the example files, the example files consist of: . | A file contained Substrait plan in JSON format | One or more input data files in Parquet format | . You can generate the example files by the following steps: . | build Velox and Gluten CPP gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . | Compiling with --build_type=Debug is good for debugging. | The executable file generic_benchmark will be generated under the directory of gluten_home/cpp/build/velox/benchmarks/. | . | build Gluten and generate the example files cd gluten_home mvn clean package -Pspark-3.2 -Pbackends-velox -Prss mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . | After the above operations, the examples files are generated under gluten_home/backends-velox | You can check it by the command tree gluten_home/backends-velox/generated-native-benchmark/ | You may replace -Pspark-3.2 with -Pspark-3.3 if your spark’s version is 3.3 $ tree gluten_home/backends-velox/generated-native-benchmark/ gluten_home/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . | . | now, run benchmarks with GDB cd gluten_home/cpp/build/velox/benchmarks/ gdb generic_benchmark . | When GDB load generic_benchmark successfully, you can set breakpoint on the main function with command b main, and then run with command r, then the process generic_benchmark will start and stop at the main function. | You can check the variables’ state with command p variable_name, or execute the program line by line with command n, or step-in the function been called with command s. | Actually, you can debug generic_benchmark with any gdb commands as debugging normal C++ program, because the generic_benchmark is a pure C++ executable file in fact. | . | gdb-tui is a valuable feature and is worth trying. You can get more help from the online docs. gdb-tui . | you can start generic_benchmark with specific JSON plan and input files . | If you omit them, the example.json, example_lineitem + example_orders under the directory of gluten_home/backends-velox/generated-native-benchmark will be used as default. | You can also edit the file example.json to custom the Substrait plan or specify the inputs files placed in the other directory. | . | get more detail information about benchmarks from MicroBenchmarks | . ",
    "url": "/developers/HowTo.html#1-how-to-debug-c",
    
    "relUrl": "/developers/HowTo.html#1-how-to-debug-c"
  },"67": {
    "doc": "How To Use Gluten",
    "title": "2 How to debug plan validation process",
    "content": "Gluten will validate generated plan before execute it, and validation usually happens in native side, so we provide a utility to help debug validation process in native side. | Run query with conf spark.gluten.sql.debug=true, and you will find generated plan be printed in stderr with json format, save it as plan.json for example. | Compile cpp part with --build_benchmarks=ON, then check plan_validator_util executable file in gluten_home/cpp/build/velox/benchmarks/. | Run or debug with ./plan_validator_util &lt;path&gt;/plan.json | . ",
    "url": "/developers/HowTo.html#2-how-to-debug-plan-validation-process",
    
    "relUrl": "/developers/HowTo.html#2-how-to-debug-plan-validation-process"
  },"68": {
    "doc": "How To Use Gluten",
    "title": "3 How to debug Java/Scala",
    "content": "wait to add . ",
    "url": "/developers/HowTo.html#3-how-to-debug-javascala",
    
    "relUrl": "/developers/HowTo.html#3-how-to-debug-javascala"
  },"69": {
    "doc": "How To Use Gluten",
    "title": "4 How to debug with core-dump",
    "content": "wait to complete . cd the_directory_of_core_file_generated gdb gluten_home/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . | the core-Executor task l-2000883-1671542526 represents the core file name. | . ",
    "url": "/developers/HowTo.html#4-how-to-debug-with-core-dump",
    
    "relUrl": "/developers/HowTo.html#4-how-to-debug-with-core-dump"
  },"70": {
    "doc": "How To Use Gluten",
    "title": "How to run TPC-H on Velox backend",
    "content": "Now, both Parquet and DWRF format files are supported, related scripts and files are under the directory of gluten_home/backends-velox/workload/tpch. The file README.md under gluten_home/backends-velox/workload/tpch offers some useful help but it’s still not enough and exact. One way of run TPC-H test is to run velox-be by workflow, you can refer to velox_be.yml . Here will explain how to run TPC-H on Velox backend with the Parquet file format. | First step, prepare the datasets, you have two choices. | One way, generate Parquet datasets using the script under gluten_home/backends-velox/workload/tpch/gen_data/parquet_dataset, You can get help from the above mentioned README.md. | The other way, using the small dataset under gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox directly, If you just want to make simple TPC-H testing, this dataset is a good choice. | . | Second step, run TPC-H on Velox backend testing. | Modify gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.scala. | . | set var parquet_file_path to correct directory. If using the small dataset directly in the step one, then modify it as below var parquet_file_path = \"gluten_home/backends-velox/src/test/resources/tpch-data-parquet-velox\" . | set var gluten_root to correct directory. If gluten_home is the directory of /home/gluten, then modify it as below var gluten_root = \"/home/gluten\" . - Modify `gluten_home/backends-velox/workload/tpch/run_tpch/tpch_parquet.sh`. | Set GLUTEN_JAR correctly. Please refer to the section of Build Gluten with Velox Backend | Set SPARK_HOME correctly. | Set the memory configurations appropriately. - Execute tpch_parquet.sh using the below command. | cd gluten_home/backends-velox/workload/tpch/run_tpch/ | ./tpch_parquet.sh | . | . ",
    "url": "/developers/HowTo.html#how-to-run-tpc-h-on-velox-backend",
    
    "relUrl": "/developers/HowTo.html#how-to-run-tpc-h-on-velox-backend"
  },"71": {
    "doc": "How To Use Gluten",
    "title": "How to run TPC-DS",
    "content": "wait to add . ",
    "url": "/developers/HowTo.html#how-to-run-tpc-ds",
    
    "relUrl": "/developers/HowTo.html#how-to-run-tpc-ds"
  },"72": {
    "doc": "How To Use Gluten",
    "title": "How to track the memory exhaust problem",
    "content": "When your gluten spark jobs failed because of OOM, you can track the memory allocation’s call stack by configuring spark.gluten.backtrace.allocation = true. The above configuration will use BacktraceAllocationListener wrapping from SparkAllocationListener to create VeloxMemoryManager. BacktraceAllocationListener will check every allocation, if a single allocation bytes exceeds a fixed value or the accumulative allocation bytes exceeds 1/2/3…G, the call stack of memory allocation will be outputted to standard output, you can check the backtrace and get some valuable information about tracking the memory exhaust issues. You can also adjust the policy to decide when to backtrace, such as the fixed value. ",
    "url": "/developers/HowTo.html#how-to-track-the-memory-exhaust-problem",
    
    "relUrl": "/developers/HowTo.html#how-to-track-the-memory-exhaust-problem"
  },"73": {
    "doc": "How To Use Gluten",
    "title": "How To Use Gluten",
    "content": "There are some common questions about developing, debugging and testing been asked again and again. In order to help the developers to contribute to Gluten as soon as possible, we collected these frequently asked questions, and organized them in the form of Q&amp;A. It’s convenient for the developers to check and learn. when you encountered a new problem and then resolved it, please add a new item to this document if you think it may be helpful to the other developers. We use gluten_home to represent the home directory of Gluten in this document. ",
    "url": "/developers/HowTo.html",
    
    "relUrl": "/developers/HowTo.html"
  },"74": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Generate Micro Benchmarks for Velox Backend",
    "content": "This document explains how to use the existing micro benchmark template in Gluten Cpp. A micro benchmark for Velox backend is provided in Gluten Cpp to simulate the execution of a first or middle stage in Spark. It serves as a more convenient alternative to debug in Gluten Cpp comparing with directly debugging in a Spark job. Developers can use it to create their own workloads, debug in native process, profile the hotspot and do optimizations. To simulate a first stage, you need to dump the Substrait plan and input split info into two JSON files. The input URIs of the splits should be exising file locations, which can be either local or HDFS paths. To simulate a middle stage, in addition to the JSON file, you also need to save the input data of this stage into Parquet files. The benchmark will load the data into Arrow format, then add Arrow2Velox to feed the data into Velox pipeline to reproduce the reducer stage. Shuffle exchange is not included. Please refer to the sections below to learn how to dump the Substrait plan and create the input data files. ",
    "url": "/developers/MicroBenchmarks.html#generate-micro-benchmarks-for-velox-backend",
    
    "relUrl": "/developers/MicroBenchmarks.html#generate-micro-benchmarks-for-velox-backend"
  },"75": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Try the example",
    "content": "To run a micro benchmark, user should provide one file that contains the Substrait plan in JSON format, and optional one or more input data files in parquet format. The commands below help to generate example input files: . cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_tests=ON --build_benchmarks=ON # Run test to generate input data files. If you are using spark 3.3, replace -Pspark-3.2 with -Pspark-3.3 mvn test -Pspark-3.2 -Pbackends-velox -Prss -pl backends-velox -am \\ -DtagsToInclude=\"io.glutenproject.tags.GenerateExample\" -Dtest=none -DfailIfNoTests=false -Darrow.version=11.0.0-gluten -Dexec.skip . The generated example files are placed in gluten/backends-velox: . $ tree gluten/backends-velox/generated-native-benchmark/ gluten/backends-velox/generated-native-benchmark/ ├── example.json ├── example_lineitem │ ├── part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet │ └── _SUCCESS └── example_orders ├── part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet └── _SUCCESS . Run micro benchmark with the generated files as input. You need to specify the absolute path to the input files: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example.json \\ --data /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_orders/part-00000-1e66fb98-4dd6-47a6-8679-8625dbc437ee-c000.snappy.parquet,\\ /home/sparkuser/github/oap-project/gluten/backends-velox/generated-native-benchmark/example_lineitem/part-00000-3ec19189-d20e-4240-85ae-88631d46b612-c000.snappy.parquet \\ --threads 1 --iterations 1 --noprint-result --benchmark_filter=InputFromBatchStream . The output should be like: . 2022-11-18T16:49:56+08:00 Running ./generic_benchmark Run on (192 X 3800 MHz CPU s) CPU Caches: L1 Data 48 KiB (x96) L1 Instruction 32 KiB (x96) L2 Unified 2048 KiB (x96) L3 Unified 99840 KiB (x2) Load Average: 0.28, 1.17, 1.59 ***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead. -- Project[expressions: (n3_0:BIGINT, ROW[\"n1_0\"]), (n3_1:VARCHAR, ROW[\"n1_1\"])] -&gt; n3_0:BIGINT, n3_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 36.33us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 queuedWallNanos sum: 2.00us, count: 2, min: 0ns, max: 2.00us -- HashJoin[RIGHT SEMI (FILTER) n0_0=n1_0] -&gt; n1_0:BIGINT, n1_1:VARCHAR Output: 535 rows (65.81KB, 1 batches), Cpu time: 191.56us, Blocked wall time: 0ns, Peak memory: 2.00MB, Memory allocations: 8 HashBuild: Input: 582 rows (16.45KB, 1 batches), Output: 0 rows (0B, 0 batches), Cpu time: 1.84us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 3, Threads: 1 distinctKey0 sum: 583, count: 1, min: 583, max: 583 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns rangeKey0 sum: 59748, count: 1, min: 59748, max: 59748 HashProbe: Input: 37897 rows (296.07KB, 1 batches), Output: 535 rows (65.81KB, 1 batches), Cpu time: 189.71us, Blocked wall time: 0ns, Peak memory: 1.00MB, Memory allocations: 5, Threads: 1 queuedWallNanos sum: 0ns, count: 1, min: 0ns, max: 0ns -- ArrowStream[] -&gt; n0_0:BIGINT Input: 0 rows (0B, 0 batches), Output: 37897 rows (296.07KB, 1 batches), Cpu time: 1.29ms, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 -- ArrowStream[] -&gt; n1_0:BIGINT, n1_1:VARCHAR Input: 0 rows (0B, 0 batches), Output: 582 rows (16.45KB, 1 batches), Cpu time: 894.22us, Blocked wall time: 0ns, Peak memory: 0B, Memory allocations: 0, Threads: 1 ----------------------------------------------------------------------------------------------------------------------------- Benchmark Time CPU Iterations UserCounters... ----------------------------------------------------------------------------------------------------------------------------- InputFromBatchVector/iterations:1/process_time/real_time/threads:1 41304520 ns 23740340 ns 1 collect_batch_time=34.7812M elapsed_time=41.3113M . ",
    "url": "/developers/MicroBenchmarks.html#try-the-example",
    
    "relUrl": "/developers/MicroBenchmarks.html#try-the-example"
  },"76": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Generate Substrait plan and input for any query",
    "content": "First, build Gluten with --build_benchmarks=ON. cd /path/to/gluten/ ./dev/buildbundle-veloxbe.sh --build_benchmarks=ON # For debugging purpose, rebuild Gluten with build type `Debug`./dev/buildbundle-veloxbe.sh --build_benchmarks=ON --build_type=Debug . First, get the Stage Id from spark UI for the stage you want to simulate. And then re-run the query with below configurations to dump the inputs to micro benchmark. | Parameters | Description | Recommend Setting | . | spark.gluten.sql.benchmark_task.stageId | Spark task stage id | target stage id | . | spark.gluten.sql.benchmark_task.partitionId | Spark task partition id, default value -1 means all the partition of this stage | 0 | . | spark.gluten.sql.benchmark_task.taskId | If not specify partition id, use spark task attempt id, default value -1 means all the partition of this stage | target task attemp id | . | spark.gluten.saveDir | Directory to save the inputs to micro benchmark, should exist and be empty. | /path/to/saveDir | . Check the files in spark.gluten.saveDir. If the simulated stage is a first stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage, without input file splits. | Split file: JSON formatted, file name split_[stageId]_[partitionId]_[splitIndex].json. There can be more than one split file in a first stage task. Contains the substrait plan piece to the input file splits. | . Run benchmark. By default, the result will be printed to stdout. You can use --noprint-result to suppress this output. Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --split /absolut_path/to/split_[stageId]_[partitionId]_0.parquet,/absolut_path/to/split_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . If the simulated stage is a middle stage, you will get 3 types of dumped file: . | Configuration file: INI formatted, file name conf_[stageId]_[partitionId].ini. Contains the configurations to init Velox backend and runtime session. | Plan file: JSON formatted, file name plan_[stageId]_[partitionId].json. Contains the substrait plan to the stage. | Data file: Parquet formatted, file name data_[stageId]_[partitionId]_[iteratorIndex].json. There can be more than one input data file in a middle stage task. The input data files of a middle stage will be loaded as iterators to serve as the inputs for the pipeline: | . \"localFiles\": { \"items\": [ { \"uriFile\": \"iterator:0\" } ] } . Sample command: . cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --conf /absolute_path/to/conf_[stageId]_[partitionId].ini \\ --plan /absolute_path/to/plan_[stageId]_[partitionId].json \\ --data /absolut_path/to/data_[stageId]_[partitionId]_0.parquet,/absolut_path/to/data_[stageId]_[partitionId]_1.parquet \\ --threads 1 --noprint-result . For some complex queries, stageId may cannot represent the Substrait plan input, please get the taskId from spark UI, and get your target parquet from saveDir. In this example, only one partition input with partition id 2, taskId is 36, iterator length is 2. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/complex_plan.json \\ --data /absolute_path/to/data_36_2_0.parquet,/absolute_path/to/data_36_2_1.parquet \\ --threads 1 --noprint-result . ",
    "url": "/developers/MicroBenchmarks.html#generate-substrait-plan-and-input-for-any-query",
    
    "relUrl": "/developers/MicroBenchmarks.html#generate-substrait-plan-and-input-for-any-query"
  },"77": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Save ouput to parquet to analyze",
    "content": "You can save the output to a parquet file to analyze. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --data /absolute_path/to/data.parquet --threads 1 --noprint-result --write-file=/absolute_path/to/result.parquet . ",
    "url": "/developers/MicroBenchmarks.html#save-ouput-to-parquet-to-analyze",
    
    "relUrl": "/developers/MicroBenchmarks.html#save-ouput-to-parquet-to-analyze"
  },"78": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Add shuffle write process",
    "content": "You can add the shuffle write process at the end of this stage. Note that this will ignore the --write-file option. cd /path/to/gluten/cpp/build/velox/benchmarks ./generic_benchmark \\ --plan /absolute_path/to/plan.json \\ --split /absolute_path/to/split.json \\ --threads 1 --noprint-result --with-shuffle . By default, the compression codec for shuffle outputs is LZ4. You can switch to other codecs by adding one of the following argument flags to the command: . | –zstd: ZSTD codec, compression level 1 | –qat-gzip: QAT GZIP codec, compression level 1 | –qat-zstd: QAT ZSTD codec, compression level 1 | –iaa-gzip: IAA GZIP codec, compression level 1 | . Note using QAT or IAA codec requires Gluten cpp is built with these features. Please check the corresponding section in Velox document first for how to setup, build and enable these features in Gluten. For QAT support, please check Intel® QuickAssist Technology (QAT) support. For IAA support, please check Intel® In-memory Analytics Accelerator (IAA/IAX) support . ",
    "url": "/developers/MicroBenchmarks.html#add-shuffle-write-process",
    
    "relUrl": "/developers/MicroBenchmarks.html#add-shuffle-write-process"
  },"79": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Simulate Spark with multiple processes and threads",
    "content": "You can use below command to launch several processes and threads to simulate parallel execution on Spark. Each thread in the same process will be pinned to the core number starting from --cpu. Suppose running on a baremetal machine with 48C, 2-socket, HT-on, launching below command will utilize all vcores. processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --threads $threads --cpu $((i*threads)) &amp; done . If you want to add the shuffle write process, you can specify multiple direcotries by setting environment variable GLUTEN_SPARK_LOCAL_DIRS to a comma-separated string for shuffle write to spread the I/O pressure to multiple disks. mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/plan.json --split /path/to/split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done . Run Examples . We also provide some example inputs in cpp/velox/benchmarks/data. E.g. generic_q5/q5_first_stage_0.json simulates a first-stage in TPCH Q5, which has the the most heaviest table scan. You can follow below steps to run this example. | Open generic_q5/q5_first_stage_0.json with file editor. Search for \"uriFile\": \"LINEITEM\" and replace LINEITEM with the URI to one partition file in lineitem. In the next line, replace the number in \"length\": \"...\" with the actual file length. Suppose you are using the provided small TPCH table in cpp/velox/benchmarks/data/tpch_sf10m, the replaced JSON should be like: | . { \"items\": [ { \"uriFile\": \"file:///path/to/gluten/cpp/velox/benchmarks/data/tpch_sf10m/lineitem/part-00000-6c374e0a-7d76-401b-8458-a8e31f8ab704-c000.snappy.parquet\", \"length\": \"1863237\", \"parquet\": {} } ] } . | Launch multiple processes and multiple threads. Set GLUTEN_SPARK_LOCAL_DIRS and add –with-shuffle to the command. | . mkdir -p {/data1,/data2,/data3}/tmp # Make sure each directory has been already created. export GLUTEN_SPARK_LOCAL_DIRS=/data1/tmp,/data2/tmp,/data3/tmp processes=24 # Same value of spark.executor.instances threads=8 # Same value of spark.executor.cores for ((i=0; i&lt;${processes}; i++)); do ./generic_benchmark --plan /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0.json --split /path/to/gluten/cpp/velox/benchmarks/data/generic_q5/q5_first_stage_0_split.json --noprint-result --with-shuffle --threads $threads --cpu $((i*threads)) &amp; done &gt;stdout.log 2&gt;stderr.log . You can find the “elapsed_time” and other metrics in stdout.log. In below output, the “elapsed_time” is ~10.75s. If you run TPCH Q5 with Gluten on Spark, a single task in the same Spark stage should take about the same time. ------------------------------------------------------------------------------------------------------------------ Benchmark Time CPU Iterations UserCounters... ------------------------------------------------------------------------------------------------------------------ SkipInput/iterations:1/process_time/real_time/threads:8 1317255379 ns 10061941861 ns 8 collect_batch_time=0 elapsed_time=10.7563G shuffle_compress_time=4.19964G shuffle_spill_time=0 shuffle_split_time=0 shuffle_write_time=1.91651G . ",
    "url": "/developers/MicroBenchmarks.html#simulate-spark-with-multiple-processes-and-threads",
    
    "relUrl": "/developers/MicroBenchmarks.html#simulate-spark-with-multiple-processes-and-threads"
  },"80": {
    "doc": "Micro Benchmarks for Velox Backend",
    "title": "Micro Benchmarks for Velox Backend",
    "content": " ",
    "url": "/developers/MicroBenchmarks.html",
    
    "relUrl": "/developers/MicroBenchmarks.html"
  },"81": {
    "doc": "New To Gluten",
    "title": "Environment",
    "content": "Now gluten supports Ubuntu20.04, Ubuntu22.04, centos8, centos7 and macOS. ",
    "url": "/developers/NewToGluten.html#environment",
    
    "relUrl": "/developers/NewToGluten.html#environment"
  },"82": {
    "doc": "New To Gluten",
    "title": "Openjdk8",
    "content": "Environment setting . For root user, the environment variables file is /etc/profile, it will make effect for all the users. For other user, you can set in ~/.bashrc. Guide for ubuntu . The default JDK version in ubuntu is java11, we need to set to java8. apt install openjdk-8-jdk update-alternatives --config java java -version . --config java to config java executable path, javac and other commands can also use this command to config. For some other uses, we suggest to set JAVA_HOME. export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # pay attention to $PATH double quote export PATH=\"$PATH:$JAVA_HOME/bin\" . Must set PATH with double quote in ubuntu. ",
    "url": "/developers/NewToGluten.html#openjdk8",
    
    "relUrl": "/developers/NewToGluten.html#openjdk8"
  },"83": {
    "doc": "New To Gluten",
    "title": "Maven 3.6.3 or above",
    "content": "Maven Dowload Page And then set the environment setting. ",
    "url": "/developers/NewToGluten.html#maven-363-or-above",
    
    "relUrl": "/developers/NewToGluten.html#maven-363-or-above"
  },"84": {
    "doc": "New To Gluten",
    "title": "GCC 9.4 or above",
    "content": " ",
    "url": "/developers/NewToGluten.html#gcc-94-or-above",
    
    "relUrl": "/developers/NewToGluten.html#gcc-94-or-above"
  },"85": {
    "doc": "New To Gluten",
    "title": "Compile gluten using debug mode",
    "content": "If you want to just debug java/scala code, there is no need to compile cpp code with debug mode. You can just refer to build-gluten-with-velox-backend. If you need to debug cpp code, please compile the backend code and gluten cpp code with debug mode. ## compile velox backend with benchmark and tests to debug gluten_home/dev/builddeps-veloxbe.sh --build_tests=ON --build_benchmarks=ON --build_type=Debug . If you need to debug the tests in /gluten-ut, You need to compile java code with `-P spark-ut`. ",
    "url": "/developers/NewToGluten.html#compile-gluten-using-debug-mode",
    
    "relUrl": "/developers/NewToGluten.html#compile-gluten-using-debug-mode"
  },"86": {
    "doc": "New To Gluten",
    "title": "Java/scala code development with Intellij",
    "content": " ",
    "url": "/developers/NewToGluten.html#javascala-code-development-with-intellij",
    
    "relUrl": "/developers/NewToGluten.html#javascala-code-development-with-intellij"
  },"87": {
    "doc": "New To Gluten",
    "title": "Linux intellij local debug",
    "content": "Install the linux intellij version, and debug code locally. | Ask your linux maintainer to install the desktop, and then restart the server. | If you use Moba-XTerm to connect linux server, you don’t need to install x11 server, If not (e.g. putty), please follow this guide: X11 Forwarding: Setup Instructions for Linux and Mac . | Download intellij linux community version to linux server | Start Idea, bash &lt;idea_dir&gt;/idea.sh | . Notes: Sometimes, your desktop may stop accidently, left idea running. root@xx2:~bash idea-IC-221.5787.30/bin/idea.sh Already running root@xx2:~ps ux | grep intellij root@xx2:kill -9 &lt;pid&gt; . And then restart idea. ",
    "url": "/developers/NewToGluten.html#linux-intellij-local-debug",
    
    "relUrl": "/developers/NewToGluten.html#linux-intellij-local-debug"
  },"88": {
    "doc": "New To Gluten",
    "title": "Windows/Mac intellij remote debug",
    "content": "If you have Ultimate intellij, you can try to debug remotely. ",
    "url": "/developers/NewToGluten.html#windowsmac-intellij-remote-debug",
    
    "relUrl": "/developers/NewToGluten.html#windowsmac-intellij-remote-debug"
  },"89": {
    "doc": "New To Gluten",
    "title": "Set up gluten project",
    "content": ". | Make sure you have compiled gluten. | Load the gluten by File-&gt;Open, select &lt;gluten_home/pom.xml&gt;. | Activate your profiles such as , and Reload Maven Project, you will find all your need modules have been activated. | Create breakpoint and debug as you wish, maybe you can try CTRL+N to find TestOperator to start your test. | . ",
    "url": "/developers/NewToGluten.html#set-up-gluten-project",
    
    "relUrl": "/developers/NewToGluten.html#set-up-gluten-project"
  },"90": {
    "doc": "New To Gluten",
    "title": "Java/Scala code style",
    "content": "Intellij IDE supports importing settings for Java/Scala code style. You can import intellij-codestyle.xml to your IDE. See Intellij guide. To generate a fix for Java/Scala code style, you can run one or more of the below commands according to the code modules involved in your PR. For Velox backend: . mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-velox -Prss -Pspark-3.3 -Pspark-ut -DskipTests . For Clickhouse backend: . mvn spotless:apply -Pbackends-clickhouse -Pspark-3.2 -Pspark-ut -DskipTests mvn spotless:apply -Pbackends-clickhouse -Pspark-3.3 -Pspark-ut -DskipTests . ",
    "url": "/developers/NewToGluten.html#javascala-code-style",
    
    "relUrl": "/developers/NewToGluten.html#javascala-code-style"
  },"91": {
    "doc": "New To Gluten",
    "title": "CPP code development with Visual Studio Code",
    "content": "This guide is for remote debug. We will connect the remote linux server by SSH. Download the windows vscode software The important leftside bar is: . | Explorer (Project structure) | Search | Run and Debug | Extensions (Install C/C++ Extension Pack, Remote Development, GitLens at least, C++ Test Mate is also suggested) | Remote Explorer (Connect linux server by ssh command, click +, then input ssh user@10.1.7.003) | Manage (Settings) | . Input your password in the above pop-up window, it will take a few minutes to install linux vscode server in remote machine folder ~/.vscode-server If download failed, delete this folder and try again. ",
    "url": "/developers/NewToGluten.html#cpp-code-development-with-visual-studio-code",
    
    "relUrl": "/developers/NewToGluten.html#cpp-code-development-with-visual-studio-code"
  },"92": {
    "doc": "New To Gluten",
    "title": "Usage",
    "content": "Set up project . File-&gt;Open Folder // select gluten folder Select cpp/CmakeList.txt as command prompt Select gcc version as command prompt . Settings . VSCode support 2 ways to set user setting. | Manage-&gt;Command Palette(Open settings.json, search by Preferences: Open Settings (JSON)) | Manage-&gt;Settings (Common setting) | . Build by vscode . VSCode will try to compile the debug version in /build. And we need to compile velox debug mode before, if you have compiled velox release mode, you just need to do. # Build the velox debug version in &lt;velox_home&gt;/_build/debug make debug EXTRA_CMAKE_FLAGS=\"-DVELOX_ENABLE_PARQUET=ON -DENABLE_HDFS=ON -DVELOX_BUILD_TESTING=OFF -DVELOX_ENABLE_DUCKDB=ON -DVELOX_BUILD_TEST_UTILS=ON\" . Then gluten will link velox debug library. Just click build in bottom bar, you will get intellisense search and link. Debug . The default compile command does not enable test and benchmark, so we cannot get any executable file Open the file in &lt;gluten_home&gt;/.vscode/settings.json (create if not exists) . { \"cmake.configureArgs\": [ \"-DBUILD_BENCHMARKS=ON\", \"-DBUILD_TESTS=ON\" ], \"C_Cpp.default.configurationProvider\": \"ms-vscode.cmake-tools\" } . Then we can get some executables, take velox_shuffle_writer_test as example . Click Run and Debug to create launch.json in &lt;gluten_home&gt;/.vscode/launch.json Click Add Configuration in the top of launch.json, select gdb launch or attach to exists program launch.json example . { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"velox shuffle writer test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/build/velox/tests/velox_shuffle_writer_test\", \"args\": [\"--gtest_filter=*TestSinglePartPartitioner*\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] }, { \"name\": \"benchmark test\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/./generic_benchmark\", \"args\": [\"/mnt/DP_disk1/code/gluten/cpp/velox/benchmarks/query.json\", \"--threads=1\"], \"stopAtEntry\": false, \"cwd\": \"${fileDirname}\", \"environment\": [], \"externalConsole\": false, \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true }, { \"description\": \"Set Disassembly Flavor to Intel\", \"text\": \"-gdb-set disassembly-flavor intel\", \"ignoreFailures\": true } ] } ] } . Change name, program, args to yours . Then you can create breakpoint and debug in Run and Debug section. Velox debug . For some velox tests such as ParquetReaderTest, tests need to read the parquet file in &lt;velox_home&gt;/velox/dwio/parquet/tests/examples, you should let the screen on ParquetReaderTest.cpp, then click Start Debuging, otherwise you will raise No such file or directory exception . ",
    "url": "/developers/NewToGluten.html#usage",
    
    "relUrl": "/developers/NewToGluten.html#usage"
  },"93": {
    "doc": "New To Gluten",
    "title": "Usefule notes",
    "content": "Upgrade vscode . No need to upgrade vscode version, if upgraded, will download linux server again, switch update mode to off Search update in Manage-&gt;Settings to turn off update mode . Colour setting . \"workbench.colorTheme\": \"Quiet Light\", \"files.autoSave\": \"afterDelay\", \"workbench.colorCustomizations\": { \"editor.wordHighlightBackground\": \"#063ef7\", // \"editor.selectionBackground\": \"#d1d1c6\", // \"tab.activeBackground\": \"#b8b9988c\", \"editor.selectionHighlightBackground\": \"#c5293e\" }, . Clang format . Now gluten uses clang-format 12 to format source files. apt-get install clang-format-12 . Set config in settings.json . \"clang-format.executable\": \"clang-format-12\", \"editor.formatOnSave\": true, . If exists multiple clang-format version, formatOnSave may not take effect, specify the default formatter Search default formatter in Settings, select Clang-Format. If your formatOnSave still make no effect, you can use shortcut SHIFT+ALT+F to format one file mannually. ",
    "url": "/developers/NewToGluten.html#usefule-notes",
    
    "relUrl": "/developers/NewToGluten.html#usefule-notes"
  },"94": {
    "doc": "New To Gluten",
    "title": "Debug cpp code with coredump",
    "content": "mkdir -p /mnt/DP_disk1/core sysctl -w kernel.core_pattern=/mnt/DP_disk1/core/core-%e-%p-%t cat /proc/sys/kernel/core_pattern # set the core file to unlimited size echo \"ulimit -c unlimited\" &gt;&gt; ~/.bashrc # then you will get the core file at `/mnt/DP_disk1/core` when the program crashes # gdb -c corefile # gdb &lt;gluten_home&gt;/cpp/build/releases/libgluten.so 'core-Executor task l-2000883-1671542526' . ‘core-Executor task l-2000883-1671542526’ is the generated core file name. (gdb) bt (gdb) f7 (gdb) set print pretty on (gdb) p *this . | Get the backtrace | Switch to 7th stack | Print the variable in a more readable way | Print the variable fields | . Sometimes you only get the cpp exception message, you can generate core dump file by the following code: . char* p = nullptr; *p = 'a'; . or by the following commands: . | gcore &lt;pid&gt; | kill -s SIGSEGV &lt;pid&gt; | . ",
    "url": "/developers/NewToGluten.html#debug-cpp-code-with-coredump",
    
    "relUrl": "/developers/NewToGluten.html#debug-cpp-code-with-coredump"
  },"95": {
    "doc": "New To Gluten",
    "title": "Debug cpp with gdb",
    "content": "You can use gdb to debug tests and benchmarks. And also you can debug jni call. Place the following code to your debug path. pid_t pid = getpid(); printf(\"----------------------------------pid: %lun\", pid); sleep(10); . You can also get the pid by java command or grep java program when executing unit test. jps 1375551 ScalaTestRunner ps ux | grep TestOperator . Execute gdb command to debug: . gdb attach &lt;pid&gt; . gdb attach 1375551 wait to attach.... (gdb) b &lt;velox_home&gt;/velox/substrait/SubstraitToVeloxPlan.cpp:577 (gdb) c . ",
    "url": "/developers/NewToGluten.html#debug-cpp-with-gdb",
    
    "relUrl": "/developers/NewToGluten.html#debug-cpp-with-gdb"
  },"96": {
    "doc": "New To Gluten",
    "title": "Run TPC-H and TPC-DS",
    "content": "We supply &lt;gluten_home&gt;/tools/gluten-it to execute these queries Refer to velox_be.yml . ",
    "url": "/developers/NewToGluten.html#run-tpc-h-and-tpc-ds",
    
    "relUrl": "/developers/NewToGluten.html#run-tpc-h-and-tpc-ds"
  },"97": {
    "doc": "New To Gluten",
    "title": "Run gluten+velox on clean machine",
    "content": "We can run gluten + velox on clean machine by one command (supported OS: Ubuntu20.04/22.04, Centos 7/8, etc.). spark-shell --name run_gluten \\ --master yarn --deploy-mode client \\ --conf spark.plugins=io.glutenproject.GlutenPlugin \\ --conf spark.memory.offHeap.enabled=true \\ --conf spark.memory.offHeap.size=20g \\ --jars https://github.com/oap-project/gluten/releases/download/v1.0.0/gluten-velox-bundle-spark3.2_2.12-ubuntu_20.04_x86_64-1.0.0.jar \\ --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager . ",
    "url": "/developers/NewToGluten.html#run-glutenvelox-on-clean-machine",
    
    "relUrl": "/developers/NewToGluten.html#run-glutenvelox-on-clean-machine"
  },"98": {
    "doc": "New To Gluten",
    "title": "New To Gluten",
    "content": "Help users to debug and test with gluten. For intel internal developer, you could refer to internal wiki New Employee Guide to get more information such as proxy settings, Gluten has cpp code and java/scala code, we can use some useful IDE to read and debug. ",
    "url": "/developers/NewToGluten.html",
    
    "relUrl": "/developers/NewToGluten.html"
  },"99": {
    "doc": "Substrait Modifications",
    "title": "Substrait Modifications in Gluten",
    "content": "Substrait is a project aiming to create a well-defined, cross-language specification for data compute operations. Since it is still under active development, there are some lacking representations for Gluten needed computing operations. At the same time, some existing representations need to be modified a bit to satisfy the needs of computing. In Gluten, the base version of Substrait is v0.23.0. This page records all the Gluten changes to Substrait proto files for reference. It is preferred to upstream these changes to Substrait, but for those cannot be upstreamed, alternatives like AdvancedExtension could be considered. ",
    "url": "/developers/SubstraitModifications.html#substrait-modifications-in-gluten",
    
    "relUrl": "/developers/SubstraitModifications.html#substrait-modifications-in-gluten"
  },"100": {
    "doc": "Substrait Modifications",
    "title": "Modifications to algebra.proto",
    "content": ". | Added JsonReadOptions and TextReadOptions in FileOrFiles(#1584). | Changed join type JOIN_TYPE_SEMI to JOIN_TYPE_LEFT_SEMI and JOIN_TYPE_RIGHT_SEMI(#408). | Added WindowRel, added column_name and window_type in WindowFunction, changed Unbounded in WindowFunction into Unbounded_Preceding and Unbounded_Following, and added WindowType(#485). | Added output_schema in RelRoot(#1901). | Added ExpandRel(#1361). | Added GenerateRel(#574). | Added PartitionColumn in LocalFiles(#2405). | Added WriteRel (#3690). | . ",
    "url": "/developers/SubstraitModifications.html#modifications-to-algebraproto",
    
    "relUrl": "/developers/SubstraitModifications.html#modifications-to-algebraproto"
  },"101": {
    "doc": "Substrait Modifications",
    "title": "Modifications to type.proto",
    "content": ". | Added Nothing in Type(#791). | Added names in Struct(#1878). | Added PartitionColumns in NamedStruct(#320). | Remove PartitionColumns and add column_types in NamedStruct(#2405). | . ",
    "url": "/developers/SubstraitModifications.html#modifications-to-typeproto",
    
    "relUrl": "/developers/SubstraitModifications.html#modifications-to-typeproto"
  },"102": {
    "doc": "Substrait Modifications",
    "title": "Substrait Modifications",
    "content": " ",
    "url": "/developers/SubstraitModifications.html",
    
    "relUrl": "/developers/SubstraitModifications.html"
  },"103": {
    "doc": "Velox Backend",
    "title": "Gluten Velox Backend Documents",
    "content": " ",
    "url": "/docs/velox/#gluten-velox-backend-documents",
    
    "relUrl": "/docs/velox/#gluten-velox-backend-documents"
  },"104": {
    "doc": "Velox Backend",
    "title": "Velox Backend",
    "content": " ",
    "url": "/docs/velox/",
    
    "relUrl": "/docs/velox/"
  },"105": {
    "doc": "Build Parameters for Velox Backend",
    "title": "Build Parameters",
    "content": "Native build parameters for buildbundle-veloxbe.sh or builddeps-veloxbe.sh . Please set them via --, e.g. --build_type=Release. | Parameters | Description | Default | . | build_type | Build type for Velox &amp; gluten cpp, CMAKE_BUILD_TYPE. | Release | . | build_tests | Build gluten cpp tests. | OFF | . | build_examples | Build udf example. | OFF | . | build_benchmarks | Build gluten cpp benchmarks. | OFF | . | build_jemalloc | Build with jemalloc. | ON | . | build_protobuf | Build protobuf lib. | ON | . | enable_qat | Enable QAT for shuffle data de/compression. | OFF | . | enable_iaa | Enable IAA for shuffle data de/compression. | OFF | . | enable_hbm | Enable HBM allocator. | OFF | . | enable_s3 | Build with S3 support. | OFF | . | enable_gcs | Build with GCs support. | OFF | . | enable_hdfs | Build with HDFS support. | OFF | . | enable_abfs | Build with ABFS support. | OFF | . | enable_ep_cache | Enable caching for external project build (Velox). | OFF | . | enable_vcpkg | Enable vcpkg for static build. | OFF | . | run_setup_script | Run setup script to install Velox dependencies. | ON | . | velox_repo | Specify your own Velox repo to build. | ”” | . | velox_branch | Specify your own Velox branch to build. | ”” | . | velox_home | Specify your own Velox source path to build. | ”” | . | build_velox_tests | Build Velox tests. | OFF | . | build_velox_benchmarks | Build Velox benchmarks (velox_tests and connectors will be disabled if ON) | OFF | . | compile_arrow_java | Compile arrow java for gluten build to use to fix invalid pointer issues. | OFF | . Velox build parameters for build_velox.sh . Please set them via --, e.g., --velox_home=/YOUR/PATH. | Parameters | Description | Default | . | velox_home | Specify Velox source path to build. | GLUTEN_SRC/ep/build-velox/build/velox_ep | . | build_type | Velox build type, i.e., CMAKE_BUILD_TYPE. | Release | . | enable_s3 | Build Velox with S3 support. | OFF | . | enable_gcs | Build Velox with GCS support. | OFF | . | enable_hdfs | Build Velox with HDFS support. | OFF | . | enable_abfs | Build Velox with ABFS support. | OFF | . | run_setup_script | Run setup script to install Velox dependencies before build. | ON | . | enable_ep_cache | Enable and reuse cache of Velox build. | OFF | . | build_test_utils | Build Velox with cmake arg -DVELOX_BUILD_TEST_UTILS=ON if ON. | OFF | . | build_tests | Build Velox test. | OFF | . | build_benchmarks | Build Velox benchmarks. | OFF | . | compile_arrow_java | Build arrow java for gluten build to use to fix invalid pointer issues. | OFF | . Maven build parameters . The below parameters can be set via -P for mvn. | Parameters | Description | Default state | . | backends-velox | Build Gluten Velox backend. | disabled | . | backends-clickhouse | Build Gluten ClickHouse backend. | disabled | . | rss | Build Gluten with Remote Shuffle Service, only applicable for Velox backend. | disabled | . | delta | Build Gluten with Delta Lake support. | disabled | . | iceberg | Build Gluten with Iceberg support. | disabled | . | spark-3.2 | Build Gluten for Spark 3.2. | enabled | . | spark-3.3 | Build Gluten for Spark 3.3. | disabled | . | spark-3.4 | Build Gluten for Spark 3.4. | disabled | . ",
    "url": "/docs/velox/build_parameters#build-parameters",
    
    "relUrl": "/docs/velox/build_parameters#build-parameters"
  },"106": {
    "doc": "Build Parameters for Velox Backend",
    "title": "Gluten Jar for Deployment",
    "content": "The gluten jar built out is under GLUTEN_SRC/package/target/. It’s name pattern is gluten-&lt;backend_type&gt;-bundle-spark&lt;spark.bundle.version&gt;_&lt;scala.binary.version&gt;-&lt;os.detected.release&gt;_&lt;os.detected.release.version&gt;-&lt;project.version&gt;.jar. | Spark Version | spark.bundle.version | scala.binary.version | . | 3.2.2 | 3.2 | 2.12 | . | 3.3.1 | 3.3 | 2.12 | . | 3.4.2 | 3.4 | 2.12 | . ",
    "url": "/docs/velox/build_parameters#gluten-jar-for-deployment",
    
    "relUrl": "/docs/velox/build_parameters#gluten-jar-for-deployment"
  },"107": {
    "doc": "Build Parameters for Velox Backend",
    "title": "Build Parameters for Velox Backend",
    "content": " ",
    "url": "/docs/velox/build_parameters",
    
    "relUrl": "/docs/velox/build_parameters"
  },"108": {
    "doc": "Using GCS with Gluten",
    "title": "Working with GCS",
    "content": " ",
    "url": "/docs/velox/gcs#working-with-gcs",
    
    "relUrl": "/docs/velox/gcs#working-with-gcs"
  },"109": {
    "doc": "Using GCS with Gluten",
    "title": "Installing the gcloud CLI",
    "content": "To access GCS Objects using Gluten and Velox, first you have to [download an install the gcloud CLI] (https://cloud.google.com/sdk/docs/install). ",
    "url": "/docs/velox/gcs#installing-the-gcloud-cli",
    
    "relUrl": "/docs/velox/gcs#installing-the-gcloud-cli"
  },"110": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS using a user account",
    "content": "This is recommended for regular users, follow the instructions to authorize a user account. After these steps, no specific configuration is required for Gluten, since the authorization was handled entirely by the gcloud tool. ",
    "url": "/docs/velox/gcs#configuring-gcs-using-a-user-account",
    
    "relUrl": "/docs/velox/gcs#configuring-gcs-using-a-user-account"
  },"111": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS using a credential file",
    "content": "For workloads that need to be fully automated, manually authorizing can be problematic. For such cases it is better to use a json file with the credentials. This is described in the [instructions to configure a service account]https://cloud.google.com/sdk/docs/authorizing#service-account. Such json file with the credetials can be passed to Gluten: . spark.hadoop.fs.gs.auth.type SERVICE_ACCOUNT_JSON_KEYFILE spark.hadoop.fs.gs.auth.service.account.json.keyfile // path to the json file with the credentials. ",
    "url": "/docs/velox/gcs#configuring-gcs-using-a-credential-file",
    
    "relUrl": "/docs/velox/gcs#configuring-gcs-using-a-credential-file"
  },"112": {
    "doc": "Using GCS with Gluten",
    "title": "Configuring GCS endpoints",
    "content": "For cases when a GCS mock is used, an optional endpoint can be provided: . spark.hadoop.fs.gs.storage.root.url // url to the mock gcs service including starting with http or https . ",
    "url": "/docs/velox/gcs#configuring-gcs-endpoints",
    
    "relUrl": "/docs/velox/gcs#configuring-gcs-endpoints"
  },"113": {
    "doc": "Using GCS with Gluten",
    "title": "Using GCS with Gluten",
    "content": "Object stores offered by CSPs such as GCS are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use a GCS endpoint as your data source, please ensure you are using the following GCS configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to GCS with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/docs/velox/gcs",
    
    "relUrl": "/docs/velox/gcs"
  },"114": {
    "doc": "Using S3 with Gluten",
    "title": "Working with S3",
    "content": " ",
    "url": "/docs/velox/s3#working-with-s3",
    
    "relUrl": "/docs/velox/s3#working-with-s3"
  },"115": {
    "doc": "Using S3 with Gluten",
    "title": "Configuring S3 endpoint",
    "content": "S3 provides the endpoint based method to access the files, here’s the example configuration. Users may need to modify some values based on real setup. spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider spark.hadoop.fs.s3a.access.key XXXXXXXXX spark.hadoop.fs.s3a.secret.key XXXXXXXXX spark.hadoop.fs.s3a.endpoint https://s3.us-west-1.amazonaws.com spark.hadoop.fs.s3a.connection.ssl.enabled true spark.hadoop.fs.s3a.path.style.access false . ",
    "url": "/docs/velox/s3#configuring-s3-endpoint",
    
    "relUrl": "/docs/velox/s3#configuring-s3-endpoint"
  },"116": {
    "doc": "Using S3 with Gluten",
    "title": "Configuring S3 instance credentials",
    "content": "S3 also provides other methods for accessing, you can also use instance credentials by setting the following config . spark.hadoop.fs.s3a.use.instance.credentials true . Note that in this case, “spark.hadoop.fs.s3a.endpoint” won’t take affect as Gluten will use the endpoint set during instance creation. ",
    "url": "/docs/velox/s3#configuring-s3-instance-credentials",
    
    "relUrl": "/docs/velox/s3#configuring-s3-instance-credentials"
  },"117": {
    "doc": "Using S3 with Gluten",
    "title": "Configuring S3 IAM roles",
    "content": "You can also use iam role credentials by setting the following configurations. Instance credentials have higher priority than iam credentials. spark.hadoop.fs.s3a.iam.role xxxx spark.hadoop.fs.s3a.iam.role.session.name xxxx . Note that spark.hadoop.fs.s3a.iam.role.session.name is optional. ",
    "url": "/docs/velox/s3#configuring-s3-iam-roles",
    
    "relUrl": "/docs/velox/s3#configuring-s3-iam-roles"
  },"118": {
    "doc": "Using S3 with Gluten",
    "title": "Other authentatication methods are not supported yet",
    "content": ". | AWS temporary credential | . ",
    "url": "/docs/velox/s3#other-authentatication-methods-are-not-supported-yet",
    
    "relUrl": "/docs/velox/s3#other-authentatication-methods-are-not-supported-yet"
  },"119": {
    "doc": "Using S3 with Gluten",
    "title": "Log granularity of AWS C++ SDK in velox",
    "content": "You can change log granularity of AWS C++ SDK by setting the spark.gluten.velox.awsSdkLogLevel configuration. The Allowed values are: . | OFF | FATAL | ERROR | WARN | INFO | DEBUG | TRACE | . ",
    "url": "/docs/velox/s3#log-granularity-of-aws-c-sdk-in-velox",
    
    "relUrl": "/docs/velox/s3#log-granularity-of-aws-c-sdk-in-velox"
  },"120": {
    "doc": "Using S3 with Gluten",
    "title": "Local Caching support",
    "content": "Velox supports a local cache when reading data from HDFS/S3. The feature is very useful if remote storage is slow, e.g., reading from a public S3 bucket and stronger performance is desired. With this feature, Velox can asynchronously cache the data on local disk when reading from remote storage, and the future reading requests on already cached blocks will be serviced from local cache files. To enable the local caching feature, below configurations are required: . spark.gluten.sql.columnar.backend.velox.cacheEnabled // enable or disable velox cache, default false. spark.gluten.sql.columnar.backend.velox.memCacheSize // the total size of in-mem cache, default is 128MB. spark.gluten.sql.columnar.backend.velox.ssdCachePath // the folder to store the cache files, default is \"/tmp\". spark.gluten.sql.columnar.backend.velox.ssdCacheSize // the total size of the SSD cache, default is 128MB. Velox will do in-mem cache only if this value is 0. spark.gluten.sql.columnar.backend.velox.ssdCacheShards // the shards of the SSD cache, default is 1. spark.gluten.sql.columnar.backend.velox.ssdCacheIOThreads // the IO threads for cache promoting, default is 1. Velox will try to do \"read-ahead\" if this value is bigger than 1 spark.gluten.sql.columnar.backend.velox.ssdODirect // enable or disable O_DIRECT on cache write, default false. It’s recommended to mount SSDs to the cache path to get the best performance of local caching. On the start up of Spark context, the cache files will be allocated under “spark.gluten.sql.columnar.backend.velox.cachePath”, with UUID based suffix, e.g. “/tmp/cache.13e8ab65-3af4-46ac-8d28-ff99b2a9ec9b0”. Gluten is not able to reuse older caches for now, and the old cache files are left there after Spark context shutdown. ",
    "url": "/docs/velox/s3#local-caching-support",
    
    "relUrl": "/docs/velox/s3#local-caching-support"
  },"121": {
    "doc": "Using S3 with Gluten",
    "title": "Using S3 with Gluten",
    "content": "Object stores offered by CSPs such as AWS S3 are important for users of Gluten to store their data. This doc will discuss all details of configs, and use cases around using Gluten with object stores. In order to use an S3 endpoint as your data source, please ensure you are using the following S3 configs in your spark-defaults.conf. If you’re experiencing any issues authenticating to S3 with additional auth mechanisms, please reach out to us using the ‘Issues’ tab. ",
    "url": "/docs/velox/s3",
    
    "relUrl": "/docs/velox/s3"
  },"122": {
    "doc": "About",
    "title": "About",
    "content": "This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com . You can find the source code for Minima at GitHub: jekyll / minima . You can find the source code for Jekyll at GitHub: jekyll / jekyll . ",
    "url": "/about/",
    
    "relUrl": "/about/"
  },"123": {
    "doc": "Apache Software Foundation",
    "title": "Apache Software Foundation",
    "content": "Apache Software Foundation(ASF) . License . Event . Sponsorship . Thanks . Security . Privacy . ",
    "url": "/asf/",
    
    "relUrl": "/asf/"
  },"124": {
    "doc": "Contact Us",
    "title": "Contact Us",
    "content": "This plugin is still under active development now, and doesn’t have a stable release. Welcome to evaluate it. If you encounter any issues or have any suggestions, please submit to our issue list. We’d love to hear your feedback . Apache has meticulously set up dedicated mailing lists for every project, underscoring the pivotal role of these platforms in fostering communication within the Apache community. Mailing lists serve as the backbone for numerous aspects of community operation and maintenance. They facilitate technical discussions, idea sharing, project Q&amp;A, decision-making on new features or changes, release voting, and more. Any topic relevant to the project can be discussed, making mailing lists the primary platform for community engagement and collaboration. By subscribing to this mailing list, you’ll receive timely updates on the latest Linkis community developments, enabling you to stay ahead and engaged with the community’s progress. | Mailing List | Description | Subscribe | Unsubscribe | . | dev@gluten.apache.org | Community Activity | subscribe | unsubscribe | . | commits@gluten.apache.org | Code Repository Activity | subscribe | unsubscribe | . ",
    "url": "/contact-us.html",
    
    "relUrl": "/contact-us.html"
  },"125": {
    "doc": "Contact Us",
    "title": "How to subscribe Gluten community",
    "content": "To subscribe to the dev@gluten.apache.org mailing list as an example, please follow these steps: . | Send a blank email to dev-subscribe@gluten.apache.org | Check your inbox for an email titled confirm subscribe to dev@gluten.apache.org If not received within a reasonable time, ensure it’s not blocked by your email provider. If not blocked and no reply is received after a while, repeat step 1 | Reply directly to the confirmation email without altering the subject or adding content. | Await an email titled WELCOME to dev@linkis.apache.org | Upon receiving the welcome email, you’re successfully subscribed. To engage in discussions, email dev@gluten.apache.org, which will reach all subscribers. | . To unsubscribe to the dev@gluten.apache.org mailing list as an example, please follow these steps: . | Send a blank email to dev-unsubscribe@gluten.apache.org | Check your inbox for an email titled confirm unsubscribe from dev@gluten.apache.org | Reply directly to the confirmation email without altering the subject or adding content. | Await an email titled GOODBYE from dev@gluten.apache.org | Unsubsscribe success | . ",
    "url": "/contact-us.html#how-to-subscribe-gluten-community",
    
    "relUrl": "/contact-us.html#how-to-subscribe-gluten-community"
  },"126": {
    "doc": "Contact Us",
    "title": "Issues and Discussions",
    "content": "We use github to track bugs, feature requests, and answer questions. File an issue for a bug or feature request. ",
    "url": "/contact-us.html#issues-and-discussions",
    
    "relUrl": "/contact-us.html#issues-and-discussions"
  },"127": {
    "doc": "Docker script for CentOS 7",
    "title": "Docker script for CentOS 7",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on CentOS 7: . Run on host as root user: . docker pull centos:7 docker run -itd --name gluten centos:7 /bin/bash docker attach gluten . Run in docker: . yum -y install epel-release centos-release-scl yum -y install \\ git \\ dnf \\ cmake3 \\ devtoolset-9 \\ java-1.8.0-openjdk \\ java-1.8.0-openjdk-devel \\ ninja-build \\ wget \\ sudo # gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin # cmake 3.x is required ln -s /usr/bin/cmake3 /usr/local/bin/cmake # enable gcc 9 . /opt/rh/devtoolset-9/enable || exit 1 export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # If you have the same error with issue-3283, you need to add the parameter `--compile_arrow_java=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/developers/docker_centos7.html",
    
    "relUrl": "/developers/docker_centos7.html"
  },"128": {
    "doc": "Docker script for CentOS 8",
    "title": "Docker script for CentOS 8",
    "content": "Here is a docker script we verified to build Gluten+Velox backend on Centos8: . Run on host as root user: . docker pull centos:8 docker run -itd --name gluten centos:8 /bin/bash docker attach gluten . Run in docker: . #update mirror sed -i -e \"s|mirrorlist=|#mirrorlist=|g\" /etc/yum.repos.d/CentOS-* sed -i -e \"s|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g\" /etc/yum.repos.d/CentOS-* dnf install -y epel-release sudo yum install -y dnf-plugins-core yum config-manager --set-enabled powertools dnf --enablerepo=powertools install -y ninja-build dnf --enablerepo=powertools install -y libdwarf-devel dnf install -y --setopt=install_weak_deps=False ccache gcc-toolset-9 git wget which libevent-devel \\ openssl-devel re2-devel libzstd-devel lz4-devel double-conversion-devel \\ curl-devel cmake libicu-devel source /opt/rh/gcc-toolset-9/enable || exit 1 yum install -y java-1.8.0-openjdk-devel patch export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk export PATH=$JAVA_HOME/bin:$PATH #gluten need maven version &gt;=3.6.3 wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz tar -xvf apache-maven-3.8.8-bin.tar.gz mv apache-maven-3.8.8 /usr/lib/maven export MAVEN_HOME=/usr/lib/maven export PATH=${PATH}:${MAVEN_HOME}/bin git clone https://github.com/oap-project/gluten.git cd gluten # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` ./dev/buildbundle-veloxbe.sh . ",
    "url": "/developers/docker_centos8.html",
    
    "relUrl": "/developers/docker_centos8.html"
  },"129": {
    "doc": "Docker script for Ubuntu 22.04/20.04",
    "title": "Docker script for Ubuntu 22.04/20.04",
    "content": "To the first build, it’s suggested to build Gluten in a clean docker image. Otherwise it’s easy to run into library version conflict issues. Here is a docker script we verified to build Gluten+Velox backend on Ubuntu22.04/20.04: . Run on host as root user: . docker pull ubuntu:22.04 docker run -itd --network host --name gluten ubuntu:22.04 /bin/bash docker attach gluten . Run in docker: . apt-get update #install gcc and libraries to build arrow apt install software-properties-common apt install maven build-essential cmake libssl-dev libre2-dev libcurl4-openssl-dev clang lldb lld libz-dev git ninja-build uuid-dev autoconf-archive curl zip unzip tar pkg-config bison libtool flex vim #velox script needs sudo to install dependency libraries apt install sudo # make sure jemalloc is uninstalled, jemalloc will be build in vcpkg, which conflicts with the default jemalloc in system apt purge libjemalloc-dev libjemalloc2 librust-jemalloc-sys-dev #make sure jdk8 is used. New version of jdk is not supported apt install -y openjdk-8-jdk apt install -y default-jdk export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin:$PATH #manually install tzdata to avoid the interactive timezone config ln -fs /usr/share/zoneinfo/America/New_York /etc/localtime DEBIAN_FRONTEND=noninteractive apt-get install -y tzdata dpkg --configure -a #setup proxy on necessary #export http_proxy=xxxx #export https_proxy=xxxx #clone gluten git clone https://github.com/oap-project/gluten.git cd gluten/ #config maven proxy #mkdir ~/.m2/ #vim ~/.m2/settings.xml # the script download velox &amp; arrow and compile all dependency library automatically # To access HDFS or S3, you need to add the parameters `--enable_hdfs=ON` and `--enable_s3=ON` # It's suggested to build using static link, enabled by `--enable_vcpkg=ON` # For developer, it's suggested to enable Debug info, by --build_type=RelWithDebInfo. Note RelWithDebInfo uses -o2, release uses -o3 ./dev/buildbundle-veloxbe.sh --enable_vcpkg=ON --build_type=RelWithDebInfo . ",
    "url": "/developers/docker_ubuntu22.04.html",
    
    "relUrl": "/developers/docker_ubuntu22.04.html"
  },"130": {
    "doc": "Developers",
    "title": "Gluten Developers",
    "content": "This document provides a developer overview of the project and covers the following topics: . ",
    "url": "/developers/#gluten-developers",
    
    "relUrl": "/developers/#gluten-developers"
  },"131": {
    "doc": "Developers",
    "title": "Developers",
    "content": " ",
    "url": "/developers/",
    
    "relUrl": "/developers/"
  },"132": {
    "doc": "Documentation",
    "title": "Gluten Documents by version",
    "content": " ",
    "url": "/docs/#gluten-documents-by-version",
    
    "relUrl": "/docs/#gluten-documents-by-version"
  },"133": {
    "doc": "Documentation",
    "title": "Documentation",
    "content": " ",
    "url": "/docs/",
    
    "relUrl": "/docs/"
  },"134": {
    "doc": "Home",
    "title": "Overview",
    "content": "Gluten: Plugin to Double SparkSQL’s Performance. ",
    "url": "/#overview",
    
    "relUrl": "/#overview"
  },"135": {
    "doc": "Home",
    "title": "1 Introduction",
    "content": " ",
    "url": "/#1-introduction",
    
    "relUrl": "/#1-introduction"
  },"136": {
    "doc": "Home",
    "title": "1.1 Problem Statement",
    "content": "Apache Spark is a stable, mature project that has been developed for many years. It is one of the best frameworks to scale out for processing petabyte-scale datasets. However, the Spark community has had to address performance challenges that require various optimizations over time. As a key optimization in Spark 2.0, Whole Stage Code Generation is introduced to replace Volcano Model, which achieves 2x speedup. Henceforth, most optimizations are at query plan level. Single operator’s performance almost stops growing. On the other side, SQL engines have been researched for many years. There are a few libraries like Clickhouse, Arrow and Velox, etc. By using features like native implementation, columnar data format and vectorized data processing, these libraries can outperform Spark’s JVM based SQL engine. However, these libraries only support single node execution. ",
    "url": "/#11-problem-statement",
    
    "relUrl": "/#11-problem-statement"
  },"137": {
    "doc": "Home",
    "title": "1.2 Gluten’s Solution",
    "content": "“Gluten” is Latin for glue. The main goal of Gluten project is to “glue” native libraries with SparkSQL. Thus, we can benefit from high scalability of Spark SQL framework and high performance of native libraries. The basic rule of Gluten’s design is that we would reuse spark’s whole control flow and as many JVM code as possible but offload the compute-intensive data processing part to native code. Here is what Gluten does: . | Transform Spark’s whole stage physical plan to Substrait plan and send to native | Offload performance-critical data processing to native library | Define clear JNI interfaces for native libraries | Switch available native backends easily | Reuse Spark’s distributed control flow | Manage data sharing between JVM and native | Extensible to support more native accelerators | . ",
    "url": "/#12-glutens-solution",
    
    "relUrl": "/#12-glutens-solution"
  },"138": {
    "doc": "Home",
    "title": "1.3 Target User",
    "content": "Gluten’s target user is anyone who wants to accelerate SparkSQL fundamentally. As a plugin to Spark, Gluten doesn’t require any change for dataframe API or SQL query, but only requires user to make correct configuration. See Gluten configuration properties here. ",
    "url": "/#13-target-user",
    
    "relUrl": "/#13-target-user"
  },"139": {
    "doc": "Home",
    "title": "1.4 References",
    "content": "You can click below links for more related information. | Gluten Intro Video at Data AI Summit 2022 | Gluten Intro Article at Medium.com | Gluten Intro Article at Kyligence.io(in Chinese) | Velox Intro from Meta | . ",
    "url": "/#14-references",
    
    "relUrl": "/#14-references"
  },"140": {
    "doc": "Home",
    "title": "2 Architecture",
    "content": "The overview chart is like below. Substrait provides a well-defined cross-language specification for data compute operations (see more details here). Spark physical plan is transformed to Substrait plan. Then Substrait plan is passed to native through JNI call. On native side, the native operator chain will be built out and offloaded to native engine. Gluten will return Columnar Batch to Spark and Spark Columnar API (since Spark-3.0) will be used at execution time. Gluten uses Apache Arrow data format as its basic data format, so the returned data to Spark JVM is ArrowColumnarBatch. Currently, Gluten only supports Clickhouse backend &amp; Velox backend. Velox is a C++ database acceleration library which provides reusable, extensible and high-performance data processing components. More details can be found from https://github.com/facebookincubator/velox/. Gluten can also be extended to support more backends. There are several key components in Gluten: . | Query Plan Conversion: converts Spark’s physical plan to Substrait plan. | Unified Memory Management: controls native memory allocation. | Columnar Shuffle: shuffles Gluten columnar data. The shuffle service still reuses the one in Spark core. A kind of columnar exchange operator is implemented to support Gluten columnar data format. | Fallback Mechanism: supports falling back to Vanilla spark for unsupported operators. Gluten ColumnarToRow (C2R) and RowToColumnar (R2C) will convert Gluten columnar data and Spark’s internal row data if needed. Both C2R and R2C are implemented in native code as well | Metrics: collected from Gluten native engine to help identify bugs, performance bottlenecks, etc. The metrics are displayed in Spark UI. | Shim Layer: supports multiple Spark versions. We plan to only support Spark’s latest 2 or 3 releases. Currently, Spark-3.2, Spark-3.3 &amp; Spark-3.4 (experimental) are supported. | . ",
    "url": "/#2-architecture",
    
    "relUrl": "/#2-architecture"
  },"141": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"142": {
    "doc": "Gluten Release",
    "title": "Latest release for velox backend",
    "content": ". | Gluten-1.1.1 (Mar. 2 2024, non-Apache release) | . ",
    "url": "/release.html#latest-release-for-velox-backend",
    
    "relUrl": "/release.html#latest-release-for-velox-backend"
  },"143": {
    "doc": "Gluten Release",
    "title": "Archived releases",
    "content": ". | Gluten-1.1.0 (Nov. 30 2023, non-Apache release) | Gluten-1.0.0 (Jul. 13 2023, non-Apache release) | Gluten-0.5.0 (Apr. 7 2023, non-Apache release) | . ",
    "url": "/release.html#archived-releases",
    
    "relUrl": "/release.html#archived-releases"
  },"144": {
    "doc": "Gluten Release",
    "title": "Gluten Release",
    "content": "Gluten is a plugin for Apache Spark to double SparkSQL’s performance. ",
    "url": "/release.html",
    
    "relUrl": "/release.html"
  },"145": {
    "doc": "Velox Function Development",
    "title": "Developer Guide for Implementing Spark Built-in SQL Functions in Velox",
    "content": "In velox, two folders prestosql &amp; sparksql are holding most sql functions, respective for presto and spark. Gluten will ask velox to firstly register prestosql functions, then sparksql functions. So if prestosql and sparksql share same signature for a function, the sparksql function will overwrite the corresponding prestosql function. If the required function is lacking in both folders (exceptions are some common functions defined outside, like cast), we need to implement the missing function in sparksql folder. It is possible that a prestosql function has some semantic difference with the corresponding spark function, even though they share the same name and function signature. If so, we also need to do an implementation in sparksql folder, generally based on the original impl. for prestosql. There are a few spark functions that can behave differently for some special cases, depending on ANSI on or off. Currently, gluten does NOT support ANSI mode. So only ANSI off needs to be considered in implementing spark built-in functions in velox. Take BitwiseAndFunction as example: . template &lt;typename T&gt; struct BitwiseAndFunction { template &lt;typename TInput&gt; // For void return type, it indicates null result will never be obtained for non-null input. // For bool return type, it indicates null result can be obtained for non-null input (false for null). FOLLY_ALWAYS_INLINE void call(TInput&amp; result, TInput a, TInput b) { result = a &amp; b; } }; . It is templated, as well as the call function, to allow multiple types. In the above impl., the result will be null for null input. Please use callNullable if you need different behavior for null input, e.g., get a non-null result for null input. Also see callNullFree in velox document. It is used for fast evaluation in the case that any input has null. The below code will register the implemented function for all kinds of integer types. The specified name bitwise_and will be actually used in calling this function. registerBinaryIntegral&lt;BitwiseAndFunction&gt;({prefix + \"bitwise_and\"}); . Functions for complex types have similar implementations. See ArrayAverageFunction in velox/functions/prestosql/ArrayFunctions.h. Reference: . Velox’s official developer guide: . | velox/docs/develop/scalar-functions.rst | velox/examples/SimpleFunctions.cpp | . ",
    "url": "/developers/velox-function-development-guide.html#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox",
    
    "relUrl": "/developers/velox-function-development-guide.html#developer-guide-for-implementing-spark-built-in-sql-functions-in-velox"
  },"146": {
    "doc": "Velox Function Development",
    "title": "Velox Function Development",
    "content": " ",
    "url": "/developers/velox-function-development-guide.html",
    
    "relUrl": "/developers/velox-function-development-guide.html"
  }
}
